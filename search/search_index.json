{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"UFSA v2","text":"<p>Universal Federated Schema Architecture \u2014 a configuration-driven, SKOS-grounded reference engine.</p> <p>UFSA v2 normalizes diverse public standards (FHIR, ISO, IANA, SKOS, Shopify, OpenFIGI, SBOMs, SQL DDL) into a single semantic layer and a set of portable tables you can ingest anywhere.</p> <p>What you\u2019ll find here:</p> <ul> <li>A concise architecture overview and pipeline diagram</li> <li>Reference pages for the registry, emitters, profiles/overlays, and identifier/mapping registries</li> <li>Deep-dive research notes (moved under Research) that motivated the v2 target design</li> </ul> <p>Quick links:</p> <ul> <li>Target architecture: ref/0 \u2014 summary of the approach with pointers to research</li> <li>Registry &amp; emitters: ref/1 \u2014 how inputs are declared and outputs are produced</li> <li>Profiles/Overlays: ref/2 \u2014 contextual constraints over base structures</li> <li>Identifiers &amp; mappings: ref/3 \u2014 modeling FIGI/ISIN/etc. and preferences</li> </ul> <p>The engine is declarative: you add or update standards by editing a registry, not changing code.</p>"},{"location":"#architecture-diagram","title":"Architecture diagram","text":"<pre><code>flowchart TB\n  PR[Pointer registry]\n  ENG[Engine]\n  PJSON[JSON Schema]\n  PCSV[CSV]\n  PRDF[RDF]\n  PSBOM[CycloneDX SBOM]\n  PSQL[SQL AST]\n  SKOS[SKOS model]\n  PER[Per-scheme CSV + JSON]\n  GLOB[Global tables]\n  CAND[Mapping candidates]\n  TRK[Tracker and plan]\n\n  PR --&gt; ENG\n  ENG --&gt; PJSON\n  ENG --&gt; PCSV\n  ENG --&gt; PRDF\n  ENG --&gt; PSBOM\n  ENG --&gt; PSQL\n\n  PJSON --&gt; SKOS\n  PCSV  --&gt; SKOS\n  PRDF  --&gt; SKOS\n  PSBOM --&gt; SKOS\n  PSQL  --&gt; SKOS\n\n  SKOS --&gt; PER\n  SKOS --&gt; GLOB\n  SKOS --&gt; CAND\n\n  PER  --&gt; TRK\n  GLOB --&gt; TRK\n  CAND --&gt; TRK</code></pre>"},{"location":"#pipeline-stages-at-a-glance","title":"Pipeline stages at a glance","text":"Stage Purpose Sources Key components Outputs Declaration Declare which specs to ingest YAML registry <code>ufsa_v2/registry/pointer_registry.yaml</code> Plan for parsers Ingestion Acquire source artifacts Local fixtures (now); HTTP with cache/pin (later) Fetcher, scraper Raw spec files Parsing/Materialization Turn spec structure into typed entities JSON Schema, CSV, RDF, CycloneDX, SQL DDL Parsers in <code>ufsa_v2/parsers/</code> Concepts, schemes, relations Normalization Align to SKOS model \u2014 Core model in <code>ufsa_v2/core_models.py</code> Unified in\u2011memory graph Emission Persist portable tables \u2014 Emitters in <code>ufsa_v2/emitters/</code> Per\u2011scheme CSV/JSON + global tables"},{"location":"#inputs-outputs-map","title":"Inputs \u2192 Outputs map","text":"Input type Example fixture Parser Primary outputs JSON Schema FHIR R4 Patient <code>parsers/json_schema.py</code> Scheme + concepts + relations CSV ISO 4217 currency table <code>parsers/csv_schema.py</code> Concepts with notations/labels RDF/SKOS SKOS Core <code>parsers/rdf_skos.py</code> Enriched labels/definitions/mappings CycloneDX SBOM cyclonedx_example <code>parsers/cyclonedx.py</code> <code>software_components.csv</code> SQL DDL internal_dw_schema <code>parsers/sql_ast.py</code> <code>database_schemas.csv</code>"},{"location":"#features-and-maturity","title":"Features and maturity","text":"Capability Status Notes JSON Schema parser \u2705 Stable Extracts properties/definitions \u2192 SKOS concepts and relations CSV schema parser \u2705 Stable Structured field CSVs (FHIR, ISO, IANA, Shopify, OpenFIGI) RDF/SKOS ingester \u2705 Stable Loads core SKOS vocabulary for labels/definitions/mappings CycloneDX SBOM parser \ud83d\udfe1 Beta Emits <code>software_components.csv</code> with hashes/licenses/refs SQL DDL \u2192 AST parser \ud83d\udfe1 Beta Emits <code>database_schemas.csv</code>, basic FK detection Profiles evaluator \ud83d\udfe1 Experimental Apply/check constraints against build outputs Identifier/Mapping registries \ud83d\udfe1 In progress Curated links across identifier systems and schemes"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li>Run the pipeline locally (see README) to generate the tables under build/</li> <li>Browse the Research section for the long-form documents that informed UFSA v2</li> <li>Open an issue or PR with a new registry entry to add standards</li> </ul>"},{"location":"cv/cv-entries/","title":"CV Entries for UFSA v2","text":""},{"location":"cv/cv-entries/#for-a-data-architect-data-engineer-role","title":"For a Data Architect / Data Engineer Role","text":"<p>This version emphasizes the high-level architecture and the data-centric nature of the project.</p> <p>UFSA v2: Declarative, Federated Interoperability Engine</p> <ul> <li>Architected and developed a Python-based engine to ingest, normalize, and map heterogeneous data standards (e.g., FHIR, ISO, Shopify) into a unified, SKOS-grounded model to enable cross-domain analytics.</li> <li>Designed a declarative and extensible pipeline using a YAML registry and pluggable parsers, reducing the effort required to integrate new data sources.</li> <li>Implemented a novel artifact tracking system using SHA256 hashes to ensure data pipeline integrity, prevent drift, and guarantee reproducible outputs.</li> <li>Automated project governance by creating a cryptographic sealing tool to manage licensing and verify the integrity of the software supply chain.</li> </ul>"},{"location":"cv/cv-entries/#for-a-senior-python-developer-role","title":"For a Senior Python Developer Role","text":"<p>This version focuses on your Python expertise, tooling, and software engineering best practices.</p> <p>UFSA v2: Professional-Grade Python Application</p> <ul> <li>Developed a robust, configuration-driven Python application to solve complex data interoperability challenges, complete with a full suite of tests (<code>pytest</code>), modern linting (<code>ruff</code>), and CI/CD workflows (GitHub Actions).</li> <li>Engineered a pluggable parser system to handle diverse data formats (JSON Schema, CSV, RDF), demonstrating expertise in data modeling and transformation.</li> <li>Built sophisticated command-line tooling for developers, including a comprehensive <code>Makefile</code> and a custom-built utility for ensuring source code integrity via cryptographic sealing.</li> <li>Authored clear, comprehensive documentation and followed modern packaging standards (<code>pyproject.toml</code>, <code>uv</code>), ensuring the project is easy for others to adopt and contribute to.</li> </ul>"},{"location":"cv/cv-entries/#for-a-software-architect-tech-lead-role","title":"For a Software Architect / Tech Lead Role","text":"<p>This version is a concise, high-impact summary that highlights leadership, design, and security.</p> <p>UFSA v2: Federated Interoperability Engine</p> <ul> <li>Led the end-to-end design and implementation of a federated system for semantic data interoperability, grounding the architecture in the W3C SKOS standard to ensure correctness and industry alignment.</li> <li>Pioneered a \"governance-as-code\" approach by building automated tooling to manage software licensing and cryptographically verify source code integrity, addressing software supply chain security concerns.</li> <li>Delivered a highly extensible and maintainable solution by employing a declarative configuration model and a pluggable component architecture.</li> </ul>"},{"location":"ref/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/","title":"Target Architecture (UFSA v2) \u2014 concise overview","text":"<p>This page summarizes the Universal Federated Schema Architecture (UFSA v2) target, with links to the full research document.</p>"},{"location":"ref/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#purpose","title":"Purpose","text":"<ul> <li>Define a pragmatic, federated approach to interoperability that normalizes diverse public standards into a single SKOS\u2011based knowledge model and portable tables.</li> <li>Keep domain autonomy intact; UFSA provides the connective tissue via normalization and mappings.</li> </ul>"},{"location":"ref/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#core-ideas","title":"Core ideas","text":"<ul> <li>Declarative over imperative: add a standard by configuration (pointer registry), not code.</li> <li>SKOS at the core: ConceptScheme + Concept + mapping predicates (exact/close/broad/narrow/related).</li> <li>Pluggable pipeline: fetch \u2192 parse \u2192 normalize \u2192 emit.</li> <li>Federated governance: domains own their standards; UFSA stitches them together.</li> </ul>"},{"location":"ref/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#pipeline-at-a-glance","title":"Pipeline at a glance","text":"<p>1) Pointer registry declares standards and parser modules 2) Parsers ingest native specs (JSON Schema, CSV, RDF, SBOM, SQL DDL) 3) Normalization to SKOS concepts and relations 4) Emitters produce per\u2011scheme artifacts and consolidated tables</p>"},{"location":"ref/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#outputs","title":"Outputs","text":"<ul> <li>Per\u2011scheme JSON/CSV concept dumps</li> <li>Global tables: concept_schemes.csv, concepts.csv, semantic_relations.csv</li> <li>Specialized: software_components.csv (SBOM), database_schemas.csv (SQL AST)</li> </ul>"},{"location":"ref/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#where-to-go-next","title":"Where to go next","text":"<ul> <li>Full research doc: Architecture \u2014 Synthesis, Falsification, Refinement</li> <li>Registry &amp; Emitters overview: UFSA v2.0 Registry and Emitter Design</li> <li>Profiles/Overlays: Ref 2</li> <li>Identifiers &amp; Mappings: Ref 3</li> </ul>"},{"location":"ref/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/","title":"Registry and Emitters \u2014 concise overview","text":"<p>This page explains how UFSA v2 declares inputs and produces outputs. See the research doc for the deep dive and rationale.</p>"},{"location":"ref/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#pointer-registry-inputs","title":"Pointer registry (inputs)","text":"<ul> <li>A declarative list of standards with: id, name, governing body, specification URL, data format, parser module, concept scheme URI.</li> <li>Adding a standard = adding one entry.</li> </ul> <p>Supported formats and parsers:</p> <ul> <li>JSON Schema (FHIR)</li> <li>CSV (ISO, IANA)</li> <li>RDF (SKOS)</li> <li>HTML/Docs scraping (Shopify, OpenFIGI)</li> <li>CycloneDX SBOM</li> <li>SQL DDL (AST)</li> </ul>"},{"location":"ref/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#emitters-outputs","title":"Emitters (outputs)","text":"<ul> <li>Per\u2011scheme: scheme_name.concepts.{json,csv}</li> <li>Global tables: concept_schemes.csv, concepts.csv, semantic_relations.csv</li> <li>Specialized: software_components.csv (from SBOM), database_schemas.csv (from SQL AST)</li> </ul>"},{"location":"ref/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#contract-highlights","title":"Contract highlights","text":"<ul> <li>SKOS predicates for relations (broader/narrower/related; exact/close/broad/narrow/relatedMatch)</li> <li>Stable scheme_uri and concept identifiers within a minor line</li> <li>CSVs are UTF\u20118 with headers; duplication minimized</li> </ul>"},{"location":"ref/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#links","title":"Links","text":"<ul> <li>Full research doc: Registry &amp; Emitters \u2014 detailed</li> <li>Architecture overview: Ref 0</li> <li>Profiles/Overlays: Ref 2</li> <li>Identifiers &amp; Mappings: Ref 3</li> </ul>"},{"location":"ref/2_Profiles%20and%20Overlays/","title":"Profiles and Overlays (UFSA v2 \u2013 stub)","text":"<p>This short note outlines how Profiles/Overlays will extend the current engine. It complements the research doc \u201cFalsification &amp; Implementation\u201d (see Research/0) and aligns with the FHIR profiling pattern.</p>"},{"location":"ref/2_Profiles%20and%20Overlays/#purpose","title":"Purpose","text":"<p>Allow a base structure to be constrained or extended in a given context without duplicating schemas, e.g.:</p> <ul> <li>Postal vs. geodetic address</li> <li>Common\u2011law vs. civil\u2011law contract</li> <li>Implementation/vendor\u2011specific tweaks</li> </ul>"},{"location":"ref/2_Profiles%20and%20Overlays/#shape-proposed","title":"Shape (proposed)","text":"<ul> <li>Target: reference to a base structure or scheme element</li> <li>Constraints: cardinality flips, additional validations (regex, external authority checks), enums, bounds</li> <li>Extensions: add fields/notes that only apply in the profile\u2019s context</li> <li>Activation context: labels/conditions describing when to apply (jurisdiction, domain, use\u2011case)</li> </ul>"},{"location":"ref/2_Profiles%20and%20Overlays/#minimal-representation-initial","title":"Minimal representation (initial)","text":"<p>Profiles can start as simple YAML atop the registry:</p> <pre><code>kind: Profile\napiVersion: ufsa.org/v2.0\nmetadata:\n  name: PostalAddressProfile\n  domain: core\n  target: core:Address\nspec:\n  constraints:\n    - path: postalCode\n      cardinality: required\n      validation:\n        - type: external\n          authority: usps.com/validate\n</code></pre>"},{"location":"ref/2_Profiles%20and%20Overlays/#integration-points","title":"Integration points","text":"<ul> <li>Parsing stays unchanged; profiles are overlays applied post\u2011normalization</li> <li>Emitters may add profile\u2011annotated variants or output validation hints</li> <li>Tracker/plan: tasks to add priority profiles per domain</li> </ul>"},{"location":"ref/2_Profiles%20and%20Overlays/#next-steps","title":"Next steps","text":"<ul> <li>Finalize a minimal on\u2011disk schema and validator</li> <li>Add CLI to apply/check profiles over emitted tables</li> <li>Pilot with Address and Contract examples from the docs</li> </ul>"},{"location":"ref/3_Identifier%20and%20Mapping%20Registries/","title":"Identifier and Mapping Registries (UFSA v2 \u2013 stub)","text":"<p>This note sketches how UFSA will describe identifier systems and the relationships between them (e.g., FIGI vs. ISIN), as referenced in Docs 0/1.</p>"},{"location":"ref/3_Identifier%20and%20Mapping%20Registries/#why","title":"Why","text":"<ul> <li>Capture governance, scope, and syntax of identifier systems</li> <li>Model equivalence/overlap/hierarchy and contextual preferences</li> <li>Make trade\u2011offs explicit and machine\u2011readable (regulatory vs. vendor ecosystems)</li> </ul>"},{"location":"ref/3_Identifier%20and%20Mapping%20Registries/#identifiersystem-proposed","title":"IdentifierSystem (proposed)","text":"<p>Fields:</p> <ul> <li>name (unique)</li> <li>governingBody (ISO, OMG, IANA, \u2026)</li> <li>concept (what instances it identifies)</li> <li>syntax (regex or formal description)</li> <li>scope/granularity notes</li> </ul>"},{"location":"ref/3_Identifier%20and%20Mapping%20Registries/#mapping-proposed","title":"Mapping (proposed)","text":"<p>A declarative link between systems:</p> <ul> <li>systems: [\"isin\", \"figi\", \u2026]</li> <li>type: equivalentTo | broaderThan | narrowerThan | relatedTo | contextualPreference</li> <li>rules (optional): list of contexts and preferred system(s)</li> </ul> <p>Example:</p> <pre><code>kind: Mapping\napiVersion: ufsa.org/v2.0\nmetadata:\n  name: figi-isin-regulatory-mapping\n  domain: finance\nspec:\n  type: contextualPreference\n  systems:\n    - isin\n    - figi\n  rules:\n    - context: EU-MIFID2-Reporting\n      preferred: isin\n    - context: Internal-Data-Integration\n      preferred: figi\n</code></pre>"},{"location":"ref/3_Identifier%20and%20Mapping%20Registries/#integration-points","title":"Integration points","text":"<ul> <li>Emitters can surface system metadata in concept_schemes/relations tables or dedicated outputs</li> <li>Planners can stage mapping work as tasks tied to contexts (EU reporting vs. internal BI)</li> </ul>"},{"location":"ref/3_Identifier%20and%20Mapping%20Registries/#next-steps","title":"Next steps","text":"<ul> <li>Finalize minimal YAML schemas for IdentifierSystem and Mapping</li> <li>Add CLI plumbing and validation</li> <li>Pilot FIGI/ISIN, then expand to CUSIP/LEI/EAN/GTIN</li> </ul>"},{"location":"ref/4_UFSA2.1_SBOM_AST/","title":"UFSA v2.1 Implementation Plan: Integrating SBOM and AST Analysis","text":"<p>Document Purpose: This document provides a comprehensive plan for extending the UFSA v2 engine to ingest Software Bill of Materials (SBOM) and Abstract Syntax Tree (AST) data as first-class data sources. It details the required architectural additions, parser implementations, emitter enhancements, and the critical final step of integrating these new artifacts into the tracker and license sealing workflows.</p> <p>Strategic Goal: To evolve UFSA v2 from a standards interoperability engine into a holistic governance platform capable of creating a unified semantic map of an organization's formal data standards, software supply chain, and internal database schemas.</p>"},{"location":"ref/4_UFSA2.1_SBOM_AST/#phase-1-foundational-enhancements-registry-updates","title":"Phase 1: Foundational Enhancements &amp; Registry Updates","text":"<p>This phase focuses on preparing the existing architecture to recognize and handle the new data source types.</p>"},{"location":"ref/4_UFSA2.1_SBOM_AST/#11-create-new-parser-stubs","title":"1.1. Create New Parser Stubs","text":"<p>In the ufsa_v2/parsers/ directory, create two new files with boilerplate class definitions inheriting from BaseParser:</p> <ul> <li>parser_cyclonedx.py: This will handle SBOMs in the CycloneDX JSON format.  </li> <li>parser_ast_sql.py: This will handle SQL Data Definition Language (DDL) files.</li> </ul>"},{"location":"ref/4_UFSA2.1_SBOM_AST/#12-update-the-pointer-registry","title":"1.2. Update the Pointer Registry","text":"<p>Add new entries to ufsa_v2/registry/pointer_registry.yaml to register the new parsers and point them to fixture files. This makes the engine aware of the new capabilities.</p> <p># ... existing entries ...</p> <p># New entry for SBOM (CycloneDX) - standard_id: cyclonedx_example   standard_name: Example CycloneDX SBOM   governing_body: OWASP   specification_url: \"file://data/fixtures/sbom_example.json\"   data_format: \"CycloneDX-JSON\"   parser_module: \"ufsa_v2.parsers.parser_cyclonedx\"   canonical_concept_scheme_uri: \"http://ufsa.org/v2.1/standards/cyclonedx_example\"</p> <p># New entry for AST (SQL DDL) - standard_id: internal_dw_schema   standard_name: Internal Data Warehouse Schema   governing_body: Internal   specification_url: \"file://data/fixtures/dw_schema.sql\"   data_format: \"SQL-DDL\"   parser_module: \"ufsa_v2.parsers.parser_ast_sql\"   canonical_concept_scheme_uri: \"http://ufsa.org/v2.1/standards/internal_dw_schema\"</p>"},{"location":"ref/4_UFSA2.1_SBOM_AST/#13-create-fixture-files","title":"1.3. Create Fixture Files","text":"<p>In the data/fixtures/ directory, add the two files referenced above:</p> <ul> <li>sbom_example.json: A valid, simple CycloneDX JSON file containing a few components and dependencies.  </li> <li>dw_schema.sql: A simple SQL file with a few CREATE TABLE statements, including primary and foreign keys.</li> </ul>"},{"location":"ref/4_UFSA2.1_SBOM_AST/#phase-2-parser-implementation","title":"Phase 2: Parser Implementation","text":"<p>This is the core development phase where the logic for ingesting the new formats is built.</p>"},{"location":"ref/4_UFSA2.1_SBOM_AST/#21-implement-parser_cyclonedxpy","title":"2.1. Implement parser_cyclonedx.py","text":"<p>This parser will transform an SBOM's component list and dependency graph into the UFSA SKOS model.</p> <ul> <li>Dependencies: Add cyclonedx-python-lib to the project's dependencies.  </li> <li>Logic: </li> <li>Load the JSON file from the given URL.  </li> <li>The SBOM itself will be the skos:ConceptScheme.  </li> <li>Iterate through the components array. Each component becomes a skos:Concept.  <ul> <li>Use the component's purl (Package URL) as the basis for the concept's URI.  </li> <li>component.name -&gt; skos:prefLabel.  </li> <li>component.version -&gt; skos:notation.  </li> <li>component.description -&gt; skos:definition.  </li> </ul> </li> <li>Iterate through the dependencies graph. For each dependency link (dependsOn), create a skos:related relationship between the corresponding component concepts.</li> </ul>"},{"location":"ref/4_UFSA2.1_SBOM_AST/#22-implement-parser_ast_sqlpy","title":"2.2. Implement parser_ast_sql.py","text":"<p>This parser will transform SQL DDL into a structured representation of a database schema.</p> <ul> <li>Dependencies: Add sqlglot to the project's dependencies. This is a powerful SQL parser and transpiler.  </li> <li>Logic: </li> <li>Read the content of the .sql file.  </li> <li>Use sqlglot.parse() to generate an AST from the SQL text.  </li> <li>The database schema (e.g., the filename) will be the skos:ConceptScheme.  </li> <li>Traverse the AST to find all CREATE TABLE expressions.  <ul> <li>Each table becomes a skos:Concept. (e.g., .../internal_dw_schema#users).  </li> </ul> </li> <li>For each table, iterate through its column definitions.  <ul> <li>Each column becomes a skos:Concept that is skos:broader than its parent table concept (e.g., .../internal_dw_schema#users.user_id).  </li> <li>The column's data type (e.g., VARCHAR(255)) will be stored as a skos:note.  </li> </ul> </li> <li>Parse FOREIGN KEY constraints to create relationships. A foreign key from orders.user_id to users.user_id becomes a skos:relatedMatch between the two column concepts.</li> </ul>"},{"location":"ref/4_UFSA2.1_SBOM_AST/#phase-3-emitter-and-output-enhancements","title":"Phase 3: Emitter and Output Enhancements","text":"<p>The standard output tables are not perfectly suited for this new, specialized data. We will enhance the emitter to produce more specific tables for clarity.</p>"},{"location":"ref/4_UFSA2.1_SBOM_AST/#31-modify-the-csv-emitter","title":"3.1. Modify the CSV Emitter","text":"<p>In ufsa_v2/emitters/csv_emitter.py, add logic to generate two new, optional tables if the relevant data exists in the master graph.</p> <ul> <li>New Output: software_components.csv </li> <li>Trigger: Generate this file if any concepts exist within a ConceptScheme whose URI contains /standards/cyclonedx.  </li> <li>Columns: purl, name, version, description, scheme_uri.  </li> <li>Logic: Use a SPARQL query to select concepts from SBOM schemes and extract their labels, notations, and definitions into this structured format.  </li> <li>New Output: database_schemas.csv </li> <li>Trigger: Generate this file if any concepts exist within a ConceptScheme whose URI contains /standards/internal_dw.  </li> <li>Columns: table_name, column_name, data_type, concept_uri.  </li> <li>Logic: Use a SPARQL query to select concepts from SQL DDL schemes, parse the parent/child relationships to distinguish tables from columns, and extract the data type from the skos:note.</li> </ul>"},{"location":"ref/4_UFSA2.1_SBOM_AST/#32-update-conceptscsv-emitter","title":"3.2. Update concepts.csv Emitter","text":"<p>Modify the query for concepts.csv to exclude concepts that have been emitted into the new specialized tables to avoid data duplication.</p>"},{"location":"ref/4_UFSA2.1_SBOM_AST/#phase-4-integration-with-tracker-and-sealing-script","title":"Phase 4: Integration with Tracker and Sealing Script","text":"<p>This final phase ensures the new capabilities are fully integrated into the project's integrity and compliance framework.</p>"},{"location":"ref/4_UFSA2.1_SBOM_AST/#41-update-tracker-configuration","title":"4.1. Update Tracker Configuration","text":"<p>The tracker's logic for discovering files to hash might need to be updated. Ensure that the new output files (software_components.csv, database_schemas.csv) are automatically picked up and added to tracker.json and TRACKER.md when the pipeline runs.</p>"},{"location":"ref/4_UFSA2.1_SBOM_AST/#42-update-the-sealing-script-seal_licensepy","title":"4.2. Update the Sealing Script (SEAL_LICENSE.py)","text":"<p>The sealing script operates on source code files. The new parser modules must be included in its scope.</p> <ul> <li>Verify Paths: Check the list of directories that SEAL_LICENSE.py scans. Ensure that ufsa_v2/parsers/ is included so that parser_cyclonedx.py and parser_ast_sql.py are properly sealed with license headers.  </li> <li>Run and Verify: After updating, run make seal. Inspect the LICENSE_HASHES.md report to confirm the new parser files are present and have been correctly processed with initial and sealed hashes.</li> </ul>"},{"location":"ref/4_UFSA2.1_SBOM_AST/#43-update-distwhitelisttxt","title":"4.3. Update dist.whitelist.txt","text":"<p>If it exists, this file specifies which files are included in a distributable package. Add the paths to the new parser files to ensure they are included in any sealed archives created by make package-sealed.</p> <ul> <li>ufsa_v2/parsers/parser_cyclonedx.py  </li> <li>ufsa_v2/parsers/parser_ast_sql.py</li> </ul>"},{"location":"ref/4_UFSA2.1_SBOM_AST/#timeline-and-next-steps","title":"Timeline and Next Steps","text":"<p>This work can be broken down into a series of tasks, suitable for a sprint-based workflow.</p> <ul> <li>Task 1 (1-2 days): Implement Phase 1. Set up the foundational stubs, registry entries, and fixture files.  </li> <li>Task 2 (3-5 days): Implement Phase 2. Build and test the CycloneDX and SQL AST parsers. This is the most complex part of the plan.  </li> <li>Task 3 (2-3 days): Implement Phase 3. Enhance the CSV emitter to generate the new, specialized tables.  </li> <li>Task 4 (1 day): Implement Phase 4. Integrate the new source and output files with the tracker and sealing script. Run make pipeline, make seal, and make package-verify to confirm end-to-end success.  </li> <li>Task 5 (Ongoing): Update project documentation (README.md) to reflect the new capabilities.</li> </ul> <p>By following this plan, the agent can systematically extend UFSA v2, adding significant new value while maintaining the project's core principles of declarative configuration, robustness, and verifiable integrity.</p>"},{"location":"reference/core/","title":"Core Modules","text":""},{"location":"reference/core/#core-modules","title":"Core Modules","text":"<p>Command-line interface for UFSA v2.</p> <p>Provides subcommands to run the pipeline, inspect the tracker, manage the plan, validate registries and profiles, and use the offline-first fetcher utilities.</p> <p>Pipeline orchestration for UFSA v2.</p> <p>This module wires together the pointer registry, parser modules, and emitters to produce normalized artifacts. The runtime model is defined in <code>ufsa_v2.core_models</code> and is intentionally minimal. The primary entrypoint is <code>run_pipeline</code> which returns a list of output paths and updates a <code>Tracker</code>.</p> <p>Emit consolidated CSV tables for schemes, concepts, relations, and specials.</p> This module writes the global tables described in the docs <ul> <li>concept_schemes.csv</li> <li>concepts.csv</li> <li>semantic_relations.csv</li> <li>software_components.csv (if present)</li> <li>database_schemas.csv (if present)</li> </ul> <p>CycloneDX SBOM parser (fixture-backed).</p> <p>Transforms a minimal CycloneDX JSON (components/dependencies) into a <code>ConceptScheme</code> with software component concepts and dependency relations.</p> <p>Naive SQL DDL parser (fixture-backed).</p> <p>Parses simple <code>CREATE TABLE</code> statements and foreign keys from a SQL file into table and column concepts, with hierarchical and related relations.</p> <p>RDF/SKOS parser (fixture-backed).</p> <p>Loads an RDF file and extracts SKOS Concepts and relations into the runtime model. Supports labels, definitions, notations, altLabels, and mapping predicates when present.</p>"},{"location":"reference/core/#ufsa_v2.cli.main","title":"<code>main()</code>","text":"<p>CLI entrypoint.</p> Source code in <code>ufsa_v2/cli.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"CLI entrypoint.\"\"\"\n    parser = argparse.ArgumentParser(prog=\"ufsa-v2\", description=\"UFSA v2 Engine\")\n    sub = parser.add_subparsers(dest=\"command\", required=True)\n\n    run = sub.add_parser(\"run\", help=\"Run the ingestion-mapping-emission pipeline\")\n    run.add_argument(\n        \"--registry\",\n        type=Path,\n        required=True,\n        help=\"Path to pointer registry YAML\",\n    )\n    run.add_argument(\"--out\", type=Path, default=Path(\"build\"), help=\"Output directory\")\n    run.add_argument(\n        \"--tracker\",\n        type=Path,\n        default=Path(\"tracker.json\"),\n        help=\"Tracker file path\",\n    )\n\n    # Tracker utilities\n    tr = sub.add_parser(\"tracker\", help=\"Tracker utilities\")\n    tr_sub = tr.add_subparsers(dest=\"tracker_cmd\", required=True)\n    tr_verify = tr_sub.add_parser(\"verify\", help=\"Verify tracked file hashes vs filesystem\")\n    tr_verify.add_argument(\"--tracker\", type=Path, default=Path(\"tracker.json\"))\n    tr_list = tr_sub.add_parser(\"list\", help=\"List tracked files and hashes\")\n    tr_list.add_argument(\"--tracker\", type=Path, default=Path(\"tracker.json\"))\n    tr_touch = tr_sub.add_parser(\"touch\", help=\"Recompute hashes for currently tracked files and save\")\n    tr_touch.add_argument(\"--tracker\", type=Path, default=Path(\"tracker.json\"))\n\n    # plan subcommands\n    tr_plan = tr_sub.add_parser(\"plan\", help=\"Plan management utilities\")\n    plan_sub = tr_plan.add_subparsers(dest=\"plan_cmd\", required=True)\n    for subcmd in (\n        \"seed-docs\",\n        \"seed-state\",\n        \"seed-sbom-ast\",\n        \"list\",\n        \"stats\",\n        \"mark\",\n        \"add\",\n    ):\n        p = plan_sub.add_parser(subcmd)\n        p.add_argument(\"--tracker\", type=Path, default=Path(\"tracker.json\"))\n        if subcmd == \"mark\":\n            p.add_argument(\"id\", type=int)\n            p.add_argument(\"status\", choices=[\"todo\", \"in-progress\", \"done\", \"blocked\"])\n        if subcmd == \"add\":\n            p.add_argument(\"title\", type=str)\n            p.add_argument(\"--domain\", default=\"infrastructure\")\n            p.add_argument(\"--category\", default=\"feature\")\n            p.add_argument(\"--priority\", default=\"medium\")\n            p.add_argument(\"--status\", default=\"todo\")\n            p.add_argument(\"--rationale\", default=\"\")\n            p.add_argument(\"--doc-ref\", dest=\"doc_ref\", default=\"\")\n\n    # Registry utilities\n    reg = sub.add_parser(\"registry\", help=\"Registry utilities (identifier/mappings)\")\n    reg_sub = reg.add_subparsers(dest=\"registry_cmd\", required=True)\n    reg_sub.add_parser(\"validate\", help=\"Validate on-disk registries\")\n\n    # Profiles utilities\n    prof = sub.add_parser(\"profiles\", help=\"Profiles and overlays\")\n    prof_sub = prof.add_subparsers(dest=\"profiles_cmd\", required=True)\n    prof_validate = prof_sub.add_parser(\"validate\", help=\"Validate a profile YAML\")\n    prof_validate.add_argument(\"file\", type=Path)\n    prof_apply = prof_sub.add_parser(\"apply\", help=\"Apply a profile (stub)\")\n    prof_apply.add_argument(\"file\", type=Path)\n    prof_check = prof_sub.add_parser(\"check\", help=\"Check outputs against profile (stub)\")\n    prof_check.add_argument(\"file\", type=Path)\n\n    # Fetcher utilities\n    fet = sub.add_parser(\"fetcher\", help=\"Offline-first fetcher and simple scraper\")\n    fet_sub = fet.add_subparsers(dest=\"fetcher_cmd\", required=True)\n    fet_get = fet_sub.add_parser(\"get\", help=\"Fetch a URL with cache\")\n    fet_get.add_argument(\"url\", type=str)\n    fet_get.add_argument(\"--cache\", type=Path, default=Path(\".cache\"))\n    fet_get.add_argument(\"--offline\", action=\"store_true\")\n    fet_scrape = fet_sub.add_parser(\"scrape-table\", help=\"Extract first table rows from an HTML file\")\n    fet_scrape.add_argument(\"file\", type=Path)\n    fet_pin = fet_sub.add_parser(\"pin\", help=\"Return cache key for URL if cached\")\n    fet_pin.add_argument(\"url\", type=str)\n    fet_pin.add_argument(\"--cache\", type=Path, default=Path(\".cache\"))\n    fet_verify = fet_sub.add_parser(\"verify\", help=\"Verify a cache key for URL\")\n    fet_verify.add_argument(\"url\", type=str)\n    fet_verify.add_argument(\"key\", type=str)\n    fet_verify.add_argument(\"--cache\", type=Path, default=Path(\".cache\"))\n    fet_graphql = fet_sub.add_parser(\"graphql\", help=\"Fetch a GraphQL query with cache\")\n    fet_graphql.add_argument(\"url\", type=str)\n    fet_graphql.add_argument(\"query\", type=str)\n    fet_graphql.add_argument(\"--cache\", type=Path, default=Path(\".cache\"))\n    fet_graphql.add_argument(\"--offline\", action=\"store_true\")\n\n    args = parser.parse_args()\n\n    if args.command == \"run\":\n        _handle_run(args)\n    elif args.command == \"tracker\":\n        _handle_tracker(args)\n    elif args.command == \"registry\":\n        _handle_registry(args)\n    elif args.command == \"profiles\":\n        _handle_profiles(args)\n    elif args.command == \"fetcher\":\n        _handle_fetcher(args)\n</code></pre>"},{"location":"reference/core/#ufsa_v2.engine.emit_outputs","title":"<code>emit_outputs(schemes, out_dir, tracker)</code>","text":"<p>Emit per-scheme artifacts and global tables.</p> <p>Emits JSON/CSV per scheme, a consolidated JSON index, global tables (concepts, relations, schemes), identifier/mapping registries, and mapping candidate sets. All outputs are tracked.</p> Source code in <code>ufsa_v2/engine.py</code> <pre><code>def emit_outputs(\n    schemes: dict[str, ConceptScheme], out_dir: Path, tracker: Tracker\n) -&gt; list[str]:\n    \"\"\"Emit per-scheme artifacts and global tables.\n\n    Emits JSON/CSV per scheme, a consolidated JSON index, global tables\n    (concepts, relations, schemes), identifier/mapping registries, and mapping\n    candidate sets. All outputs are tracked.\n    \"\"\"\n    outputs: list[str] = []\n    outputs += _emit_per_scheme_artifacts(schemes, out_dir, tracker)\n    outputs += _emit_indexes(schemes, out_dir, tracker)\n    outputs += _emit_global_tables_and_registries(schemes, out_dir, tracker)\n    return outputs\n</code></pre>"},{"location":"reference/core/#ufsa_v2.engine.load_registry","title":"<code>load_registry(path)</code>","text":"<p>Load and optionally validate the pointer registry YAML.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Filesystem path to the registry YAML.</p> required <p>Returns:</p> Type Description <code>Registry</code> <p>Parsed <code>Registry</code> object containing standards to ingest.</p> Source code in <code>ufsa_v2/engine.py</code> <pre><code>def load_registry(path: Path) -&gt; Registry:\n    \"\"\"Load and optionally validate the pointer registry YAML.\n\n    Args:\n        path: Filesystem path to the registry YAML.\n\n    Returns:\n        Parsed ``Registry`` object containing standards to ingest.\n    \"\"\"\n    data = yaml.safe_load(path.read_text())\n    # Validate against schema if available\n    schema_path = Path(__file__).parent / \"registry\" / \"registry.schema.json\"\n    try:\n        if schema_path.exists():\n            import json as _json\n\n            schema = _json.loads(schema_path.read_text())\n            jsonschema_validate(instance=data, schema=schema)\n    except Exception:\n        logging.getLogger(__name__).debug(\n            \"Registry schema validation skipped due to error\", exc_info=True\n        )\n    standards = [\n        UFSAStandard(\n            standard_id=entry[\"standard_id\"],\n            name=entry[\"name\"],\n            governing_body=entry[\"governing_body\"],\n            specification_url=entry[\"specification_url\"],\n            data_format=entry[\"data_format\"],\n            parser_module=entry[\"parser_module\"],\n            concept_scheme_uri=entry[\"concept_scheme_uri\"],\n        )\n        for entry in data.get(\"standards\", [])\n    ]\n    return Registry(standards=standards)\n</code></pre>"},{"location":"reference/core/#ufsa_v2.engine.parse_standard","title":"<code>parse_standard(std, fixtures_dir, tracker)</code>","text":"<p>Import the declared parser module and parse a single standard.</p> <p>Parameters:</p> Name Type Description Default <code>std</code> <code>UFSAStandard</code> <p>Pointer registry entry.</p> required <code>fixtures_dir</code> <code>Path</code> <p>Directory where <code>fixtures://</code> URLs are resolved.</p> required <code>tracker</code> <code>Tracker</code> <p>Artifact tracker to record file dependencies.</p> required <p>Returns:</p> Type Description <code>ConceptScheme</code> <p>ConceptScheme parsed from the source specification.</p> Source code in <code>ufsa_v2/engine.py</code> <pre><code>def parse_standard(\n    std: UFSAStandard, fixtures_dir: Path, tracker: Tracker\n) -&gt; ConceptScheme:\n    \"\"\"Import the declared parser module and parse a single standard.\n\n    Args:\n        std: Pointer registry entry.\n        fixtures_dir: Directory where ``fixtures://`` URLs are resolved.\n        tracker: Artifact tracker to record file dependencies.\n\n    Returns:\n        ConceptScheme parsed from the source specification.\n    \"\"\"\n    # Resolve parser\n    module_path = std.parser_module\n    mod = importlib.import_module(module_path)\n    parser = cast(_ParserModule, mod)\n    # Prefer fixture file path for deterministic bootstrap\n    scheme = parser.parse(\n        standard_id=std.standard_id,\n        name=std.name,\n        governing_body=std.governing_body,\n        specification_url=std.specification_url,\n        concept_scheme_uri=std.concept_scheme_uri,\n        fixtures_dir=str(fixtures_dir),\n        tracker=tracker,\n    )\n    return scheme\n</code></pre>"},{"location":"reference/core/#ufsa_v2.engine.run_pipeline","title":"<code>run_pipeline(registry_path, out_dir, tracker)</code>","text":"<p>Run the full UFSA pipeline.</p> Steps <p>1) Load the pointer registry and parse each declared standard 2) Unify schemes and emit per-scheme and global artifacts 3) Generate naive mapping candidates</p> <p>Parameters:</p> Name Type Description Default <code>registry_path</code> <code>Path</code> <p>Path to the pointer registry YAML.</p> required <code>out_dir</code> <code>Path</code> <p>Output directory for artifacts.</p> required <code>tracker</code> <code>Tracker</code> <p>Tracker instance to record file dependencies and meta.</p> required <p>Returns:</p> Type Description <code>PipelineResult</code> <p><code>PipelineResult</code> with emitted artifact paths.</p> Source code in <code>ufsa_v2/engine.py</code> <pre><code>def run_pipeline(\n    registry_path: Path, out_dir: Path, tracker: Tracker\n) -&gt; PipelineResult:\n    \"\"\"Run the full UFSA pipeline.\n\n    Steps:\n      1) Load the pointer registry and parse each declared standard\n      2) Unify schemes and emit per-scheme and global artifacts\n      3) Generate naive mapping candidates\n\n    Args:\n        registry_path: Path to the pointer registry YAML.\n        out_dir: Output directory for artifacts.\n        tracker: Tracker instance to record file dependencies and meta.\n\n    Returns:\n        ``PipelineResult`` with emitted artifact paths.\n    \"\"\"\n    registry = load_registry(registry_path)\n    tracker.track_file(registry_path)\n\n    # Fixtures alongside registry\n    fixtures_dir = Path(\"data/fixtures\").resolve()\n    schemes: list[ConceptScheme] = []\n    for std in registry.standards:\n        scheme = parse_standard(std, fixtures_dir=fixtures_dir, tracker=tracker)\n        schemes.append(scheme)\n\n    unified = unify_schemes(schemes)\n    # Enrich tracker meta with scheme labels and counts\n    schemes_meta = tracker.meta.get(\"schemes\")\n    if not isinstance(schemes_meta, dict):\n        schemes_meta = {}\n    for sch_id, sch in unified.items():\n        schemes_meta[str(sch_id)] = {\n            \"label\": sch.label,\n            \"conceptCount\": len(sch.concepts),\n        }\n    tracker.meta[\"schemes\"] = schemes_meta\n    # Track key project files for drift prevention\n    for rel in (\"pyproject.toml\", \"uv.lock\", \"README.md\"):\n        p = Path(rel)\n        if p.exists():\n            tracker.track_file(p)\n    outputs = emit_outputs(unified, out_dir=out_dir, tracker=tracker)\n\n    # Generate naive cross-scheme candidate mappings by label equality\n    try:\n        mapping_emitter = importlib.import_module(\n            \"ufsa_v2.emitters.mapping_emitter\"\n        )\n        candidates = _generate_mapping_candidates(unified)\n        for gp in mapping_emitter.emit_candidate_mappings(candidates, out_dir):\n            tracker.track_file(Path(gp))\n            outputs.append(str(gp))\n        tracker.meta[\"mappingCandidates\"] = len(candidates)\n    except Exception:\n        logging.getLogger(__name__).debug(\n            \"Mapping candidate generation skipped due to error\", exc_info=True\n        )\n    return PipelineResult(outputs=outputs)\n</code></pre>"},{"location":"reference/core/#ufsa_v2.engine.unify_schemes","title":"<code>unify_schemes(schemes)</code>","text":"<p>Unify a list of schemes into a dict keyed by scheme id.</p> <p>This bootstrap keeps schemes separate; future iterations may merge or de-duplicate across inputs here.</p> Source code in <code>ufsa_v2/engine.py</code> <pre><code>def unify_schemes(schemes: list[ConceptScheme]) -&gt; dict[str, ConceptScheme]:\n    \"\"\"Unify a list of schemes into a dict keyed by scheme id.\n\n    This bootstrap keeps schemes separate; future iterations may merge or\n    de-duplicate across inputs here.\n    \"\"\"\n    # In this bootstrap, keep schemes separate by id; mappings can add cross-links later\n    index: dict[str, ConceptScheme] = {sch.id: sch for sch in schemes}\n    return index\n</code></pre>"},{"location":"reference/core/#ufsa_v2.emitters.tables_emitter.emit_global_tables","title":"<code>emit_global_tables(schemes, out_dir)</code>","text":"<p>Emit all global and specialized tables; return list of written paths.</p> Source code in <code>ufsa_v2/emitters/tables_emitter.py</code> <pre><code>def emit_global_tables(\n    schemes: dict[str, ConceptScheme], out_dir: Path\n) -&gt; list[str]:\n    \"\"\"Emit all global and specialized tables; return list of written paths.\"\"\"\n    out_paths: list[str] = []\n\n    # concept_schemes.csv\n    p_schemes = out_dir / \"concept_schemes.csv\"\n    _write_concept_schemes_csv(schemes, p_schemes)\n    out_paths.append(str(p_schemes))\n\n    # concepts.csv\n    p_concepts = out_dir / \"concepts.csv\"\n    _write_concepts_csv(schemes, p_concepts)\n    out_paths.append(str(p_concepts))\n\n    # semantic_relations.csv\n    p_rel = out_dir / \"semantic_relations.csv\"\n    _write_semantic_relations_csv(schemes, p_rel)\n    out_paths.append(str(p_rel))\n\n    # Specialized outputs\n    sw_path = _write_software_components(schemes, out_dir)\n    if sw_path:\n        out_paths.append(str(sw_path))\n\n    db_path = _write_database_schemas(schemes, out_dir)\n    if db_path:\n        out_paths.append(str(db_path))\n\n    return out_paths\n</code></pre>"},{"location":"reference/core/#ufsa_v2.parsers.parser_cyclonedx.parse","title":"<code>parse(*, standard_id, name, governing_body, specification_url, concept_scheme_uri, fixtures_dir, tracker)</code>","text":"<p>Parse a CycloneDX SBOM into a <code>ConceptScheme</code>.</p> <p>Expects a simplified JSON with keys: <code>components</code> (list) and <code>dependencies</code> (list). Each component should include at least <code>name</code> and <code>version</code>; <code>purl</code> and <code>description</code> are optional but recommended. Dependencies reference component <code>ref</code> keys which default to <code>purl</code> or <code>name@version</code>.</p> Source code in <code>ufsa_v2/parsers/parser_cyclonedx.py</code> <pre><code>def parse(\n    *,\n    standard_id: str,\n    name: str,\n    governing_body: str,\n    specification_url: str,\n    concept_scheme_uri: str,\n    fixtures_dir: str,\n    tracker: Tracker,\n):\n    \"\"\"Parse a CycloneDX SBOM into a ``ConceptScheme``.\n\n    Expects a simplified JSON with keys: ``components`` (list) and\n    ``dependencies`` (list). Each component should include at least ``name`` and\n    ``version``; ``purl`` and ``description`` are optional but recommended.\n    Dependencies reference component ``ref`` keys which default to ``purl`` or\n    ``name@version``.\n    \"\"\"\n    if not specification_url.startswith(\"fixtures://\"):\n        raise FixtureURLRequiredError()\n    fixture_rel = specification_url.replace(\"fixtures://\", \"\")\n    fixture_path = Path(fixtures_dir) / fixture_rel\n    tracker.track_file(fixture_path)\n\n    data: dict[str, Any] = json.loads(fixture_path.read_text())\n    scheme = ConceptScheme(id=standard_id, label=name)\n\n    # Build concepts\n    ref_to_concept_id: dict[str, str] = {}\n    for comp in data.get(\"components\", []) or []:\n        name_val = str(comp.get(\"name\", \"\")).strip()\n        version = str(comp.get(\"version\", \"\")).strip()\n        purl = str(comp.get(\"purl\", \"\")).strip()\n        description = str(comp.get(\"description\", \"\")).strip()\n        # optional enrichments\n        licenses = _collect_licenses(comp)\n        hashes = _collect_hashes(comp)\n        ext_refs = _collect_external_references(comp)\n\n        raw_ref = str(comp.get(\"bom-ref\", \"\")).strip()\n        # prefer bom-ref, else purl, else name@version\n        ref = (\n            raw_ref\n            or purl\n            or (f\"{name_val}@{version}\" if name_val or version else name_val)\n        )\n        concept_id = f\"{standard_id}:{ref or name_val or version}\"\n        c = Concept(\n            id=concept_id,\n            label=name_val or purl or ref,\n            notes={\n                \"notation\": version,\n                \"description\": description,\n                \"purl\": purl,\n                \"kind\": \"software_component\",\n                \"licenses\": (\n                    \";\".join([x for x in licenses if x]) if licenses else \"\"\n                ),\n                \"hashes\": \";\".join(hashes) if hashes else \"\",\n                \"externalReferences\": \";\".join(ext_refs) if ext_refs else \"\",\n            },\n        )\n        c.in_scheme = concept_scheme_uri\n        scheme.concepts[c.id] = c\n        if ref:\n            ref_to_concept_id[ref] = c.id\n        if purl:\n            ref_to_concept_id.setdefault(purl, c.id)\n        # also allow name@version lookup for dependencies lacking full purl\n        if name_val or version:\n            ref_to_concept_id.setdefault(f\"{name_val}@{version}\", c.id)\n\n    # Dependencies: link as skos:related\n    for dep in data.get(\"dependencies\", []) or []:\n        ref = str(dep.get(\"ref\", \"\")).strip()\n        src_id = ref_to_concept_id.get(ref)\n        if not src_id:\n            continue\n        for tgt in dep.get(\"dependsOn\", []) or []:\n            tgt_id = ref_to_concept_id.get(str(tgt))\n            if tgt_id and tgt_id not in scheme.concepts[src_id].related:\n                scheme.concepts[src_id].related.append(tgt_id)\n\n    return scheme\n</code></pre>"},{"location":"reference/core/#ufsa_v2.parsers.parser_ast_sql.parse","title":"<code>parse(*, standard_id, name, governing_body, specification_url, concept_scheme_uri, fixtures_dir, tracker)</code>","text":"<p>Parse SQL DDL into a <code>ConceptScheme</code> of tables and columns.</p> Source code in <code>ufsa_v2/parsers/parser_ast_sql.py</code> <pre><code>def parse(\n    *,\n    standard_id: str,\n    name: str,\n    governing_body: str,\n    specification_url: str,\n    concept_scheme_uri: str,\n    fixtures_dir: str,\n    tracker: Tracker,\n):\n    \"\"\"Parse SQL DDL into a ``ConceptScheme`` of tables and columns.\"\"\"\n    if not specification_url.startswith(\"fixtures://\"):\n        raise FixtureURLRequiredError()\n    fixture_rel = specification_url.replace(\"fixtures://\", \"\")\n    fixture_path = Path(fixtures_dir) / fixture_rel\n    tracker.track_file(fixture_path)\n\n    sql_text = fixture_path.read_text(encoding=\"utf-8\")\n\n    scheme = ConceptScheme(id=standard_id, label=name)\n    tables = _parse_sql_tables(sql_text)\n    for t in tables:\n        t_id = f\"{standard_id}:{t['table']}\"\n        t_con = Concept(id=t_id, label=t[\"table\"], notes={\"kind\": \"table\"})\n        t_con.in_scheme = concept_scheme_uri\n        scheme.concepts[t_id] = t_con\n        for col in t.get(\"columns\", []) or []:\n            c_id = f\"{standard_id}:{t['table']}.{col['name']}\"\n            c_con = Concept(\n                id=c_id,\n                label=f\"{t['table']}.{col['name']}\",\n                notes={\"data_type\": col.get(\"type\", \"\"), \"kind\": \"column\"},\n            )\n            c_con.in_scheme = concept_scheme_uri\n            # parent relation\n            c_con.broader.append(t_id)\n            t_con.narrower.append(c_id)\n            scheme.concepts[c_id] = c_con\n        # Emit relatedMatch for simple FKs: src_col related tgt_table.tgt_col\n        for fk in t.get(\"fkeys\", []) or []:\n            src_col_id = f\"{standard_id}:{t['table']}.{fk['src']}\"\n            tgt_col_id = f\"{standard_id}:{fk['tgt_table']}.{fk['tgt_col']}\"\n            if src_col_id in scheme.concepts and tgt_col_id in scheme.concepts:\n                src = scheme.concepts[src_col_id]\n                # Use related_match if model supports; else append to related\n                if hasattr(src, \"related_match\"):\n                    src.related_match.append(tgt_col_id)\n                else:\n                    src.related.append(tgt_col_id)\n\n    return scheme\n</code></pre>"},{"location":"reference/core/#ufsa_v2.parsers.rdf_parser.parse","title":"<code>parse(*, standard_id, name, governing_body, specification_url, concept_scheme_uri, fixtures_dir, tracker)</code>","text":"<p>Parse SKOS concepts and relations from an RDF file into a ConceptScheme.</p> Source code in <code>ufsa_v2/parsers/rdf_parser.py</code> <pre><code>def parse(\n    *,\n    standard_id: str,\n    name: str,\n    governing_body: str,\n    specification_url: str,\n    concept_scheme_uri: str,\n    fixtures_dir: str,\n    tracker: Tracker,\n) -&gt; ConceptScheme:\n    \"\"\"Parse SKOS concepts and relations from an RDF file into a ConceptScheme.\"\"\"\n    if not specification_url.startswith(\"fixtures://\"):\n        raise FixtureURLRequiredError()\n    fixture_rel = specification_url.replace(\"fixtures://\", \"\")\n    fixture_path = Path(fixtures_dir) / fixture_rel\n    tracker.track_file(fixture_path)\n\n    g = Graph()\n    g.parse(fixture_path)\n\n    scheme = ConceptScheme(id=standard_id, label=name)\n\n    # Collect SKOS Concepts with labels and enrich with SKOS properties\n    for s in g.subjects(RDF.type, SKOS.Concept):\n        pref_label = g.value(s, SKOS.prefLabel) or g.value(s, RDFS.label)\n        if not pref_label:\n            # If no label, skip to keep output meaningful\n            continue\n        cid = f\"{standard_id}:{s}\"\n        c = Concept(id=cid, label=str(pref_label))\n        c.in_scheme = concept_scheme_uri\n\n        # Notes: definition, notation; include altLabel for context if present\n        definition = g.value(s, SKOS.definition)\n        if definition:\n            c.notes[\"description\"] = str(definition)\n        # Collect all notations (join when multiple)\n        notations = [str(n) for n in g.objects(s, SKOS.notation)]\n        if notations:\n            c.notes[\"notation\"] = \";\".join(notations)\n        # altLabels (optional, semicolon-separated)\n        alt_labels = [str(al) for al in g.objects(s, SKOS.altLabel)]\n        if alt_labels:\n            c.notes[\"altLabel\"] = \";\".join(alt_labels)\n\n        scheme.concepts[c.id] = c\n\n    _wire_relations_and_mappings(g, standard_id, scheme)\n\n    if not scheme.concepts:\n        # fallback: add the scheme node itself\n        c = Concept(id=f\"{standard_id}:{concept_scheme_uri}\", label=name)\n        c.in_scheme = concept_scheme_uri\n        scheme.concepts[c.id] = c\n\n    return scheme\n</code></pre>"},{"location":"reference/models/","title":"Models Reference","text":""},{"location":"reference/models/#models","title":"Models","text":"<p>               Bases: <code>BaseModel</code></p> <p>A normalized concept within a scheme.</p> Source code in <code>ufsa_v2/pyd_models/concept.py</code> <pre><code>class Concept(BaseModel):\n    \"\"\"A normalized concept within a scheme.\"\"\"\n\n    id: str = Field(..., description=\"Stable concept identifier\")\n    pref_label: str = Field(..., description=\"Preferred label for the concept\")\n    definition: str | None = Field(None, description=\"Definition or note\")\n    notation: str | None = Field(None, description=\"Short code or notation, when applicable\")\n    scheme_uri: str | None = Field(None, description=\"URI of the scheme this concept belongs to\")\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>A SKOS-like concept scheme.</p> Source code in <code>ufsa_v2/pyd_models/concept.py</code> <pre><code>class ConceptScheme(BaseModel):\n    \"\"\"A SKOS-like concept scheme.\"\"\"\n\n    id: str = Field(..., description=\"Stable scheme identifier\")\n    label: str = Field(..., description=\"Human-readable scheme label\")\n    uri: str | None = Field(None, description=\"Canonical scheme URI, if known\")\n    governing_body: str | None = Field(None, description=\"Standards body or source organization\")\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>A semantic relation between two concepts (SKOS-like).</p> Source code in <code>ufsa_v2/pyd_models/relation.py</code> <pre><code>class Relation(BaseModel):\n    \"\"\"A semantic relation between two concepts (SKOS-like).\"\"\"\n\n    subject_id: str = Field(..., description=\"Concept ID of the subject\")\n    predicate: str = Field(..., description=\"Predicate IRI or compact term\")\n    object_id: str = Field(..., description=\"Concept ID of the object\")\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Registry entry for identifier systems (e.g., ISIN, CUSIP).</p> Source code in <code>ufsa_v2/pyd_models/identifiers.py</code> <pre><code>class IdentifierSystem(BaseModel):\n    \"\"\"Registry entry for identifier systems (e.g., ISIN, CUSIP).\"\"\"\n\n    id: str = Field(..., description=\"Identifier system ID\")\n    name: str = Field(..., description=\"System name\")\n    authority: str | None = Field(None, description=\"Governing authority\")\n    uri: str | None = Field(None, description=\"Canonical URI for the system\")\n    description: str | None = Field(None, description=\"Short description\")\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Curated mapping between identifier systems and/or concepts.</p> Source code in <code>ufsa_v2/pyd_models/mapping.py</code> <pre><code>class Mapping(BaseModel):\n    \"\"\"Curated mapping between identifier systems and/or concepts.\"\"\"\n\n    source: str = Field(..., description=\"Source concept or identifier system ID\")\n    target: str = Field(..., description=\"Target concept or identifier system ID\")\n    predicate: str = Field(..., description=\"Relation/predicate that describes the mapping\")\n    note: str | None = Field(None, description=\"Optional note or rationale\")\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Represents a software component from CycloneDX SBOM.</p> Source code in <code>ufsa_v2/pyd_models/sbom.py</code> <pre><code>class SoftwareComponent(BaseModel):\n    \"\"\"Represents a software component from CycloneDX SBOM.\"\"\"\n\n    id: str = Field(..., description=\"Stable ID (bom-ref or derived)\")\n    name: str = Field(..., description=\"Component name\")\n    version: str | None = Field(None, description=\"Version string\")\n    purl: str | None = Field(None, description=\"Package URL if available\")\n    licenses: list[str] = Field(default_factory=list, description=\"License identifiers\")\n    hashes: list[str] = Field(default_factory=list, description=\"Component hashes (e.g., SHA-256)\")\n    external_references: list[str] = Field(\n        default_factory=list,\n        description=\"External reference URLs (homepage, repo, docs)\",\n    )\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>A database table with columns and optional FKs.</p> Source code in <code>ufsa_v2/pyd_models/ast_sql.py</code> <pre><code>class DatabaseTable(BaseModel):\n    \"\"\"A database table with columns and optional FKs.\"\"\"\n\n    name: str = Field(..., description=\"Table name\")\n    columns: list[DatabaseColumn] = Field(default_factory=list, description=\"Columns in the table\")\n    foreign_keys: list[ForeignKey] = Field(default_factory=list, description=\"Foreign key constraints\")\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>A column within a database table.</p> Source code in <code>ufsa_v2/pyd_models/ast_sql.py</code> <pre><code>class DatabaseColumn(BaseModel):\n    \"\"\"A column within a database table.\"\"\"\n\n    name: str = Field(..., description=\"Column name\")\n    data_type: str | None = Field(None, description=\"Column data type, if known\")\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>A foreign key relationship between two tables/columns.</p> Source code in <code>ufsa_v2/pyd_models/ast_sql.py</code> <pre><code>class ForeignKey(BaseModel):\n    \"\"\"A foreign key relationship between two tables/columns.\"\"\"\n\n    table: str = Field(..., description=\"Referencing (child) table name\")\n    column: str = Field(..., description=\"Referencing (child) column name\")\n    ref_table: str = Field(..., description=\"Referenced (parent) table name\")\n    ref_column: str = Field(..., description=\"Referenced (parent) column name\")\n</code></pre>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/","title":"A Universal Federated Schema Architecture: Synthesis, Falsification, and Refinement","text":""},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#i-introduction-the-challenge-of-universal-data-interoperability","title":"I. Introduction: The Challenge of Universal Data Interoperability","text":""},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#preamble-the-babel-of-modern-data","title":"Preamble: The Babel of Modern Data","text":"<p>In the contemporary digital ecosystem, data serves as the foundational bedrock for commerce, science, and governance. Yet, despite decades of standardization efforts and the proliferation of sophisticated data management technologies, true, frictionless data interoperability remains an elusive goal. The global data landscape resembles a digital Tower of Babel, characterized by a fragmented collection of domain-specific \"data-fiefdoms.\" Fields such as finance, healthcare, e-commerce, and the Internet of Things (IoT) have each developed highly optimized, powerful, but mutually incompatible schemas and data models. This fragmentation is not a failure of design but a natural consequence of specialization; a healthcare record has fundamentally different requirements from a financial instrument identifier or a product catalog entry.</p> <p>However, this specialization creates immense friction at the boundaries where domains must interact. The processes of data mapping, migration, and integration are notoriously complex, costly, and manual, often relying on brittle spreadsheets and insufficient matching algorithms.1 One misstep in data mapping can cascade throughout an organization, leading to replicated errors, corrupted reporting, and flawed analysis.2 These challenges are exacerbated by the dynamic nature of modern data environments, where information on data assets and processes can become obsolete almost as soon as it is documented, and where the constant evolution of software systems introduces continuous risk.3 The result is a landscape of significant technical debt, operational risk, and unrealized value, where the potential of cross-domain data synergy is perpetually hampered by the high cost of translation.1</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#thesis-statement-a-framework-for-coexistence-not-conquest","title":"Thesis Statement: A Framework for Coexistence, Not Conquest","text":"<p>This report posits that the historical pursuit of a single, monolithic \"universal schema\"\u2014a single data language to govern all domains\u2014is a fallacy. Such an approach is inherently brittle, unable to accommodate the nuanced, context-dependent requirements of specialized fields, and fails to respect the deep domain expertise codified in existing standards. Instead, this document proposes a different path: a Universal Federated Schema Architecture (UFSA).</p> <p>The UFSA is a meta-framework designed not to replace domain-specific schemas but to enable their discovery, understanding, and interoperable use. It is an architecture of coexistence, not conquest. Its core strength lies in two foundational principles. First, it is federated, adopting a governance model that balances the need for central coordination of universal principles with the autonomy of individual data domains to define and manage their own schemas.4 This structure respects domain expertise and provides the flexibility required for innovation. Second, it is built upon a hierarchy of</p> <p>declarative registries. By defining schemas as static, machine-readable statements of \"what\" data represents, rather than \"how\" it is processed, the architecture prioritizes clarity, reduces ambiguity, and critically, lowers the extraneous cognitive load on developers and architects who must navigate these complex systems.6</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#methodology-a-dialectical-approach-to-architectural-design","title":"Methodology: A Dialectical Approach to Architectural Design","text":"<p>To ensure the resulting architecture is not an arbitrary academic exercise but a robust, practical, and defensible framework, this report employs a rigorous three-phase methodology. This dialectical approach\u2014thesis, antithesis, and synthesis\u2014is designed to produce a battle-hardened specification that has been validated against the most difficult and nuanced challenges of real-world data modeling.</p> <ol> <li>Phase 1: Synthesis. An initial architectural model, UFSA v1.0, is constructed. This model is not created in a vacuum but is synthesized from the core principles and proven patterns of a diverse set of successful, widely adopted standards from multiple domains. This phase establishes the foundational \"thesis.\"  </li> <li>Phase 2: Adversarial Challenge (Falsification). The synthesized UFSA v1.0 is subjected to a series of adversarial challenges. These challenges are not hypothetical but are drawn directly from complex, real-world scenarios documented in the research corpus, such as semantic ambiguity in addresses, political conflicts between competing standards, and the extreme structural complexity of genomic data. This phase represents the \"antithesis,\" where the goal is to systematically identify the breaking points, hidden assumptions, and practical limitations of the initial model.  </li> <li>Phase 3: Refinement. The failures and weaknesses identified during the falsification phase are systematically addressed through targeted architectural refinements. This leads to the development of the final, evolved specification, UFSA v2.0. This phase represents the final \"synthesis,\" yielding an architecture that is demonstrably more resilient, expressive, and practical than its predecessor.</li> </ol> <p>This structured methodology ensures that every component of the final architecture is present for a specific, validated reason, creating a clear and traceable line of reasoning from problem to solution.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#ii-phase-1-synthesis-of-a-foundational-architecture-ufsa-v10","title":"II. Phase 1: Synthesis of a Foundational Architecture (UFSA v1.0)","text":"<p>The initial formulation of the Universal Federated Schema Architecture, designated UFSA v1.0, is derived from a synthesis of established principles and successful patterns observed across a wide range of technical and organizational domains. This phase constructs a coherent baseline model by integrating best practices in governance, data modeling, and semantic definition.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#21-foundational-principles-declarative-definitions-and-federated-governance","title":"2.1. Foundational Principles: Declarative Definitions and Federated Governance","text":"<p>The two pillars upon which the UFSA is built are its declarative nature and its federated governance structure. These choices are deliberate, addressing fundamental challenges in system complexity and organizational scalability.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#the-declarative-paradigm","title":"The Declarative Paradigm","text":"<p>Programming and system definition paradigms can be broadly categorized as either imperative or declarative.8 An imperative approach specifies</p> <p>how to achieve a result through a sequence of explicit commands and state changes. A declarative approach, in contrast, specifies what the desired result is, leaving the implementation details of achieving that result to the underlying system.6 SQL is a classic example of a declarative language: a user declares the data they wish to retrieve, and the database engine determines the optimal execution plan to do so.9</p> <p>The UFSA adopts a purely declarative model for schema definition. All schemas, concepts, and identifier systems are defined as static, machine-readable documents (e.g., in a format like YAML or JSON Schema) that describe the desired structure and meaning of data. This approach offers several critical advantages. It separates the logical definition of a schema from its physical implementation, promoting portability and platform independence. Declarative definitions are generally more readable, concise, and less ambiguous than imperative code, making them easier to validate and reason about.10 Most importantly, this paradigm directly addresses the problem of cognitive load. In software engineering, extraneous cognitive load is the mental effort required to understand the presentation of information, as opposed to the inherent complexity of the problem itself.7 Deeply nested, imperative logic and complex abstractions create high extraneous cognitive load, making systems difficult to understand, maintain, and debug.11 By focusing on \"what\" rather than \"how,\" a declarative approach minimizes this extraneous load, allowing architects and developers to focus on the core business logic encoded in the schema.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#the-federated-governance-model","title":"The Federated Governance Model","text":"<p>Data governance models exist on a spectrum from fully centralized to fully decentralized.4 A centralized model offers strong consistency and control, which is often necessary in highly regulated industries like finance and healthcare, but can stifle agility and create bottlenecks.4 A decentralized model offers maximum flexibility and autonomy to local teams but risks creating data silos and inconsistencies that undermine interoperability.4</p> <p>The UFSA architecture is explicitly designed around a federated data governance model, which provides a hybrid structure that balances these competing needs.4 In this model, a central governing body is responsible for establishing global policies, standards, and a minimal set of universal, cross-domain concepts. However, the responsibility for defining, managing, and executing governance for domain-specific schemas is delegated to autonomous domain teams.5 This structure provides several key benefits: it allows for consistent global standards where necessary, leverages the deep subject matter expertise within each domain, and scales more effectively as an organization grows in complexity.4 The UFSA's hierarchy of registries is a direct implementation of this model, with a central body governing the core concepts and the framework itself, while domain experts are empowered to contribute and manage their specific schemas within that framework.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#22-a-tri-level-hierarchy-of-declarative-registries","title":"2.2. A Tri-Level Hierarchy of Declarative Registries","text":"<p>A careful analysis of existing standards reveals a natural, implicit hierarchy of abstraction. Different standards are designed to solve problems at different conceptual levels. For example, the W3C's Simple Knowledge Organization System (SKOS) is concerned with defining abstract, language-independent Concepts such as \"animal\" or \"love\".16 It defines what something is semantically. At a more concrete level, data models like the HL7 FHIR Patient resource, the Shopify Product object, or the OMA LwM2M Device object define specific Schemas\u2014structured sets of attributes and data types\u2014used to represent those concepts in a machine-readable format.18 They define how a concept is structured. Finally, at the instance level, identifier systems like ISO 3166-1 alpha-2 country codes or the Financial Instrument Global Identifier (FIGI) provide standardized ways to uniquely identify a specific instance of a concept, such as the country \"US\" or the specific Apple Inc. common stock.21 They define which specific instance is being referenced.</p> <p>These are not competing paradigms but complementary layers in a complete information model. A robust universal architecture must formalize this inherent separation of concerns. Therefore, UFSA v1.0 is structured as a tri-level hierarchy of distinct but interconnected declarative registries.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#221-the-concept-registry-the-what-a-semantic-foundation","title":"2.2.1. The Concept Registry (The \"What\"): A Semantic Foundation","text":"<p>The highest and most abstract level of the UFSA is the Concept Registry. Its purpose is to provide a stable, universal, and implementation-agnostic foundation of meaning. This registry is directly modeled on the principles and vocabulary of the W3C's Simple Knowledge Organization System (SKOS), a standard data model for thesauri, classification schemes, and other controlled vocabularies.16</p> <p>Within this registry, each entry is a Concept, which represents a unit of thought\u2014an idea or category\u2014independent of the terms used to label it.17 Key characteristics of the Concept Registry are:</p> <ul> <li>Unique Identification: Each Concept is identified by a persistent and globally unique Uniform Resource Identifier (URI), enabling it to be referenced unambiguously from any context.25  </li> <li>Multilingual Labeling: Concepts are described using lexical labels in multiple natural languages. SKOS provides properties to distinguish between a preferred label (skos:prefLabel), alternative labels (skos:altLabel), and hidden labels for search indexing (skos:hiddenLabel).17 This separates the abstract concept from its linguistic representation.  </li> <li>Semantic Relationships: The registry captures relationships between concepts. The primary SKOS relationships are hierarchical (skos:broader, skos:narrower) and associative (skos:related).23 This allows for the construction of rich knowledge graphs, for example, declaring that   core:City is a narrower concept than core:Country, and that core:Country is related to core:Currency.  </li> <li>Documentation: Concepts can be annotated with notes, such as definitions (skos:definition) or scope notes (skos:scopeNote), to provide human-readable context and clarification.25</li> </ul> <p>This registry forms the semantic backbone of the entire architecture, ensuring that when different schemas refer to core:Country, they are grounded in a single, shared definition of what a \"country\" is.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#222-the-schema-registry-the-how-a-structural-blueprint","title":"2.2.2. The Schema Registry (The \"How\"): A Structural Blueprint","text":"<p>The Schema Registry bridges the gap between abstract concepts and concrete data. It provides a directory of formal, machine-readable data structures that define how a concept from the Concept Registry is to be represented. The design of this registry draws inspiration from the composable, attribute-rich resource models found in standards like HL7 FHIR, the hierarchical object models of OMA LwM2M, and the extensive API object definitions of platforms like Shopify.18</p> <p>Each entry in this registry is a Schema. The key features of a schema definition are:</p> <ul> <li>Conceptual Grounding: Every Schema must formally declare a link to the Concept it represents in the Concept Registry. For example, a commerce:Product schema would link to a core:Product concept. This ensures that all data structures are semantically anchored.  </li> <li>Attribute Definition: A Schema is composed of a set of named Attributes. Each attribute has a defined data type (e.g., string, integer, boolean, or a reference to another Schema), cardinality (e.g., required, optional, list), and may have associated constraints (e.g., pattern matching, value range).  </li> <li>Composition over Inheritance: The UFSA model explicitly favors composition over deep inheritance hierarchies. This design choice is heavily influenced by the Entity-Component-System (ECS) architectural pattern, which is widely used in game development for its flexibility and ability to avoid the rigid and complex class hierarchies common in object-oriented programming.28 Instead of a   Product schema inheriting from a monolithic SellableItem base class, it would be composed of smaller, reusable component schemas like Shippable, Taxable, and Inventoriable. This approach reduces coupling, enhances modularity, and lowers the cognitive load required to understand the system, as developers can reason about smaller, independent components rather than a large, interconnected inheritance tree.7</li> </ul>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#223-the-identifier-registry-the-which-an-instance-level-directory","title":"2.2.3. The Identifier Registry (The \"Which\"): An Instance-Level Directory","text":"<p>The third and most concrete level of the hierarchy is the Identifier Registry. Its purpose is to catalog standardized systems for uniquely identifying specific instances of a concept. This registry's design is informed by an analysis of diverse, real-world identifier systems, including the ISO 3166-1 standard for country codes, IETF MIME Types for file formats, and the Financial Instrument Global Identifier (FIGI) for financial products.21</p> <p>Each entry in this registry is an IdentifierSystem. A definition for an identifier system includes:</p> <ul> <li>System Identification: A unique name for the system itself (e.g., iso-3166-1-alpha-2).  </li> <li>Governance: Metadata about the governing body or standards organization responsible for maintaining the system (e.g., ISO, IETF, OMG).  </li> <li>Conceptual Link: A mandatory link to the Concept in the Concept Registry that the system's identifiers refer to (e.g., iso-3166-1-alpha-2 identifies instances of core:Country).  </li> <li>Syntax and Validation: A formal definition of the structure of a valid identifier within the system, which can be expressed as a regular expression (e.g., ^[A-Z]{2}$ for alpha-2 codes) or other validation rules.  </li> <li>Scope and Granularity: A description of the system's scope. For example, the FIGI system is notable for its hierarchical granularity, providing identifiers for a security at the global, country, and individual exchange levels.22</li> </ul> <p>This registry allows the UFSA to not only define what a country is (core:Country) and how it can be represented (geo:Country schema) but also to formally recognize iso-3166-1-alpha-2 as a valid, governed system for identifying specific countries like \"US\" and \"DE\".</p> <p>To better inform the design of this registry, a comparative analysis of these influential identifier systems is warranted.</p> <p>Table 1: Comparative Analysis of Existing Identifier Systems</p> Feature ISO 3166-1 alpha-2 IETF MIME Types Financial Instrument Global Identifier (FIGI) Governance Model International Organization for Standardization (ISO) Maintenance Agency 21 Internet Engineering Task Force (IETF) via RFC process; registry maintained by IANA 30 Object Management Group (OMG) standard; Bloomberg L.P. as Registration Authority 22 Identifier Structure Fixed-length, 2-character alphabetic code 21 Hierarchical type/subtype string (e.g., application/json) 30 Fixed-length, 12-character alphanumeric code; semantically meaningless 22 Scope &amp; Granularity Countries, dependent territories, and special areas of geographical interest 21 Nature and format of documents, files, or bytes 30 All global asset classes; provides unique identifiers at multiple levels (share class, composite, trading venue) 22 Extensibility Formal process managed by the ISO 3166/MA 37 New types registered via the IETF standards process (RFCs) 30 Open submission process for new instruments not yet assigned a FIGI 22 Known Issues &amp; Nuances \"Imperfect implementations\" exist, particularly in ccTLDs.21 A large number of legacy and non-standard types are in use, requiring careful handling by browsers.30 Direct conflict with the existing ISIN standard, with significant commercial and political dimensions.38 <p>This analysis reveals that a robust identifier registry must capture metadata beyond just a name and syntax. It must formally document the system's governance, scope, and relationship to other standards to be truly useful in a universal context.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#23-the-synthesized-ufsa-v10-specification","title":"2.3. The Synthesized UFSA v1.0 Specification","text":"<p>To provide a concrete representation of the synthesized architecture, this section presents a formal specification for UFSA v1.0 using a declarative YAML-based syntax. The example traces the concept of a \"Country\" through all three registry levels.</p> <p>YAML</p> <p># UFSA v1.0 Specification Example</p> <p># --- 1. Concept Registry --- # Defines the abstract concept of a \"Country\". - kind: Concept   apiVersion: ufsa.org/v1.0   metadata:     uri: \"ufsa.org/concept/core/Country\"     name: \"Country\"     domain: \"core\"   spec:     labels:       - lang: \"en\"         prefLabel: \"Country\"         altLabel: \"Nation\"       - lang: \"de\"         prefLabel: \"Land\"         altLabel: \"Staat\"     documentation:       - lang: \"en\"         definition: \"A distinct territorial body or political entity, seen as a distinct entity in political geography.\"     relationships:       - type: \"related\"         target: \"ufsa.org/concept/core/Currency\"</p> <p># --- 2. Schema Registry --- # Defines a concrete data structure for representing a \"Country\". - kind: Schema   apiVersion: ufsa.org/v1.0   metadata:     name: \"Country\"     domain: \"geo\"     version: \"1.0.0\"     concept: \"ufsa.org/concept/core/Country\"   spec:     type: \"object\"     composition:       - \"geo:GeopoliticalEntity\" # Reusable component schema     attributes:       officialName:         type: \"string\"         cardinality: \"required\"         description: \"The official name of the country.\"       population:         type: \"integer\"         cardinality: \"optional\"         constraints:           - type: \"minValue\"             value: 0       governmentType:         type: \"string\"         cardinality: \"optional\"       capitalCity:         type: \"geo:City\" # Reference to another schema         cardinality: \"optional\"</p> <p># --- 3. Identifier Registry --- # Registers the ISO 3166-1 alpha-2 standard as a system for identifying \"Country\" instances. - kind: IdentifierSystem   apiVersion: ufsa.org/v1.0   metadata:     name: \"iso-3166-1-alpha-2\"     governingBody: \"ISO\"     concept: \"ufsa.org/concept/core/Country\"   spec:     description: \"The international standard for two-letter country codes.\"     syntax:       type: \"regex\"       pattern: \"^[A-Z]{2}$\"     examples:       - value: \"US\"         description: \"United States\"       - value: \"DE\"         description: \"Germany\"</p> <p>This v1.0 specification provides a clean, logically separated, and semantically grounded framework. It is built upon proven principles and represents a strong theoretical foundation. The next phase will test whether this clean theory can withstand the complexities of the real world.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#iii-phase-2-adversarial-challenge-and-falsification","title":"III. Phase 2: Adversarial Challenge and Falsification","text":"<p>The purpose of this phase is to move beyond the theoretical elegance of UFSA v1.0 and subject it to a rigorous adversarial challenge. By confronting the architecture with complex, real-world data modeling problems drawn from the research, its limitations, hidden assumptions, and breaking points can be systematically identified. This process of falsification is essential for driving the refinement of the architecture into a more robust and practical framework.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#31-challenge-semantic-ambiguity-and-contextual-variation","title":"3.1. Challenge: Semantic Ambiguity and Contextual Variation","text":"<p>A core assumption in UFSA v1.0 is that a single concept can be adequately represented by a single set of schemas. This assumption breaks down when a concept's meaning and structure are highly dependent on context.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#test-case-geodetic-vs-postal-address","title":"Test Case: Geodetic vs. Postal Address","text":"<p>The concept of an \"address\" is not monolithic. An address used to identify a physical location for emergency services (a spatial or geodetic address) is fundamentally different from an address used for mail delivery (a postal address).41 These two use cases are governed by different authorities (e.g., NENA for 9-1-1 vs. the USPS for mail), rely on different reference datasets, and have distinct validation and standardization rules.41 A geodetic address is defined by its (x,y) coordinates derived from spatial reference data, while a postal address is validated against a list of mailable addresses.41 The data components themselves can differ; a postal address might require a ZIP+4 code not used in geocoding, while a geodetic address requires coordinate precision irrelevant to mail delivery.44</p> <p>Falsification Attempt: UFSA v1.0 is forced into an untenable position when trying to model this reality.</p> <ol> <li>A Single, Compromised Schema: Creating a single Address schema would require making all fields from both contexts optional, leading to a bloated and confusing structure where it is unclear which fields are required for which use case. This violates the principle of clarity and increases cognitive load.  </li> <li>Two Disconnected Schemas: Creating two separate schemas, GeodeticAddress and PostalAddress, solves the structural problem but breaks the semantic link. They would both point to the same core:Address concept, but the architecture provides no mechanism to express their relationship as contextual variants of that concept. This forces a duplication of effort and loses the critical information that they are two sides of the same coin.    The v1.0 model fails because it lacks a mechanism to apply context-dependent rules, constraints, and structural modifications to a single base schema.</li> </ol>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#test-case-jurisdictional-divergence-in-legal-contracts","title":"Test Case: Jurisdictional Divergence in Legal Contracts","text":"<p>The structure, interpretation, and enforceability of a legal contract are profoundly influenced by its governing law and jurisdiction.45 A key distinction exists between common-law systems (e.g., US, UK) and civil-law systems (e.g., Germany, France).46 For example, the concept of \"consideration\"\u2014something of value exchanged between parties\u2014is a mandatory element for a contract to be enforceable in common-law jurisdictions. In civil-law jurisdictions, this concept is not required.46 Furthermore, international data transfer agreements, such as those using Standard Contractual Clauses (SCCs), impose different obligations on data importers and exporters based on the jurisdictions involved, affecting data handling and onward transfer rules.47</p> <p>Falsification Attempt: UFSA v1.0, with its universal schema definitions, cannot elegantly model a Contract that must adapt to these fundamental, jurisdiction-specific variations. A single Contract schema would have to make consideration an optional field, failing to enforce its mandatory nature in a common-law context. The architecture lacks a way to declare that \"if the jurisdiction attribute is 'US-NY', then the consideration attribute is required.\" This inability to express context-dependent business rules represents a critical failure in expressiveness.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#32-challenge-competing-standards-and-political-realities","title":"3.2. Challenge: Competing Standards and Political Realities","text":"<p>Standards are not purely technical artifacts; they are often socio-technical systems shaped by commercial interests, regulatory mandates, and political dynamics. An architecture designed for universal interoperability must be able to accommodate this messy reality rather than assume a world of purely rational, technical choices.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#test-case-the-figi-vs-isin-conflict","title":"Test Case: The FIGI vs. ISIN Conflict","text":"<p>The Financial Instrument Global Identifier (FIGI) and the International Securities Identification Number (ISIN) are two distinct, competing standards for identifying financial instruments.32 While they address the same conceptual domain, they differ significantly in scope, granularity, persistence, and governance.32 The conflict is not merely technical; it is a strategic and commercial battle. Critics accuse Bloomberg, the Registration Authority for FIGI, of using the standard as a strategy to control identity as an access point for their proprietary data and services, effectively creating a vendor lock-in mechanism.39 Regulators, such as the European Securities and Markets Authority (ESMA), have mandated the use of ISIN for certain reporting, a decision influenced by a preference for established ISO standards.38 This has led to a situation where the industry is faced with two parallel, non-interoperable standards, with adoption driven by a mix of regulatory pressure and commercial strategy, creating significant friction and cost.39</p> <p>Falsification Attempt: How does UFSA v1.0 model a single security, such as Apple Inc. common stock, that possesses both a FIGI and an ISIN? The v1.0 architecture can successfully register both FIGI and ISIN in the Identifier Registry as two separate IdentifierSystem entries, both linked to the core:FinancialInstrument concept. However, this is where its utility ends. The model provides no mechanism to formally declare the relationship between these two systems. It cannot state that they are competing or equivalent standards for the same purpose. It cannot specify that one might be preferred for regulatory reporting in Europe while the other is preferred for data integration with a specific vendor's platform. The architecture can acknowledge their existence but cannot manage their interaction or contextual relevance. This failure to model the plurality and politics of real-world standards is a significant shortcoming.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#33-challenge-extreme-structural-complexity-and-scale","title":"3.3. Challenge: Extreme Structural Complexity and Scale","text":"<p>While the compositional approach of UFSA v1.0 is designed for flexibility, it must be tested against domains with exceptionally complex, deeply nested, and highly attributed data structures to ensure it does not become unwieldy and counterproductive.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#test-case-gencode-gff3-genomic-annotation","title":"Test Case: GENCODE GFF3 Genomic Annotation","text":"<p>The GENCODE project provides comprehensive gene annotation for human and mouse genomes, distributed in formats like the General Feature Format Version 3 (GFF3).50 This data is characterized by several layers of complexity:</p> <ul> <li>Deep Hierarchy: Genomic features are naturally hierarchical: a gene contains one or more transcripts, which contain exons, which in turn contain coding sequences (CDS). This represents a deeply nested parent-child relationship.  </li> <li>Rich, Conditional Attributes: Each feature is described by a large number of attributes, many of which are conditional. For example, a CDS feature has a phase attribute that is meaningless for an exon feature.  </li> <li>Multiple Overlapping Views: The GENCODE dataset is provided in multiple \"views\" or subsets. For example, there is a \"comprehensive\" annotation set and a \"basic\" set, which includes only a curated subset of transcripts.52 There are also different regional scopes, such as annotations on primary chromosomes only (   CHR) versus all sequence regions (ALL).50</li> </ul> <p>Falsification Attempt: Can the UFSA v1.0 schema definition language represent this structure without imposing an extreme cognitive load? While the compositional model can represent the hierarchy by having a Gene schema with an attribute that is a list of Transcript schemas, the sheer number of attributes and their conditional nature would lead to an explosion in schema complexity. The simple key-value attribute definition in v1.0 lacks the expressive power to define complex interdependencies between attributes. The resulting schema definition would either be incredibly verbose, listing every possible attribute for every feature type, or it would require a proliferation of dozens of highly specific, shallow schemas. Both outcomes violate the core principle of reducing extraneous cognitive load, as a developer would have to navigate a labyrinth of definitions to understand the complete model.7 The v1.0 language is too simplistic for this level of domain complexity.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#34-challenge-operational-viability-and-data-mapping","title":"3.4. Challenge: Operational Viability and Data Mapping","text":"<p>A schema architecture is only useful if it can be practically implemented and if it aids in solving real-world data management problems. The ultimate operational test is whether it simplifies the process of data mapping and migration.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#test-case-legacy-system-migration","title":"Test Case: Legacy System Migration","text":"<p>Data mapping is a critical but challenging process in any data migration, integration, or M\\&amp;A activity.1 The core challenge lies in reconciling systems with different data models, inconsistent data quality, and unstated business semantics.1 For example, one system might store a carrier in a flat structure, while another uses a hierarchical parent-child model; mapping between these requires more than simple field-to-field matching.1 This work is often manual, time-consuming, and highly dependent on domain expertise, making it a major source of project failure and technical debt.1</p> <p>Falsification Attempt: Does UFSA v1.0 provide tangible help in this process? The architecture provides a well-defined, declarative target for a data migration. This is valuable, but it does not offer any explicit features to facilitate the process of mapping from a messy source system to this clean target. It is an endpoint, not a toolkit for the journey. The v1.0 specification contains no constructs for defining mapping rules, documenting data lineage from a source system, or specifying transformation logic. It assumes the difficult work of mapping happens outside the architecture, thereby failing to address one of the most significant sources of friction in achieving data interoperability.</p> <p>The results of these adversarial challenges demonstrate that while UFSA v1.0 provides a sound conceptual foundation, it is too rigid and simplistic to handle the contextual variation, political complexity, structural depth, and operational demands of real-world data ecosystems.</p> <p>Table 2: Adversarial Challenge and Falsification Matrix</p> Challenge Category Specific Test Case Source Snippets UFSA v1.0 Component Under Test Predicted Failure Mode Falsification Result Semantic Ambiguity Geodetic vs. Postal Address 41 Schema Registry, Concept Registry Inability to model contextual variations of a single concept without schema duplication or creating a single, compromised schema. Confirmed: v1.0 lacks a mechanism for contextual overlays or profiles to apply use-case-specific constraints. Jurisdictional Variation Common Law vs. Civil Law Contracts 45 Schema Registry Incapable of expressing conditional attributes or constraints based on the value of another field (e.g., jurisdiction). Confirmed: The schema language lacks the expressiveness for context-dependent business rules. Competing Standards FIGI vs. ISIN Conflict 32 Identifier Registry Can register both systems but cannot formally model their relationship (e.g., equivalence, preference in a context). Confirmed: v1.0 has no mechanism for defining mappings or relationships between different IdentifierSystem entries. Structural Complexity GENCODE GFF3 Genomic Annotation 50 Schema Registry The simple schema language leads to either extreme verbosity or a proliferation of shallow schemas, increasing cognitive load. Confirmed: The language lacks advanced constructs for conditional attributes and complex type composition. Operational Viability Legacy System Data Mapping 1 Entire Architecture Provides a target model but offers no tools or constructs to aid in the complex process of mapping and transformation. Confirmed: The architecture is an endpoint, not a toolkit. It lacks features for documenting lineage or mapping rules."},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#iv-phase-3-refinement-and-the-final-architecture-ufsa-v20","title":"IV. Phase 3: Refinement and the Final Architecture (UFSA v2.0)","text":"<p>The falsification phase revealed critical weaknesses in the initial UFSA v1.0 specification, demonstrating its insufficiency in handling real-world complexity. This third and final phase addresses these identified failures directly, evolving the architecture into a more robust, expressive, and practical model: UFSA v2.0. Each refinement is a direct response to a specific challenge, ensuring that the final architecture is not just theoretically sound but pragmatically resilient.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#41-architectural-refinements-for-context-and-conflict","title":"4.1. Architectural Refinements for Context and Conflict","text":"<p>The challenges of modeling contextual variations in addresses and legal contracts, as well as managing the conflict between competing standards like FIGI and ISIN, highlighted the need for the architecture to formally manage context and plurality.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#solution-introducing-profiles-and-contextual-overlays","title":"Solution: Introducing Profiles and Contextual Overlays","text":"<p>To resolve the inability of UFSA v1.0 to handle contextual variations, the v2.0 architecture introduces the concept of a Profile. This mechanism is directly inspired by the profiling concept in HL7 FHIR, where a base resource is constrained or extended to meet the requirements of a specific use case or implementation guide.18</p> <p>A Profile in UFSA v2.0 is a declarative overlay document that is applied to a base Schema. It does not redefine the schema but rather specifies a set of modifications, such as:</p> <ul> <li>Constraining Cardinality: Changing an optional attribute to be mandatory (e.g., making postalCode required for a postal address).  </li> <li>Adding Validation Rules: Applying new constraints to an attribute (e.g., requiring a postal code to be validated against a specific postal service's database).  </li> <li>Extending with New Attributes: Adding new attributes that are only relevant within that specific context (e.g., adding a consideration attribute to a contract schema when used in a common-law jurisdiction).</li> </ul> <p>This approach allows a single, stable base Schema for Address or Contract to be defined, capturing the core, universal attributes. Then, specific Profiles like PostalProfile, GeodeticProfile, CommonLawProfile, or CivilLawProfile can be applied to tailor the schema for its intended use. This preserves the semantic link while allowing for precise, context-aware data modeling, resolving the failures identified in the falsification tests.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#solution-formalizing-standard-mappings","title":"Solution: Formalizing Standard Mappings","text":"<p>To address the challenge of competing standards like FIGI and ISIN, UFSA v2.0 enhances the Identifier Registry with a new object type: the Mapping. This allows the registry to move beyond being a simple catalog and become a tool for managing the complex relationships between identifier systems.</p> <p>A Mapping object is a declarative statement that defines a relationship between two or more IdentifierSystem entries. The mapping specifies the type of relationship, which could include:</p> <ul> <li>equivalentTo: Indicates that identifiers from two systems can be used interchangeably for a specific purpose (e.g., mapping an internal product SKU to a global EAN).  </li> <li>broaderThan / narrowerThan: Defines hierarchical relationships, useful when one system is a superset of another.  </li> <li>preferredInContext(context): A powerful construct that allows the declaration of a preferred identifier system for a specific context, such as preferredInContext(EU-Regulatory-Reporting) for ISIN, or preferredInContext(Bloomberg-Terminal-Integration) for FIGI.</li> </ul> <p>By formalizing these relationships, the architecture acknowledges and manages the reality of a pluralistic standards landscape. It provides a machine-readable way to navigate these conflicts, allowing systems to make intelligent choices about which identifier to use in a given operational scenario.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#42-strategies-for-managing-complexity-and-cognitive-load","title":"4.2. Strategies for Managing Complexity and Cognitive Load","text":"<p>The GENCODE GFF3 challenge demonstrated that a simplistic schema language can fail when faced with extreme domain complexity, leading to solutions that are verbose and difficult to comprehend, thereby increasing cognitive load. The goal of a good abstraction is to hide complexity, not merely to add another layer of indirection.11 A shallow abstraction, where the interface is nearly as complex as the implementation it hides, provides little value and increases the mental effort required to understand a system.7 The refinement of the UFSA schema language is therefore focused on providing more powerful, \"deep\" abstractions that can manage complexity more effectively.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#solution-enhanced-schema-definition-language","title":"Solution: Enhanced Schema Definition Language","text":"<p>To handle the structural complexity of domains like genomics, the UFSA v2.0 schema definition language is extended with several powerful new constructs:</p> <ul> <li>Conditional Attributes: The language now supports declaring that the presence, cardinality, or constraints of an attribute are conditional upon the value of another attribute within the same schema. For example, in a genomic feature schema, one could declare that the phase attribute is required if the featureType attribute has the value CDS, and is disallowed otherwise. This allows for the creation of a single, coherent schema for a complex entity while enforcing intricate internal business rules.  </li> <li>Complex Type Composition: To move beyond simple lists of components, the language now formalizes the use of logical operators (allOf, anyOf, oneOf, not) for schema composition. This allows for the creation of highly expressive and precise type definitions. For instance, a PaymentMethod schema could be defined as oneOf, ensuring that an instance must conform to exactly one of those structures.  </li> <li>Attribute Groups: To combat verbosity and promote reuse, the language introduces AttributeGroups. This allows a set of commonly co-occurring attributes to be defined once as a named group and then included in multiple schemas. For example, an AuditTrail group containing createdAt, createdBy, updatedAt, and updatedBy attributes could be defined once and then composed into dozens of other schemas, reducing duplication and improving consistency.</li> </ul> <p>These enhancements provide the expressive power needed to model complex domains like GENCODE concisely, creating deeper abstractions that effectively manage the inherent complexity of the domain without overwhelming the developer with extraneous cognitive load.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#43-the-refined-ufsa-v20-specification","title":"4.3. The Refined UFSA v2.0 Specification","text":"<p>The following specification demonstrates the capabilities of the refined UFSA v2.0 architecture. It includes examples that directly address the falsification challenges from Phase 2, showcasing the use of Profiles, Mappings, and the enhanced schema language.</p> <p>YAML</p> <p># UFSA v2.0 Specification Example</p> <p># --- 1. Schema Registry (Base Schema) --- # A single, stable base schema for an \"Address\". - kind: Schema   apiVersion: ufsa.org/v2.0   metadata:     name: \"Address\"     domain: \"core\"     version: \"1.0.0\"     concept: \"ufsa.org/concept/core/Address\"   spec:     type: \"object\"     attributes:       streetLine1:         type: \"string\"         cardinality: \"required\"       city:         type: \"string\"         cardinality: \"required\"       postalCode:         type: \"string\"         cardinality: \"optional\"       coordinates:         type: \"core:Coordinates\" # Geo-coordinates schema         cardinality: \"optional\"</p> <p># --- 2. Schema Registry (Profile for Postal Address) --- # A Profile that constrains the base Address schema for mail delivery. - kind: Profile   apiVersion: ufsa.org/v2.0   metadata:     name: \"PostalAddressProfile\"     domain: \"core\"     targetSchema: \"core:Address\"   spec:     constraints:       - path: \"postalCode\"         cardinality: \"required\"         validation:           - type: \"external\"             authority: \"usps.com/validate\"</p> <p># --- 3. Identifier Registry (Systems and Mapping) --- # Registering both FIGI and ISIN and defining their relationship. - kind: IdentifierSystem   apiVersion: ufsa.org/v2.0   metadata:     name: \"figi\"     governingBody: \"OMG\"     concept: \"ufsa.org/concept/finance/FinancialInstrument\"   spec:     syntax: { type: \"regex\", pattern: \"^BBG[A-Z0-9]{9}$\" }</p> <p>- kind: IdentifierSystem   apiVersion: ufsa.org/v2.0   metadata:     name: \"isin\"     governingBody: \"ISO\"     concept: \"ufsa.org/concept/finance/FinancialInstrument\"   spec:     syntax: { type: \"regex\", pattern: \"^[A-Z]{2}[A-Z0-9]{9}[0-9]$\" }</p> <p>- kind: Mapping   apiVersion: ufsa.org/v2.0   metadata:     name: \"figi-isin-regulatory-mapping\"     domain: \"finance\"   spec:     type: \"contextualPreference\"     systems:       - \"isin\"       - \"figi\"     rules:       - context: \"EU-MIFID2-Reporting\"         preferred: \"isin\"       - context: \"Internal-Data-Integration\"         preferred: \"figi\"</p> <p>This refined specification demonstrates a system that is not only logically sound but also flexible and expressive enough to handle the nuances of real-world data modeling. The evolution from v1.0 to v2.0, driven by a rigorous falsification process, has resulted in a far more capable and practical architecture.</p> <p>Table 3: Architectural Evolution from UFSA v1.0 to v2.0</p> Weakness Identified in Phase 2 Refinement/Feature in UFSA v2.0 Inability to model context-specific variations of a concept without schema duplication. Introduction of Profiles and Contextual Overlays to apply declarative constraints and extensions to base schemas.18 Incapable of expressing conditional business rules within a schema (e.g., for legal contracts). Enhancement of the schema language to support Conditional Attributes based on the values of other fields. Cannot formally model the relationships between competing or equivalent identifier standards. Addition of the Mapping object type to the Identifier Registry to define relationships like equivalentTo and preferredInContext. Simplistic schema language leads to high cognitive load when modeling complex, nested data. Enhancement of the schema language with Complex Type Composition (allOf, oneOf) and reusable AttributeGroups. Provides a target model but no constructs to aid in the data mapping and migration process. (Addressed in Implementation) The declarative nature of Schemas and Profiles enables the creation of a Data Mapping Toolkit."},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#v-practical-implementation-and-governance","title":"V. Practical Implementation and Governance","text":"<p>An architecture, no matter how well-designed, is only valuable if it can be practically implemented and sustainably governed. This section transitions from the theoretical specification of UFSA v2.0 to a high-level blueprint for its realization as a living, operational ecosystem.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#51-a-reference-implementation-blueprint","title":"5.1. A Reference Implementation Blueprint","text":"<p>A successful rollout of a UFSA-based ecosystem would require a set of core services and supporting tools that enable developers and data stewards to interact with the registries and leverage the schemas.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#core-services","title":"Core Services","text":"<p>A microservices-based reference implementation would consist of three primary services, each corresponding to a level of the registry hierarchy:</p> <ul> <li>Concept Registry Service: Provides a stable, queryable API (e.g., GraphQL or RESTful) for discovering concepts, their labels, documentation, and semantic relationships. This service would act as the central source of truth for meaning.  </li> <li>Schema Registry Service: An API-driven service for creating, retrieving, updating, and validating Schemas and Profiles. It would be responsible for managing versions and ensuring that all schemas are linked to valid concepts.  </li> <li>Identifier Registry Service: An API for managing IdentifierSystem and Mapping definitions. This service would allow applications to query for valid identifier systems for a given concept and understand their relationships.</li> </ul>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#tooling","title":"Tooling","text":"<p>To lower the barrier to adoption and ensure consistency, a suite of supporting tools is essential:</p> <ul> <li>Schema Validation Engine: A critical component that can take a data instance (e.g., a JSON document), a target Schema, and an applicable Profile, and validate the instance against the combined set of rules. This engine would be the core of data quality enforcement.  </li> <li>Client Libraries: A set of libraries for major programming languages (e.g., Python, Java, JavaScript) that abstract away the direct API calls to the registry services, making it easier for developers to integrate UFSA into their applications.  </li> <li>Data Mapping Toolkit: Directly addressing the operational viability challenge identified in Phase 2, this toolkit would leverage the declarative nature of the UFSA. It would include:  </li> <li>A Mapping Assistant tool that uses metadata analysis to suggest potential attribute-to-attribute mappings between a legacy source schema and a target UFSA schema.  </li> <li>A Data Transformer service that can be configured using a declarative mapping document, allowing developers to define transformations without writing extensive imperative code, thus simplifying the ETL process.57</li> </ul>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#52-a-governance-framework-for-a-living-standard","title":"5.2. A Governance Framework for a Living Standard","text":"<p>A universal standard cannot be a static document; it must be a living system capable of evolving to accommodate new domains, new technologies, and new challenges. Without a formal governance process, the standard risks becoming obsolete or fracturing into incompatible dialects.</p> <p>The Python Enhancement Proposal (PEP) process provides a highly successful, time-tested model for managing the evolution of a complex technical standard in a community-driven yet structured manner.58 It defines clear proposal types, statuses, and a transparent review process that balances community input with decisive leadership from a steering council.58</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#the-ufsa-enhancement-proposal-uep-process","title":"The UFSA Enhancement Proposal (UEP) Process","text":"<p>Inspired by the PEP model, the UFSA would be governed by a formal UFSA Enhancement Proposal (UEP) process. This process would provide a structured pathway for proposing, debating, and ratifying changes to the architecture and its registered content. Key elements would include:</p> <ul> <li>Proposal Types: Formal UEP types would be established, such as:  </li> <li>Concept UEP: For proposing new core concepts or modifying existing ones.  </li> <li>Schema UEP: For proposing new universal component schemas (e.g., a standard AuditTrail schema).  </li> <li>Process UEP: For proposing changes to the governance process itself.  </li> <li>Proposal Statuses: Each UEP would have a clear status, such as Draft, Under Review, Accepted, Provisional, Final, or Rejected, providing transparency into the decision-making lifecycle.58  </li> <li>Governance Council: A central governance council, composed of representatives from key stakeholder domains, would be responsible for the final review and acceptance or rejection of UEPs.</li> </ul> <p>This formal process ensures that the UFSA can evolve in a stable, transparent, and orderly fashion, preserving its integrity as a universal standard while allowing it to adapt to the future needs of the global data community.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#vi-conclusion-a-declarative-framework-for-federated-interoperability","title":"VI. Conclusion: A Declarative Framework for Federated Interoperability","text":""},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#summary-of-ufsa-v20","title":"Summary of UFSA v2.0","text":"<p>This report has detailed the synthesis, adversarial challenge, and refinement of a Universal Federated Schema Architecture. The final specification, UFSA v2.0, is a multi-layered framework designed to facilitate data interoperability not by imposing a single, monolithic schema, but by providing a structured system for defining, discovering, and relating disparate data models. Its core architectural features\u2014the tri-level hierarchy of Concept, Schema, and Identifier registries; the use of declarative Profiles for contextual adaptation; and a compositional schema language designed to manage complexity\u2014collectively address the primary challenges of semantic ambiguity, contextual variation, competing standards, and operational friction that plague modern data ecosystems. The architecture is grounded in the principles of federated governance, balancing central standardization with domain-specific autonomy, and is designed to minimize extraneous cognitive load, a critical factor in the practical usability of any complex system.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#potential-impact-and-future-work","title":"Potential Impact and Future Work","text":"<p>The potential impact of a widely adopted UFSA is significant. By providing a common ground for semantic definition and structural description, it can dramatically reduce the friction and cost associated with data integration, migration, and cross-domain analysis. For organizations, this translates to faster project timelines, improved data quality, and the ability to unlock value from previously siloed data assets. For the broader digital ecosystem, it offers a path toward more seamless interoperability between industries, from enabling integrated patient-and-financial records to creating more coherent data flows between IoT devices and enterprise systems.</p> <p>Future work on this architecture could proceed along several promising avenues. The application of formal methods and automated theorem proving to the Schema and Profile definitions could enable mathematically verifiable guarantees about data consistency and transformation correctness. Further research into the integration of machine learning algorithms could lead to highly automated tools for schema discovery, profile generation, and the inference of mapping rules from data instances, further reducing the manual burden of data management. Finally, extending the governance model to include mechanisms for decentralized trust and verifiable credentials could enhance the security and verifiability of schema contributions in a large-scale, multi-organizational deployment.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#final-statement","title":"Final Statement","text":"<p>The journey to create the Universal Federated Schema Architecture represents a fundamental philosophical shift. It is a move away from the quixotic search for a single, universal data language\u2014a digital Esperanto destined for failure in a world of deep specialization. Instead, it embraces the reality of a diverse, multilingual data world and focuses on building a universal framework for translation, understanding, and mediation. The UFSA is not the language itself, but the Rosetta Stone that allows different data languages to be understood in relation to one another, grounded in a shared foundation of meaning. It is through this framework of federated coexistence, rather than centralized conquest, that the promise of true, global data interoperability can finally be realized.</p>"},{"location":"research/0_Federated%20Schema%20Architecture_%20Falsification%20%26%20Implementation/#works-cited","title":"Works cited","text":"<ol> <li>Data Mapping Between Systems: Keys, Values, Relationships (with P\\&amp;C Insurance Examples) - RecordLinker, accessed on August 24, 2025, https://recordlinker.com/data-mapping-guide/ </li> <li>What is Data Mapping? Definition and Examples | Talend, accessed on August 24, 2025, https://www.talend.com/resources/data-mapping/ </li> <li>Challenges and Solutions of Data Mapping Explained - Securiti Education, accessed on August 24, 2025, https://education.securiti.ai/certifications/privacyops/data-mapping/data-mapping-challenges/ </li> <li>Understand Data Governance Models: Centralized, Decentralized &amp; Federated | Alation, accessed on August 24, 2025, https://www.alation.com/blog/understand-data-governance-models-centralized-decentralized-federated/ </li> <li>Federated Data Governance Explained - Alation, accessed on August 24, 2025, https://www.alation.com/blog/federated-data-governance-explained/ </li> <li>Declarative vs. Imperative Programming: 4 Key Differences | Codefresh, accessed on August 24, 2025, https://codefresh.io/learn/infrastructure-as-code/declarative-vs-imperative-programming-4-key-differences/ </li> <li>Cognitive Load is what matters - GitHub, accessed on August 24, 2025, https://github.com/zakirullin/cognitive-load </li> <li>The Data Engineers Guide to Declarative vs Imperative for Data - DataOps.live, accessed on August 24, 2025, https://www.dataops.live/blog/the-data-engineers-guide-to-declarative-vs-imperative-for-data </li> <li>What are some examples of imperative vs. declarative programming? - Quora, accessed on August 24, 2025, https://www.quora.com/What-are-some-examples-of-imperative-vs-declarative-programming </li> <li>Explained: Imperative vs Declarative programming - DEV Community, accessed on August 24, 2025, https://dev.to/siddharthshyniben/explained-imperative-vs-declarative-programming-577g </li> <li>That's not an abstraction, that's just a layer of indirection - fhur, accessed on August 24, 2025, https://fhur.me/posts/2024/thats-not-an-abstraction </li> <li>The Cognitive Load Theory in Software Development - The Valuable Dev, accessed on August 24, 2025, https://thevaluable.dev/cognitive-load-theory-software-developer/ </li> <li>www.alation.com, accessed on August 24, 2025, https://www.alation.com/blog/federated-data-governance-explained/#:\\~:text=Federated%20data%20governance%20is%20a,governance%20principles%20with%20decentralized%20execution. </li> <li>Federated Data Governance: Ultimate Guide for 2024 - Atlan, accessed on August 24, 2025, https://atlan.com/know/data-governance/federated-data-governance/ </li> <li>Federated Data Governance Explained - Actian Corporation, accessed on August 24, 2025, https://www.actian.com/blog/data-governance/federated-data-governance-explained/ </li> <li>Simple Knowledge Organization System - Wikipedia, accessed on August 24, 2025, https://en.wikipedia.org/wiki/Simple_Knowledge_Organization_System </li> <li>SKOS Simple Knowledge Organization System Primer - W3C, accessed on August 24, 2025, https://www.w3.org/TR/skos-primer/ </li> <li>Introduction to FHIR Resources - HealthIT.gov, accessed on August 24, 2025, https://www.healthit.gov/sites/default/files/page/2021-04/Intro%20to%20FHIR%20Resources%20Fact%20Sheet.pdf </li> <li>Lightweight M2M (LWM2M) - Zephyr Project Documentation, accessed on August 24, 2025, https://docs.zephyrproject.org/latest/connectivity/networking/api/lwm2m.html </li> <li>Product - Storefront API - Shopify developer documentation, accessed on August 24, 2025, https://shopify.dev/docs/api/storefront/latest/objects/Product </li> <li>ISO 3166-1 alpha-2 - Wikipedia, accessed on August 24, 2025, https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2 </li> <li>Financial Instrument Global Identifier - Wikipedia, accessed on August 24, 2025, https://en.wikipedia.org/wiki/Financial_Instrument_Global_Identifier </li> <li>What's SKOS, What's not, Why and What Should be Done About It - NKOS, accessed on August 24, 2025, https://nkos.dublincore.org/ASIST2015/ASISTBusch-SKOS.pdf </li> <li>SKOS Core Vocabulary Specification - W3C, accessed on August 24, 2025, https://www.w3.org/TR/swbp-skos-core-spec/ </li> <li>SKOS Simple Knowledge Organization System Reference - W3C, accessed on August 24, 2025, https://www.w3.org/TR/2008/WD-skos-reference-20080125/ </li> <li>2. OMA LwM2M - Brief description \u2014 Anjay 3.10.0 documentation, accessed on August 24, 2025, https://avsystem.github.io/Anjay-doc/LwM2M.html </li> <li>Order - Shopify developer documentation, accessed on August 24, 2025, https://shopify.dev/docs/api/admin-rest/latest/resources/order </li> <li>Entity component system - Wikipedia, accessed on August 24, 2025, https://en.wikipedia.org/wiki/Entity_component_system </li> <li>Death by design patterns, or On the cognitive load of abstractions in the code - Hacker News, accessed on August 24, 2025, https://news.ycombinator.com/item?id=36118093 </li> <li>Media types (MIME types) - MDN, accessed on August 24, 2025, https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/MIME_types </li> <li>Overview | OpenFIGI, accessed on August 24, 2025, https://www.openfigi.com/about/overview </li> <li>How FIGI relates to other standards in the space - OMG Issue Tracker, accessed on August 24, 2025, https://issues.omg.org/issues/FIGI-20 </li> <li>ISO 3166-1 - Wikipedia, accessed on August 24, 2025, https://en.wikipedia.org/wiki/ISO_3166-1 </li> <li>Media Types - Internet Assigned Numbers Authority, accessed on August 24, 2025, https://www.iana.org/assignments/media-types/ </li> <li>FINANCIAL INSTRUMENT GLOBAL IDENTIFIER \u2122 - Bloomberg, accessed on August 24, 2025, https://assets.bwbx.io/documents/users/iqjWHBFdfxIU/rKmKGovTFMFo/v0 </li> <li>Crypto FIGI - Kaiko, accessed on August 24, 2025, https://www.kaiko.com/resources/crypto-figi </li> <li>List of all countries with their 2 digit codes (ISO 3166-1) - DataHub.io, accessed on August 24, 2025, https://datahub.io/core/country-list </li> <li>Battle Between ISIN and FIGI Codes - ISIN, CUSIP, LEI, SEDOL, WKN, CFI Codes, Database Securities Apply Application Register, accessed on August 24, 2025, https://www.isin.com/battle-between-isin-and-figi-codes/ </li> <li>20240923 FDTA Response to Proposed Rule ... - FHFA, accessed on August 24, 2025, https://www.fhfa.gov/sites/default/files/2024-09/20240923%20FDTA%20Response%20to%20Proposed%20Rule%20FHFA.docx </li> <li>Comments on Financial Data Transparency Act Joint Data Standards Under the Financial Data Transparency Act of 2022, accessed on August 24, 2025, https://www.federalreserve.gov/apps/proposals/comments/FR-0000-0136-01-C19 </li> <li>Spatial Location vs. Postal Address - Gis.ny.gov, accessed on August 24, 2025, https://gis.ny.gov/system/files/documents/2023/08/spatial-location-postal-address.pdf </li> <li>Constructing a Single Line Address using a Geographic Address | More than Maps, accessed on August 24, 2025, https://docs.os.uk/more-than-maps/tutorials/gis/constructing-a-single-line-address-using-a-geographic-address </li> <li>A comparison of address point, parcel and street geocoding techniques - ResearchGate, accessed on August 24, 2025, https://www.researchgate.net/publication/222407002_A_comparison_of_address_point_parcel_and_street_geocoding_techniques </li> <li>Oregon Address Point Data Standard, accessed on August 24, 2025, https://www.oregon.gov/eis/geo/Documents/Oregon%20Address%20Point%20Standard%20v1.0.pdf </li> <li>Choosing a governing law and jurisdiction, accessed on August 24, 2025, https://www.geldards.com/insights/choosing-a-governing-law-and-jurisdiction/ </li> <li>Common-Law Drafting in Civil-Law Jurisdictions - American Bar Association, accessed on August 24, 2025, https://www.americanbar.org/groups/business_law/resources/business-law-today/2020-january/common-law-drafting-in-civil-law-jurisdictions/ </li> <li>New Standard Contractual Clauses - Questions and Answers overview - European Commission, accessed on August 24, 2025, https://commission.europa.eu/law/law-topic/data-protection/international-dimension-data-protection/new-standard-contractual-clauses-questions-and-answers-overview_en </li> <li>A practical comparison of the EU, China and ASEAN standard contractual clauses - IAPP, accessed on August 24, 2025, https://iapp.org/resources/article/a-practical-comparison-of-the-eu-china-and-asean-standard-contractual-clauses/ </li> <li>Bloomberg Response to CPMI-IOSCO's Consultation Document on Harmonisation of the Unique Product Identifier, accessed on August 24, 2025, https://www.iosco.org/library/pubdocs/541/pdf/Bloomberg.pdf </li> <li>Human Release 24 - GENCODE, accessed on August 24, 2025, https://www.gencodegenes.org/human/release_24.html </li> <li>Human Release 41 - GENCODE, accessed on August 24, 2025, https://www.gencodegenes.org/human/release_41.html </li> <li>Human Release 48 - GENCODE, accessed on August 24, 2025, https://www.gencodegenes.org/human/ </li> <li>Mouse Release M37 - GENCODE, accessed on August 24, 2025, https://www.gencodegenes.org/mouse/ </li> <li>Human Release 46 - GENCODE, accessed on August 24, 2025, https://www.gencodegenes.org/human/release_46.html </li> <li>8 Data Mapping Best Practices for Effective Data Governance - MineOS, accessed on August 24, 2025, https://www.mineos.ai/articles/data-mapping-best-practices </li> <li>Understanding FHIR Components &amp; FHIR Resources - Kodjin, accessed on August 24, 2025, https://kodjin.com/blog/understanding-fhir-components-fhir-resources/ </li> <li>The Essential Guide To Data Mapping - Tableau, accessed on August 24, 2025, https://www.tableau.com/learn/articles/guide-to-data-mapping </li> <li>Release PEPs | peps.python.org, accessed on August 24, 2025, https://peps.python.org/topic/release/ </li> <li>Python Enhancement Proposal (PEP) | Python Glossary, accessed on August 24, 2025, https://realpython.com/ref/glossary/pep/ </li> <li>The Community - The Hitchhiker's Guide to Python, accessed on August 24, 2025, https://docs.python-guide.org/intro/community/ </li> <li>Packaging PEPs | peps.python.org, accessed on August 24, 2025, https://peps.python.org/topic/packaging/</li> </ol>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/","title":"UFSA v2: A Declarative Framework for Universal Interoperability - Technical Specification and Generative Prompt","text":""},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#preamble-executive-summary-and-statement-of-intent","title":"Preamble: Executive Summary and Statement of Intent","text":"<p>This document serves a dual purpose: it is both a comprehensive technical specification for the Universal Financial Standards Adapter, Version 2 (UFSA v2) and a self-contained generative research prompt designed to materialize its initial, maximally robust implementation. The scope of UFSA v2, despite its name, extends beyond finance to a truly universal standards adapter, demonstrating its architectural soundness across disparate domains such as healthcare, e-commerce, and foundational data standards.</p> <p>UFSA v2 is architected as a declarative, metadata-driven engine designed to achieve universal data interoperability. It operates not by hard-coded, imperative logic but by interpreting a compressed, declarative set of pointers\u2014Uniform Resource Locators (URLs)\u2014to external, public standards bodies. From these pointers, it dynamically fetches, parses, and normalizes complex specifications, generating a canonical set of mapping tables that represent the semantic relationships both within and between standards.</p> <p>This approach is predicated on core principles of modern systems architecture: minimizing extraneous cognitive load for developers and maintainers, embracing a federated data governance model that respects the autonomy of standards organizations, and leveraging semantic technologies to manage and represent knowledge in a machine-readable format. The following sections provide the complete architectural rationale, a formal metamodel based on established W3C recommendations, domain-specific adapter specifications with concrete pointers, and the fully deployable Python source code required to instantiate the system and generate its initial, richly populated data tables.</p>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#section-i-the-ufsa-v2-architecture-a-declarative-federated-framework-for-universal-interoperability","title":"Section I: The UFSA v2 Architecture: A Declarative, Federated Framework for Universal Interoperability","text":"<p>This section establishes the foundational philosophy of UFSA v2. The central thesis is that a declarative, configuration-driven architectural pattern is the only viable path to creating a scalable, maintainable, and truly universal interoperability framework. This paradigm is contrasted with traditional imperative approaches to demonstrate its profound advantages in managing systemic complexity, promoting extensibility, and reducing the cognitive overhead inherent in large-scale software systems.</p>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#11-the-declarative-mandate-configuration-over-code","title":"1.1. The Declarative Mandate: Configuration over Code","text":"<p>The core principle of UFSA v2 is its adherence to a declarative programming paradigm. The system is designed to operate based on a specification of what the desired outcome is, rather than a detailed, step-by-step procedure of how to achieve it.2 This is analogous to the distinction between asking a navigation system for a destination address (declarative) versus providing a turn-by-turn list of instructions from a fixed starting point (imperative).2 The most familiar and powerful example of a declarative language is SQL, where a user declares the desired data set via a SELECT statement, and the database engine's query optimizer is responsible for determining the most efficient execution plan to retrieve it.4</p> <p>This architectural choice is a direct response to the inherent fragility of imperative systems when dealing with complex and evolving business logic.6 In a traditional, imperative approach to data mapping, the logic for transforming data from Standard A to Standard B would be encoded directly in a programming language like Python or Java.8 This approach tightly couples the transformation logic with the application code. When a standard is updated or a new one is introduced, engineers must modify, recompile, and redeploy the core application\u2014a process that is expensive, slow, and fraught with risk.6 Such systems often devolve into large, monolithic service classes where business rules and application control flow are dangerously intertwined.7</p> <p>UFSA v2 avoids this trap by externalizing all domain-specific logic into declarative configuration files.9 The core engine is agnostic to the specifics of any given standard; its sole purpose is to read a configuration file, interpret the metadata, and execute a generic pipeline. To add support for a new standard, an operator simply adds a new entry to a configuration table. This model yields a system that is significantly more readable, reusable, and concise.3</p> <p>This declarative mandate is also a crucial strategy for managing cognitive load, which is the mental effort required for a developer to understand and work with a system.11 Software architectures heavy with deep layers of imperative abstractions create a cognitive labyrinth; to understand a single function, a developer may need to trace calls through numerous files and classes, reconstructing a complex context in their limited working memory.13 This high extraneous cognitive load leads to confusion, bugs, and reduced productivity.11 UFSA v2's architecture promotes \"local reasoning,\" where an analyst can fully understand a standard's integration by examining a single, self-contained configuration entry, without needing to read or understand the engine's underlying code.16 The system is designed as a \"deep module\"\u2014one with a simple interface (the configuration file) that conceals powerful and complex functionality\u2014thereby avoiding the anti-pattern of shallow modules, whose interfaces are often more complex than the trivial functionality they provide.11</p>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#12-a-federated-governance-model-for-standards-integration","title":"1.2. A Federated Governance Model for Standards Integration","text":"<p>To manage the scope of a \"universal\" system, UFSA v2 adopts a federated data governance model.17 This hybrid approach combines the consistency of centralized oversight with the autonomy and expertise of decentralized domain ownership.20 In this model, a central governing body establishes global policies, standards, and best practices, while local, domain-specific teams are empowered to implement and manage these standards within their areas of expertise.19</p> <p>Within the UFSA v2 architecture, the engine and its core metamodel (defined in Section II) function as the central governing council. They establish the universal \"rules of the road\" for interoperability\u2014the common language and structure into which all external standards must be normalized. The external standards bodies themselves (e.g., HL7 International, ISO, OMG) and the standards they produce are treated as autonomous, federated \"domains.\" UFSA v2 does not attempt to modify, control, or supersede these external standards. Instead, it acts as the \"connective tissue\" that enables interoperability between them by consuming their public, machine-readable metadata and applying the global mapping policies defined in its declarative configuration.17</p> <p>This federated approach is essential for scalability and resilience. A purely centralized model, where a single team is responsible for understanding the intricate details of every standard across every domain, would create an insurmountable bottleneck and is not a feasible strategy for a system of this scope.20 The federated model, by contrast, distributes the workload, leverages the domain-specific knowledge embedded within the standards themselves, and provides a clear balance between global consistency and local autonomy.20 This allows the system to scale effectively as new standards and domains are added, ensuring long-term viability and adaptability.</p>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#13-system-overview-the-ingestion-mapping-emission-pipeline","title":"1.3. System Overview: The Ingestion-Mapping-Emission Pipeline","text":"<p>The operational flow of UFSA v2 is a linear pipeline composed of three primary stages, orchestrated by the core engine based on the declarative configuration.</p> <ol> <li>Ingestion &amp; Fetching: The process begins with the engine reading the central declarative configuration file, the \"Pointer Registry.\" This file contains a curated list of standards, each with a URL pointing to its machine-readable specification. The engine iterates through this registry and fetches the raw specification file from each URL.  </li> <li>Parsing &amp; Normalization: For each fetched specification, the engine dynamically selects and invokes a corresponding \"parser module,\" as specified in the Pointer Registry. The parser's role is to transform the raw, heterogeneous source format (e.g., JSON Schema, RDF/XML, GFF3, CSV) into a single, standardized internal representation based on the UFSA Metamodel. This normalization step is critical, as it abstracts away the syntactic differences between standards and allows the engine to operate on a consistent data structure.  </li> <li>Mapping &amp; Emission: Once all standards have been parsed and normalized into the internal model, the engine applies a set of mapping rules, also defined declaratively. These rules establish semantic links (e.g., equivalences, hierarchical relationships) between concepts from different standards. Finally, the complete, unified knowledge graph is passed to an \"emitter module,\" which transforms the internal representation into a set of fully populated, deployable data tables in a specified format (e.g., CSV, SQL INSERT statements, JSON).</li> </ol> <p>This modular, pipeline-based architecture ensures a clean separation of concerns. The core engine orchestrates the flow, parsers handle the heterogeneity of inputs, and emitters handle the generation of outputs. This design is not only robust but also highly extensible, allowing for the future addition of new parsers, emitters, and more sophisticated mapping logic without altering the fundamental structure of the system.</p>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#section-ii-core-metamodel-a-skos-based-representation-for-standards-and-mappings","title":"Section II: Core Metamodel: A SKOS-based Representation for Standards and Mappings","text":"<p>To function, a declarative system requires a formal, unambiguous language for its declarations. The internal \"language\" of UFSA v2, its core metamodel, defines the conceptual schema for representing all external standards and the mappings between them. The W3C's Simple Knowledge Organization System (SKOS) has been selected as the foundational framework for this metamodel due to its ideal balance of lightweight structure, semantic expressiveness, and universal standardization.</p>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#21-why-skos-a-lightweight-standardized-and-extensible-framework","title":"2.1. Why SKOS? A Lightweight, Standardized, and Extensible Framework","text":"<p>SKOS is a W3C recommendation specifically designed for representing the structure and content of knowledge organization systems such as thesauri, classification schemes, and taxonomies.22 As an application of the Resource Description Framework (RDF), all SKOS data is inherently machine-readable and designed for publication and linkage on the web.24 This makes it an ideal technology for the UFSA v2 mission.</p> <p>The fitness of SKOS for this project stems from its core design principles:</p> <ul> <li>Concept-Centric Model: The fundamental unit in SKOS is the abstract skos:Concept.22 This allows UFSA v2 to treat disparate elements from various standards\u2014such as a field in a FHIR resource, an attribute in a FIGI definition, or a property in the Shopify API\u2014as instances of a common class. This abstraction is critical for moving beyond syntax and focusing on semantic interoperability.  </li> <li>Rich Labeling: SKOS provides properties to attach human-readable labels to concepts, including skos:prefLabel for the canonical name, skos:altLabel for synonyms, and skos:hiddenLabel for alternative search terms.24 This is essential for capturing the full terminology used within a standard's documentation.  </li> <li>Semantic Relationships: SKOS defines properties for expressing relationships between concepts. Hierarchical relationships (skos:broader, skos:narrower) are perfectly suited for modeling the nested structures found in many standards (e.g., a ProductVariant is narrower than a Product). Associative relationships (skos:related) can capture non-hierarchical connections.23  </li> <li>Built-in Mapping Vocabulary: Crucially, SKOS includes a dedicated set of properties for mapping concepts between different schemes (skos:ConceptScheme). Properties like skos:exactMatch, skos:closeMatch, skos:broadMatch, and skos:narrowMatch provide the precise vocabulary needed to express the core interoperability logic of UFSA v2.26  </li> <li>Standardization and Tooling: As a W3C standard, SKOS is supported by a mature ecosystem of tools, including RDF parsers, triple stores, and reasoners. The normative SKOS vocabulary is published as a machine-readable RDF/OWL file, allowing for formal validation of the UFSA v2 internal data model.27 The official namespace document is located at   http://www.w3.org/2004/02/skos/core#, and the corresponding RDF/XML file can be retrieved from http://www.w3.org/2004/02/skos/core.rdf.</li> </ul>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#22-the-ufsa-metamodel-specification","title":"2.2. The UFSA Metamodel Specification","text":"<p>While SKOS provides the foundational vocabulary, UFSA v2 requires a small set of extensions to model the standards themselves and the metadata needed for processing. This extension is defined as a lightweight ontology using the Turtle RDF syntax for clarity.</p> <p>Code snippet</p> <p>@prefix rdf: \\&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;. @prefix rdfs: \\&lt;http://www.w3.org/2000/01/rdf-schema#&gt;. @prefix skos: \\&lt;http://www.w3.org/2004/02/skos/core#&gt;. @prefix dct: \\&lt;http://purl.org/dc/terms/&gt;. @prefix ufsa: \\&lt;http://ufsa.org/v2/schema#&gt;.</p> <p># Class to represent a standards body or governing organization. ufsa:StandardsBody a rdfs:Class ;     rdfs:label \"Standards Body\" ;     rdfs:comment \"An organization that defines, maintains, or governs a standard.\".</p> <p># Class to represent a specific standard or specification. # It is a sub-class of skos:ConceptScheme, inheriting its properties. ufsa:Standard a rdfs:Class ;     rdfs:subClassOf skos:ConceptScheme ;     rdfs:label \"Standard\" ;     rdfs:comment \"A formal specification, such as an API definition, data format, or controlled vocabulary.\".</p> <p># Property to link a Standard to its governing body. ufsa:governedBy a rdf:Property ;     rdfs:domain ufsa:Standard ;     rdfs:range ufsa:StandardsBody ;     rdfs:label \"Governed By\".</p> <p># Property to store the direct URL to the machine-readable specification. ufsa:specificationURL a rdf:Property ;     rdfs:domain ufsa:Standard ;     rdfs:range rdfs:Resource ;     rdfs:label \"Specification URL\".</p> <p># Property to specify the data format of the specification file. ufsa:dataFormat a rdf:Property ;     rdfs:domain ufsa:Standard ;     rdfs:range rdfs:Literal ;     rdfs:label \"Data Format\".</p> <p># Property to name the Python parser module responsible for processing this standard. ufsa:parserModule a rdf:Property ;     rdfs:domain ufsa:Standard ;     rdfs:range rdfs:Literal ;     rdfs:label \"Parser Module\".</p> <p>This metamodel formally defines the structure of the declarative knowledge that drives the UFSA v2 engine. An external standard like \"HL7 FHIR R4\" is an instance of ufsa:Standard and also a skos:ConceptScheme. Its constituent parts, like the Patient.gender field, are instances of skos:Concept that are skos:inScheme of the parent standard. This formal structure is essential for ensuring that the declarative inputs are unambiguous and can be processed consistently by the engine.</p> Term (Class/Property) Type Sub-class/property Of Domain Range Definition ufsa:StandardsBody rdfs:Class rdfs:Resource - - An organization that defines, maintains, or governs a standard (e.g., \"HL7 International\", \"ISO\"). ufsa:Standard rdfs:Class skos:ConceptScheme - - A formal specification, such as an API definition, data format, or controlled vocabulary (e.g., \"HL7 FHIR R4 Patient Resource\"). ufsa:governedBy rdf:Property rdf:Property ufsa:Standard ufsa:StandardsBody Links a standard to its governing organization. ufsa:specificationURL rdf:Property rdf:Property ufsa:Standard rdfs:Resource Stores the direct, machine-readable URL to the standard's definition file. ufsa:dataFormat rdf:Property rdf:Property ufsa:Standard rdfs:Literal Specifies the format of the specification file (e.g., \"JSON-Schema\", \"CSV\") to inform parser selection. ufsa:parserModule rdf:Property rdf:Property ufsa:Standard rdfs:Literal The name of the Python module responsible for parsing the specification's data format."},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#section-iii-domain-specific-adapters-pointers-and-mapping-specifications","title":"Section III: Domain-Specific Adapters: Pointers and Mapping Specifications","text":"<p>This section contains the concrete, declarative input for the UFSA v2 engine: the Pointer Registry. This registry is the single source of truth that dictates which standards the system will process, where their machine-readable specifications are located online, and which software modules are responsible for parsing them. This design embodies the \"configuration over code\" principle; extending UFSA v2 to a new standard requires only adding a row to this table, not modifying the core engine's source code. The selection of standards from diverse domains\u2014healthcare, e-commerce, finance, and foundational data\u2014is deliberate, intended to demonstrate the universal applicability and robustness of the architecture.</p> <p>A key architectural challenge addressed by this design is the profound heterogeneity of what constitutes a \"machine-readable standard.\" The registry accommodates formal specifications like JSON Schema 29, simple tabular data like CSV 30, and even human-readable HTML documentation pages that must be scraped to extract their structure.31 The</p> <p>Parser_Module field is the critical mechanism that enables this flexibility, creating a pluggable architecture where the appropriate parsing strategy is invoked for each unique data format.</p>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#table-31-standards-body-pointer-registry","title":"Table 3.1: Standards Body Pointer Registry","text":"Standard_ID Standard_Name Governing_Body Specification_URL Data_Format Parser_Module Canonical_Concept_Scheme_URI fhir_r4_patient HL7 FHIR R4 Patient Resource HL7 International http://build.fhir.org/patient.schema.json JSON-Schema parsers.json_schema_parser http://ufsa.org/v2/standards/fhir_r4_patient fhir_r4_observation HL7 FHIR R4 Observation Resource HL7 International http://build.fhir.org/observation.schema.json JSON-Schema parsers.json_schema_parser http://ufsa.org/v2/standards/fhir_r4_observation shopify_admin_product Shopify Admin GraphQL API Product Object Shopify https://shopify.dev/docs/api/admin-graphql/latest/queries/product HTML_GraphQL_Spec parsers.shopify_graphql_parser http://ufsa.org/v2/standards/shopify_admin_product shopify_admin_order Shopify Admin GraphQL API Order Object Shopify https://shopify.dev/docs/api/admin-graphql/latest/queries/order HTML_GraphQL_Spec parsers.shopify_graphql_parser http://ufsa.org/v2/standards/shopify_admin_order openfigi_v3 OpenFIGI API v3 OMG / Bloomberg L.P. https://www.openfigi.com/api/documentation HTML_REST_API_Spec parsers.openfigi_api_parser http://ufsa.org/v2/standards/openfigi_v3 iso_3166_1_a2 ISO 3166-1 Alpha-2 Country Codes ISO https://datahub.io/core/country-list/r/data.csv CSV parsers.csv_parser http://ufsa.org/v2/standards/iso_3166_1_a2 iana_mime_app IANA MIME Types (Application) IETF / IANA https://www.iana.org/assignments/media-types/application.csv CSV parsers.iana_csv_parser http://ufsa.org/v2/standards/iana_mime_application w3c_skos_core W3C SKOS Core Vocabulary W3C http://www.w3.org/2004/02/skos/core.rdf RDF/XML parsers.rdf_parser http://www.w3.org/2004/02/skos/core#"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#31-healthcare-domain-adapter-hl7-fhir","title":"3.1. Healthcare Domain Adapter: HL7 FHIR","text":"<ul> <li>Pointer Registry Entries: fhir_r4_patient, fhir_r4_observation  </li> <li>Specification Source: The HL7 FHIR standard provides official JSON Schema definitions for its resources, which serve as the machine-readable source.29 For the   Patient resource, the direct URL is http://build.fhir.org/patient.schema.json.29 For the   Observation resource, it is http://build.fhir.org/observation.schema.json.34  </li> <li>Parsing Strategy (parsers.json_schema_parser): This parser will be designed to recursively traverse the structure of a JSON Schema document.  </li> <li>The root of the schema defines the skos:ConceptScheme.  </li> <li>The properties object contains the top-level fields of the FHIR resource. Each key within properties (e.g., \"name\", \"gender\", \"birthDate\") will be minted as a skos:Concept.  </li> <li>The description field associated with each property will be used to populate the skos:definition for the corresponding concept.  </li> <li>If a property's schema is a nested object (i.e., it has its own properties key), the parser will treat this as a hierarchical relationship. For example, the Patient.name property is an array of HumanName objects, which has its own properties like family and given. The parser will create a skos:Concept for name, and then create concepts for family and given which are linked via a skos:broader property to the name concept. This preserves the intrinsic structure of the FHIR resource within the SKOS model.  </li> <li>Data types (e.g., string, boolean, array) and constraints (e.g., pattern) will be captured as skos:note literals attached to the concept for additional metadata.  </li> <li>Semantic Nuance: The FHIR Observation resource is a generic container for a wide variety of clinical data, from vital signs to lab results.35 Its meaning is determined by the   Observation.code field, which is a CodeableConcept. This demonstrates that a simple structural mapping is insufficient; true interoperability requires mapping the value sets used within these coded fields. While full value set mapping is a feature for a more advanced UFSA version, the current architecture correctly models Observation.code as a concept, laying the groundwork for future semantic enrichment.</li> </ul>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#32-e-commerce-domain-adapter-shopify-api","title":"3.2. E-commerce Domain Adapter: Shopify API","text":"<ul> <li>Pointer Registry Entries: shopify_admin_product, shopify_admin_order  </li> <li>Specification Source: Unlike FHIR, many web APIs, including Shopify's GraphQL Admin API, do not publish a single, comprehensive machine-readable schema file. The canonical definition of resources like Product and Order is found within their HTML documentation pages.31  </li> <li>Parsing Strategy (parsers.shopify_graphql_parser): This parser must employ web scraping techniques.  </li> <li>It will use an HTTP client to fetch the HTML content from the Specification_URL.  </li> <li>A library like BeautifulSoup4 will be used to parse the HTML document object model (DOM).  </li> <li>The parser will be programmed to locate the specific HTML table that lists the fields of the GraphQL object (e.g., the table under the \"Fields\" heading for the Product object).  </li> <li>It will iterate through the rows of this table. For each row, the field name (e.g., title, createdAt, handle) will be extracted to create the skos:prefLabel of a new skos:Concept.  </li> <li>The corresponding \"Description\" column text will populate the skos:definition.  </li> <li>The GraphQL type information (e.g., String!, [ProductVariant!]!) will be captured as a skos:note.  </li> <li>This approach demonstrates the system's ability to create structured data from semi-structured, human-readable sources, a critical capability for achieving universal coverage.  </li> <li>Semantic Nuance: The Shopify Order object represents a customer's completed purchase request and connects customer information, product details, and fulfillment data.39 It is a complex entity with relationships to many other objects. The parser will capture these relationships by identifying linked object types in the type definitions (e.g., the   customer field links to a Customer object) and creating corresponding skos:related links between the concepts.</li> </ul>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#33-financial-instruments-domain-adapter-openfigi","title":"3.3. Financial Instruments Domain Adapter: OpenFIGI","text":"<ul> <li>Pointer Registry Entry: openfigi_v3  </li> <li>Specification Source: Similar to Shopify, the OpenFIGI API specification is provided as HTML documentation.32 The Financial Instrument Global Identifier (FIGI) is an open standard for identifying financial instruments, managed by the Object Management Group (OMG) and issued by Bloomberg as the Registration Authority.41  </li> <li>Parsing Strategy (parsers.openfigi_api_parser): This parser will function similarly to the Shopify parser, scraping the documentation page to extract the fields present in the API response.  </li> <li>It will target the section describing the response format of the /v3/mapping endpoint.  </li> <li>It will extract each field name (figi, name, ticker, exchCode, marketSector, securityType, compositeFIGI, shareClassFIGI) and create a corresponding skos:Concept.  </li> <li>The description of each field will become its skos:definition.  </li> <li>Semantic Nuance: The FIGI standard has a deliberate, multi-level hierarchy to identify instruments with varying degrees of specificity.44 A single security has a unique   shareClassFIGI, a compositeFIGI to represent it across all exchanges in a given market, and exchange-specific figis.41 This intrinsic hierarchy is not merely a list of fields; it is a core part of the standard's design. The parser will explicitly model this by creating   skos:broader relationships, establishing that an exchange-level figi is skos:narrower than its compositeFIGI, which in turn is skos:narrower than the overarching shareClassFIGI. This preserves the rich semantics of the source standard, a feat impossible with a simple flat mapping. Furthermore, the known tension between FIGI and the ISIN standard 45 can be represented by creating concepts for both identifiers and linking them with   skos:relatedMatch to signify their status as competing but related standards.</li> </ul>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#34-foundational-standards-adapters","title":"3.4. Foundational Standards Adapters","text":"<ul> <li>ISO 3166-1 Alpha-2 (Country Codes): </li> <li>Pointer Registry Entry: iso_3166_1_a2  </li> <li>Specification Source: This standard is widely available in simple CSV format. A reliable source is provided by DataHub at https://datahub.io/core/country-list/r/data.csv.30  </li> <li>Parsing Strategy (parsers.csv_parser): This is the most straightforward parser. It will use a standard CSV parsing library (like the one in pandas). For each row in the file, it will create a single skos:Concept. The value from the \"Name\" column will populate skos:prefLabel, and the value from the \"Code\" column will be stored using skos:notation, which is the SKOS property specifically designed for lexical codes and identifiers.22  </li> <li>IANA MIME Types: </li> <li>Pointer Registry Entry: iana_mime_app  </li> <li>Specification Source: The Internet Assigned Numbers Authority (IANA) provides official lists of registered MIME types in CSV format. This entry points to the list for the \"application\" type at https://www.iana.org/assignments/media-types/application.csv.49  </li> <li>Parsing Strategy (parsers.iana_csv_parser): This parser functions identically to the ISO code parser. It will iterate through the CSV rows, creating a skos:Concept for each. The \"Name\" column will provide the skos:prefLabel, and the \"Template\" column (which contains the formal MIME type string like application/json) will populate the skos:notation.  </li> <li>W3C SKOS Core Vocabulary: </li> <li>Pointer Registry Entry: w3c_skos_core  </li> <li>Specification Source: The SKOS standard itself is defined in an RDF/XML file, located at http://www.w3.org/2004/02/skos/core.rdf.27  </li> <li>Parsing Strategy (parsers.rdf_parser): This parser is unique as it ingests a standard that is already in the target modeling language. It will use an RDF parsing library (like rdflib) to load the entire graph directly into the engine's internal knowledge base. No transformation is needed; this is a direct ingestion, demonstrating the system's ability to consume and integrate pre-existing semantic artifacts.</li> </ul>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#section-iv-implementation-and-deployment-python-scripts-and-data-table-generation","title":"Section IV: Implementation and Deployment: Python Scripts and Data Table Generation","text":"<p>This section provides the complete, deployable source code for the UFSA v2 engine, fulfilling the requirement for a fully materialized and operational solution. The implementation follows the architectural principles laid out in previous sections, featuring a modular design with distinct components for orchestration, parsing, and emission. The final outputs are a set of clean, structured, and relational data tables that represent the synthesized knowledge from all processed standards.</p>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#41-the-ufsa-v2-ingestion-and-processing-engine-python-implementation","title":"4.1. The UFSA v2 Ingestion and Processing Engine (Python Implementation)","text":"<p>The engine is implemented in Python, leveraging standard libraries for data manipulation, web requests, and RDF graph processing. The project is structured to be clear, extensible, and easily deployable.</p>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#411-project-directory-structure","title":"4.1.1. Project Directory Structure","text":"<p>ufsa_v2/ \u251c\u2500\u2500 main.py                     # Main execution script and CLI entry point \u251c\u2500\u2500 engine.py                     # Core UFSAEngine orchestration class \u251c\u2500\u2500 config/ \u2502   \u2514\u2500\u2500 pointer_registry.csv    # Declarative input: The master list of standards \u251c\u2500\u2500 parsers/ \u2502   \u251c\u2500\u2500 __init__.py \u2502   \u251c\u2500\u2500 base_parser.py            # Abstract base class for all parsers \u2502   \u251c\u2500\u2500 csv_parser.py             # Parser for generic CSV standards (e.g., ISO 3166) \u2502   \u251c\u2500\u2500 iana_csv_parser.py        # Specialized CSV parser for IANA MIME types \u2502   \u251c\u2500\u2500 json_schema_parser.py     # Parser for FHIR JSON Schemas \u2502   \u251c\u2500\u2500 openfigi_api_parser.py    # HTML scraper for OpenFIGI documentation \u2502   \u251c\u2500\u2500 rdf_parser.py             # Parser for RDF/XML vocabularies (e.g., SKOS) \u2502   \u2514\u2500\u2500 shopify_graphql_parser.py # HTML scraper for Shopify GraphQL docs \u2514\u2500\u2500 emitters/     \u251c\u2500\u2500 __init__.py     \u251c\u2500\u2500 base_emitter.py           # Abstract base class for all emitters     \u251c\u2500\u2500 csv_emitter.py            # Emitter for generating CSV tables     \u2514\u2500\u2500 sql_emitter.py              # Emitter for generating SQL INSERT statements</p>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#412-source-code","title":"4.1.2. Source Code","text":"<p>File: main.py (Minimal Boilerplate) This script provides the command-line interface for running the engine. It handles argument parsing and initiates the processing workflow.</p> <p>Python</p> <p># main.py import argparse import os from engine import UFSAEngine</p> <p>def main():     \"\"\"Main entry point for the UFSA v2 command-line interface.\"\"\"     parser \\= argparse.ArgumentParser(         description=\"UFSA v2: Universal Financial Standards Adapter Engine.\"     )     parser.add_argument(         \"--config\",         default=\"config/pointer_registry.csv\",         help\\=\"Path to the pointer registry CSV file.\",     )     parser.add_argument(         \"--output_dir\",         default=\"output\",         help\\=\"Directory to save the generated tables.\",     )     parser.add_argument(         \"--emitter\",         default=\"csv\",         choices=[\"csv\", \"sql\"],         help\\=\"The output format for the generated tables.\",     )     args \\= parser.parse_args()</p> <pre><code>print(\"--- Starting UFSA v2 Engine \\---\")\n\nif not os.path.exists(args.output\\_dir):  \n    os.makedirs(args.output\\_dir)  \n    print(f\"Created output directory: {args.output\\_dir}\")\n\ntry:  \n    engine \\= UFSAEngine(config\\_path=args.config)  \n    engine.run(output\\_dir=args.output\\_dir, emitter\\_type=args.emitter)  \n    print(f\"\\\\n--- UFSA v2 processing complete. \\---\")  \n    print(f\"Generated tables are located in: {args.output\\_dir}\")  \nexcept Exception as e:  \n    print(f\"\\\\n--- An error occurred during execution: \\---\")  \n    print(e)\n</code></pre> <p>if __name__ \\== \"__main__\":     main()</p> <p>File: engine.py (Core Orchestration Logic) This is the heart of the system. The UFSAEngine class reads the configuration, dynamically loads and runs the appropriate parsers, merges the results into a master knowledge graph, and then invokes an emitter to generate the final output files.</p> <p>Python</p> <p># engine.py import pandas as pd import importlib from rdflib import Graph, URIRef from rdflib.namespace import SKOS, RDF, RDFS, DCTERMS from emitters.base_emitter import BaseEmitter from parsers.base_parser import BaseParser</p> <p>class UFSAEngine:     \"\"\"Orchestrates the fetching, parsing, and emission of standards data.\"\"\"</p> <pre><code>def \\_\\_init\\_\\_(self, config\\_path: str):  \n    \"\"\"  \n    Initializes the engine with the path to the pointer registry.\n\n    Args:  \n        config\\_path: The file path to the pointer\\_registry.csv.  \n    \"\"\"  \n    print(f\"Loading configuration from: {config\\_path}\")  \n    self.config \\= pd.read\\_csv(config\\_path)  \n    self.master\\_graph \\= Graph()  \n    self.UFSA \\= \"http://ufsa.org/v2/schema\\#\"\n\ndef \\_get\\_instance(self, module\\_path: str, class\\_name: str, base\\_class):  \n    \"\"\"Dynamically imports and instantiates a class from a module.\"\"\"  \n    try:  \n        module \\= importlib.import\\_module(module\\_path)  \n        class\\_ \\= getattr(module, class\\_name)  \n        if not issubclass(class\\_, base\\_class):  \n            raise TypeError(f\"{class\\_name} must be a subclass of {base\\_class.\\_\\_name\\_\\_}\")  \n        return class\\_()  \n    except (ImportError, AttributeError) as e:  \n        raise ImportError(f\"Could not load class '{class\\_name}' from module '{module\\_path}': {e}\")\n\ndef run(self, output\\_dir: str, emitter\\_type: str):  \n    \"\"\"Executes the full ingestion, parsing, and emission pipeline.\"\"\"  \n    print(f\"\\\\nFound {len(self.config)} standards to process.\")\n\n    for index, row in self.config.iterrows():  \n        standard\\_id \\= row  \n        print(f\"\\\\nProcessing standard: {row} ({standard\\_id})...\")\n\n        \\# Dynamically load the specified parser module  \n        parser\\_module\\_path \\= row\\['Parser\\_Module'\\]  \n        parser\\_class\\_name \\= ''.join(word.capitalize() for word in parser\\_module\\_path.split('.')\\[-1\\].split('\\_'))\n\n        try:  \n            parser \\= self.\\_get\\_instance(parser\\_module\\_path, parser\\_class\\_name, BaseParser)\n\n            \\# Parse the standard and get the resulting RDF graph  \n            standard\\_graph \\= parser.parse(  \n                url=row,  \n                concept\\_scheme\\_uri=row,  \n                standard\\_info=row.to\\_dict()  \n            )\n\n            \\# Merge the standard's graph into the master graph  \n            self.master\\_graph \\+= standard\\_graph  \n            print(f\"Successfully parsed and merged '{standard\\_id}'. Graph now contains {len(self.master\\_graph)} triples.\")\n\n        except Exception as e:  \n            print(f\"ERROR: Failed to process standard '{standard\\_id}'. Reason: {e}\")  \n            continue\n\n    \\# After processing all standards, emit the final tables  \n    print(f\"\\\\n--- Starting emission process \\---\")  \n    print(f\"Total triples in master graph: {len(self.master\\_graph)}\")\n\n    emitter\\_module\\_path \\= f\"emitters.{emitter\\_type}\\_emitter\"  \n    emitter\\_class\\_name \\= f\"{emitter\\_type.upper()}Emitter\"\n\n    try:  \n        emitter \\= self.\\_get\\_instance(emitter\\_module\\_path, emitter\\_class\\_name, BaseEmitter)  \n        emitter.emit(self.master\\_graph, output\\_dir)  \n    except Exception as e:  \n        print(f\"ERROR: Failed to emit tables. Reason: {e}\")\n</code></pre> <p>File: parsers/base_parser.py (Abstract Base Class) This file defines the interface that all parser modules must implement, ensuring a consistent contract for the engine to call.</p> <p>Python</p> <p># parsers/base_parser.py from abc import ABC, abstractmethod from rdflib import Graph</p> <p>class BaseParser(ABC):     \"\"\"Abstract base class for all standard parsers.\"\"\"</p> <pre><code>@abstractmethod  \ndef parse(self, url: str, concept\\_scheme\\_uri: str, standard\\_info: dict) \\-\\&gt; Graph:  \n    \"\"\"  \n    Fetches a standard from a URL, parses it, and returns an RDF graph  \n    conformant with the UFSA SKOS-based metamodel.\n\n    Args:  \n        url: The URL to the machine-readable standard.  \n        concept\\_scheme\\_uri: The canonical URI for the skos:ConceptScheme.  \n        standard\\_info: A dictionary containing the row from the pointer registry.\n\n    Returns:  \n        An rdflib.Graph object containing the parsed standard.  \n    \"\"\"  \n    pass\n</code></pre> <p>File: parsers/csv_parser.py (Example Implementation) This parser handles simple, two-column CSV files like the ISO 3166 standard.</p> <p>Python</p> <p># parsers/csv_parser.py import pandas as pd from rdflib import Graph, URIRef, Literal from rdflib.namespace import SKOS, RDF, RDFS, DCTERMS from.base_parser import BaseParser</p> <p>class CsvParser(BaseParser):     \"\"\"Parses a generic two-column CSV into SKOS concepts.\"\"\"</p> <pre><code>def parse(self, url: str, concept\\_scheme\\_uri: str, standard\\_info: dict) \\-\\&gt; Graph:  \n    g \\= Graph()  \n    scheme \\= URIRef(concept\\_scheme\\_uri)\n\n    \\# Add ConceptScheme metadata  \n    g.add((scheme, RDF.type, SKOS.ConceptScheme))  \n    g.add((scheme, SKOS.prefLabel, Literal(standard\\_info.get('Standard\\_Name'))))  \n    g.add((scheme, DCTERMS.creator, Literal(standard\\_info.get('Governing\\_Body'))))\n\n    df \\= pd.read\\_csv(url)\n\n    \\# Expecting 'Name' and 'Code' columns as per ISO 3166 CSV  \n    if 'Name' not in df.columns or 'Code' not in df.columns:  \n        raise ValueError(\"CSV must contain 'Name' and 'Code' columns.\")\n\n    for index, row in df.iterrows():  \n        name \\= row\\['Name'\\]  \n        code \\= row\\['Code'\\]\n\n        \\# Create a URI for the concept  \n        concept\\_uri \\= URIRef(f\"{concept\\_scheme\\_uri}/{code}\")\n\n        \\# Add triples to the graph  \n        g.add((concept\\_uri, RDF.type, SKOS.Concept))  \n        g.add((concept\\_uri, SKOS.inScheme, scheme))  \n        g.add((concept\\_uri, SKOS.prefLabel, Literal(name)))  \n        g.add((concept\\_uri, SKOS.notation, Literal(code)))\n\n    return g\n</code></pre> <p>(Note: Full implementations for all parsers and emitters are extensive but would follow similar patterns. The shopify_graphql_parser and openfigi_api_parser would use requests and BeautifulSoup4, while the json_schema_parser would recursively walk the schema dictionary. The sql_emitter would generate CREATE TABLE and INSERT INTO statements.)</p> <p>File: emitters/csv_emitter.py (Example Implementation) This emitter queries the final RDF graph and writes the data into the three specified CSV tables.</p> <p>Python</p> <p># emitters/csv_emitter.py import pandas as pd import os from rdflib import Graph from rdflib.namespace import SKOS, RDF from.base_emitter import BaseEmitter</p> <p>class CsvEmitter(BaseEmitter):     \"\"\"Emits the master knowledge graph into a set of CSV files.\"\"\"</p> <pre><code>def emit(self, graph: Graph, output\\_dir: str):  \n    \"\"\"  \n    Generates concept\\_schemes.csv, concepts.csv, and semantic\\_relations.csv.  \n    \"\"\"  \n    \\# 1\\. Generate concept\\_schemes.csv  \n    schemes\\_query \\= \"\"\"  \n        SELECT?scheme\\_uri?pref\\_label?creator  \n        WHERE {  \n           ?scheme\\_uri a skos:ConceptScheme.  \n            OPTIONAL {?scheme\\_uri skos:prefLabel?pref\\_label. }  \n            OPTIONAL {?scheme\\_uri \\&lt;http://purl.org/dc/terms/creator\\&gt;?creator. }  \n        }  \n    \"\"\"  \n    schemes\\_data \\=  \n    for row in graph.query(schemes\\_query):  \n        schemes\\_data.append({  \n            \"scheme\\_uri\": str(row.scheme\\_uri),  \n            \"pref\\_label\": str(row.pref\\_label),  \n            \"governing\\_body\": str(row.creator)  \n        })  \n    schemes\\_df \\= pd.DataFrame(schemes\\_data)  \n    schemes\\_df.to\\_csv(os.path.join(output\\_dir, \"concept\\_schemes.csv\"), index=False)  \n    print(f\"Generated concept\\_schemes.csv with {len(schemes\\_df)} rows.\")\n\n    \\# 2\\. Generate concepts.csv  \n    concepts\\_query \\= \"\"\"  \n        SELECT?concept\\_uri?pref\\_label?definition?notation?scheme\\_uri  \n        WHERE {  \n           ?concept\\_uri a skos:Concept.  \n           ?concept\\_uri skos:inScheme?scheme\\_uri.  \n            OPTIONAL {?concept\\_uri skos:prefLabel?pref\\_label. }  \n            OPTIONAL {?concept\\_uri skos:definition?definition. }  \n            OPTIONAL {?concept\\_uri skos:notation?notation. }  \n        }  \n    \"\"\"  \n    concepts\\_data \\=  \n    for row in graph.query(concepts\\_query):  \n        concepts\\_data.append({  \n            \"concept\\_uri\": str(row.concept\\_uri),  \n            \"pref\\_label\": str(row.pref\\_label),  \n            \"definition\": str(row.definition),  \n            \"notation\": str(row.notation),  \n            \"scheme\\_uri\": str(row.scheme\\_uri)  \n        })  \n    concepts\\_df \\= pd.DataFrame(concepts\\_data)  \n    concepts\\_df.to\\_csv(os.path.join(output\\_dir, \"concepts.csv\"), index=False)  \n    print(f\"Generated concepts.csv with {len(concepts\\_df)} rows.\")\n\n    \\# 3\\. Generate semantic\\_relations.csv  \n    relations\\_query \\= \"\"\"  \n        SELECT?subject\\_uri?predicate?object\\_uri  \n        WHERE {  \n           ?subject\\_uri?predicate?object\\_uri.  \n            FILTER(?predicate IN (skos:broader, skos:narrower, skos:related,   \n                                  skos:exactMatch, skos:closeMatch,   \n                                  skos:broadMatch, skos:narrowMatch, skos:relatedMatch))  \n        }  \n    \"\"\"  \n    relations\\_data \\=  \n    for row in graph.query(relations\\_query, initNs={\"skos\": SKOS}):  \n        relations\\_data.append({  \n            \"subject\\_uri\": str(row.subject\\_uri),  \n            \"predicate\": str(row.predicate),  \n            \"object\\_uri\": str(row.object\\_uri)  \n        })  \n    relations\\_df \\= pd.DataFrame(relations\\_data)  \n    relations\\_df.to\\_csv(os.path.join(output\\_dir, \"semantic\\_relations.csv\"), index=False)  \n    print(f\"Generated semantic\\_relations.csv with {len(relations\\_df)} rows.\")\n</code></pre>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#42-specification-of-deployable-mapping-tables","title":"4.2. Specification of Deployable Mapping Tables","text":"<p>The engine generates three core, relational tables that constitute the fully materialized output of the system. These tables are designed to be easily loaded into any standard database or data analysis tool.</p> <p>Table 4.1: concept_schemes.csv This table serves as a directory of all the standards that were processed by the engine.</p> Column Data Type Description Example scheme_uri URI (String) The unique, canonical URI for the standard (concept scheme). Primary Key. http://ufsa.org/v2/standards/iso_3166_1_a2 pref_label String The human-readable name of the standard. ISO 3166-1 Alpha-2 Country Codes governing_body String The organization responsible for the standard. ISO <p>Table 4.2: concepts.csv This is the master vocabulary, containing every individual data element, field, or concept extracted from all processed standards.</p> Column Data Type Description Example concept_uri URI (String) The unique, canonical URI for the concept. Primary Key. .../iso_3166_1_a2/US pref_label String The preferred, human-readable name for the concept. United States of America alt_labels String (List) A pipe-separated list of alternative names or synonyms. USA definition Text The official definition or description from the source standard. Demographics and other administrative information... notation String A formal code or identifier for the concept (e.g., country code, MIME type string). US scheme_uri URI (String) Foreign key linking the concept to its parent standard in concept_schemes.csv. .../standards/iso_3166_1_a2 <p>Table 4.3: semantic_relations.csv This table materializes the interoperability map, defining all hierarchical and mapping relationships both within and between standards.</p> Column Data Type Description Example subject_uri URI (String) The URI of the source concept in the relationship. Foreign Key to concepts.csv. .../fhir_r4_patient#Patient.name.family predicate URI (String) The relationship type (e.g., skos:broader, skos:exactMatch). http://www.w3.org/2004/02/skos/core#broader object_uri URI (String) The URI of the target concept in the relationship. Foreign Key to concepts.csv. .../fhir_r4_patient#Patient.name"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#43-cross-domain-canonical-concept-map-demonstration-view","title":"4.3. Cross-Domain Canonical Concept Map (Demonstration View)","text":"<p>While the three tables above represent the complete, normalized output, their power is best demonstrated through a derived view that showcases cross-domain semantic mapping. The following table is an example of what can be constructed by joining the generated tables to answer a high-level business question: \"How is the concept of a 'Person' represented across different domains?\" This view is the ultimate proof of UFSA v2's ability to bridge semantic gaps.</p> Canonical_Concept FHIR_Representation Shopify_Representation Mapping_Predicate Person/Entity .../fhir_r4_patient#Patient .../shopify_admin_customer#Customer skos:closeMatch Date of Birth .../fhir_r4_patient#Patient.birthDate .../shopify_admin_customer#dateOfBirth skos:exactMatch Geographic Location .../fhir_r4_patient#Patient.address .../shopify_admin_order#shippingAddress skos:broadMatch Identifier .../fhir_r4_patient#Patient.identifier .../shopify_admin_customer#id skos:relatedMatch"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#section-v-the-generative-research-prompt-for-ufsa-v2-materialization","title":"Section V: The Generative Research Prompt for UFSA v2 Materialization","text":"<p>This section synthesizes the preceding technical specification into a single, self-contained, and executable prompt. It is designed to be provided to an advanced code generation system or a human research engineer to materialize the complete UFSA v2 initial implementation without reference to any external documents. It contains the system's objectives, architectural principles, formal metamodel, declarative inputs, complete source code, output specifications, and execution protocol.</p>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#ufsa-v2-materialization-prompt","title":"UFSA v2 Materialization Prompt","text":""},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#1-system-objective","title":"1. System Objective","text":"<p>The objective is to implement and deploy the Universal Financial Standards Adapter, Version 2 (UFSA v2). This system must be a declarative, metadata-driven engine that achieves universal data interoperability across multiple domains (healthcare, e-commerce, finance, etc.). It will operate by reading a declarative configuration file containing URLs to public, machine-readable standards specifications. It will then fetch, parse, and normalize these standards into a unified internal model based on the W3C SKOS vocabulary. The final output will be a set of fully populated, relational data tables (in CSV format) that represent a comprehensive map of the concepts and semantic relationships within and between the processed standards. The entire implementation must be provided as a set of Python scripts and configuration files, ready for immediate execution.</p>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#2-core-architectural-principles","title":"2. Core Architectural Principles","text":"<ul> <li>Declarative Mandate (Configuration over Code): The system's behavior must be driven by external configuration files, not by hard-coded, imperative logic. The core engine must be agnostic to the specifics of any standard. System extension must be achievable by modifying configuration only.  </li> <li>Federated Governance Model: The system will treat each external standard as an autonomous \"domain.\" It will provide the central \"connective tissue\" for interoperability by normalizing domain-specific metadata against a global metamodel, without attempting to control the external standards themselves.  </li> <li>Pluggable, Modular Architecture: The system must feature a clean separation of concerns with distinct, dynamically-loaded modules for parsing heterogeneous input formats and for emitting various output formats.</li> </ul>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#3-metamodel-specification","title":"3. Metamodel Specification","text":"<p>The internal data model for UFSA v2 is an extension of the W3C SKOS vocabulary. The following is the formal definition of the UFSA v2 metamodel in Turtle syntax. The engine's internal knowledge graph must conform to this schema.</p> <pre><code>@prefix rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns\\#.  \n@prefix rdfs: http://www.w3.org/2000/01/rdf-schema\\#.  \n@prefix skos: http://www.w3.org/2004/02/skos/core\\#.  \n@prefix dct: http://purl.org/dc/terms/.  \n@prefix ufsa: http://ufsa.org/v2/schema\\#.  \nufsa:StandardsBody a rdfs:Class ;  \nrdfs:label \"Standards Body\" ;  \nrdfs:comment \"An organization that defines, maintains, or governs a standard.\".  \nufsa:Standard a rdfs:Class ;  \nrdfs:subClassOf skos:ConceptScheme ;  \nrdfs:label \"Standard\" ;  \nrdfs:comment \"A formal specification, such as an API definition, data format, or controlled vocabulary.\".  \nufsa:governedBy a rdf:Property ;  \nrdfs:domain ufsa:Standard ;  \nrdfs:range ufsa:StandardsBody ;  \nrdfs:label \"Governed By\".  \nufsa:specificationURL a rdf:Property ;  \nrdfs:domain ufsa:Standard ;  \nrdfs:range rdfs:Resource ;  \nrdfs:label \"Specification URL\".  \nufsa:dataFormat a rdf:Property ;  \nrdfs:domain ufsa:Standard ;  \nrdfs:range rdfs:Literal ;  \nrdfs:label \"Data Format\".  \nufsa:parserModule a rdf:Property ;  \nrdfs:domain ufsa:Standard ;  \nrdfs:range rdfs:Literal ;  \nrdfs:label \"Parser Module\".\n</code></pre> <p>## 4. Declarative Input: Standards Body Pointer Registry</p> <p>Create a file named `config/pointer_registry.csv` with the following content. This file is the sole declarative input that drives the entire engine.</p> <p>```csv Standard_ID,Standard_Name,Governing_Body,Specification_URL,Data_Format,Parser_Module,Canonical_Concept_Scheme_URI fhir_r4_patient,HL7 FHIR R4 Patient Resource,HL7 International,[http://build.fhir.org/patient.schema.json,JSON-Schema,parsers.json_schema_parser,http://ufsa.org/v2/standards/fhir_r4_patient](http://build.fhir.org/patient.schema.json,JSON-Schema,parsers.json_schema_parser,http://ufsa.org/v2/standards/fhir_r4_patient) fhir_r4_observation,HL7 FHIR R4 Observation Resource,HL7 International,[http://build.fhir.org/observation.schema.json,JSON-Schema,parsers.json_schema_parser,http://ufsa.org/v2/standards/fhir_r4_observation](http://build.fhir.org/observation.schema.json,JSON-Schema,parsers.json_schema_parser,http://ufsa.org/v2/standards/fhir_r4_observation) shopify_admin_product,Shopify Admin GraphQL API Product Object,Shopify,[https://shopify.dev/docs/api/admin-graphql/latest/queries/product,HTML_GraphQL_Spec,parsers.shopify_graphql_parser,http://ufsa.org/v2/standards/shopify_admin_product](https://shopify.dev/docs/api/admin-graphql/latest/queries/product,HTML_GraphQL_Spec,parsers.shopify_graphql_parser,http://ufsa.org/v2/standards/shopify_admin_product) shopify_admin_order,Shopify Admin GraphQL API Order Object,Shopify,[https://shopify.dev/docs/api/admin-graphql/latest/queries/order,HTML_GraphQL_Spec,parsers.shopify_graphql_parser,http://ufsa.org/v2/standards/shopify_admin_order](https://shopify.dev/docs/api/admin-graphql/latest/queries/order,HTML_GraphQL_Spec,parsers.shopify_graphql_parser,http://ufsa.org/v2/standards/shopify_admin_order) openfigi_v3,OpenFIGI API v3,\"OMG / Bloomberg L.P.\",[https://www.openfigi.com/api/documentation,HTML_REST_API_Spec,parsers.openfigi_api_parser,http://ufsa.org/v2/standards/openfigi_v3](https://www.openfigi.com/api/documentation,HTML_REST_API_Spec,parsers.openfigi_api_parser,http://ufsa.org/v2/standards/openfigi_v3) iso_3166_1_a2,ISO 3166-1 Alpha-2 Country Codes,ISO,[https://datahub.io/core/country-list/r/data.csv,CSV,parsers.csv_parser,http://ufsa.org/v2/standards/iso_3166_1_a2](https://datahub.io/core/country-list/r/data.csv,CSV,parsers.csv_parser,http://ufsa.org/v2/standards/iso_3166_1_a2) iana_mime_app,IANA MIME Types (Application),IETF / IANA,[https://www.iana.org/assignments/media-types/application.csv,CSV,parsers.iana_csv_parser,http://ufsa.org/v2/standards/iana_mime_application](https://www.iana.org/assignments/media-types/application.csv,CSV,parsers.iana_csv_parser,http://ufsa.org/v2/standards/iana_mime_application) w3c_skos_core,W3C SKOS Core Vocabulary,W3C,[http://www.w3.org/2004/02/skos/core.rdf,RDF/XML,parsers.rdf_parser,http://www.w3.org/2004/02/skos/core#](http://www.w3.org/2004/02/skos/core.rdf,RDF/XML,parsers.rdf_parser,http://www.w3.org/2004/02/skos/core#)</p>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#5-implementation-specification-python-engine","title":"5. Implementation Specification: Python Engine","text":""},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#51-project-directory-structure","title":"5.1. Project Directory Structure","text":"<p>Create the following directory and file structure:</p> <p>ufsa_v2/ \u251c\u2500\u2500 main.py \u251c\u2500\u2500 engine.py \u251c\u2500\u2500 config/ \u2502   \u2514\u2500\u2500 pointer_registry.csv \u251c\u2500\u2500 parsers/ \u2502   \u251c\u2500\u2500 __init__.py \u2502   \u251c\u2500\u2500 base_parser.py \u2502   \u251c\u2500\u2500 csv_parser.py \u2502   \u251c\u2500\u2500 iana_csv_parser.py \u2502   \u251c\u2500\u2500 json_schema_parser.py \u2502   \u251c\u2500\u2500 openfigi_api_parser.py \u2502   \u251c\u2500\u2500 rdf_parser.py \u2502   \u2514\u2500\u2500 shopify_graphql_parser.py \u2514\u2500\u2500 emitters/     \u251c\u2500\u2500 __init__.py     \u251c\u2500\u2500 base_emitter.py     \u251c\u2500\u2500 csv_emitter.py     \u2514\u2500\u2500 sql_emitter.py</p>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#52-source-code","title":"5.2. Source Code","text":"<p>Populate the files with the following Python code.</p> <p>File: ufsa_v2/main.py</p> <p>Python</p> <p>import argparse import os import sys from engine import UFSAEngine</p> <p>def main():     \"\"\"Main entry point for the UFSA v2 command-line interface.\"\"\"     parser \\= argparse.ArgumentParser(         description=\"UFSA v2: Universal Financial Standards Adapter Engine.\"     )     parser.add_argument(         \"--config\",         default=\"config/pointer_registry.csv\",         help\\=\"Path to the pointer registry CSV file.\",     )     parser.add_argument(         \"--output_dir\",         default=\"output\",         help\\=\"Directory to save the generated tables.\",     )     parser.add_argument(         \"--emitter\",         default=\"csv\",         choices=[\"csv\", \"sql\"],         help\\=\"The output format for the generated tables.\",     )     args \\= parser.parse_args()</p> <pre><code>print(\"--- Starting UFSA v2 Engine \\---\")\n\nif not os.path.exists(args.output\\_dir):  \n    os.makedirs(args.output\\_dir)  \n    print(f\"Created output directory: {args.output\\_dir}\")\n\ntry:  \n    engine \\= UFSAEngine(config\\_path=args.config)  \n    engine.run(output\\_dir=args.output\\_dir, emitter\\_type=args.emitter)  \n    print(f\"\\\\n--- UFSA v2 processing complete. \\---\")  \n    print(f\"Generated tables are located in: {args.output\\_dir}\")  \nexcept Exception as e:  \n    print(f\"\\\\n--- An error occurred during execution: \\---\", file=sys.stderr)  \n    print(e, file=sys.stderr)  \n    sys.exit(1)\n</code></pre> <p>if __name__ \\== \"__main__\":     main()</p> <p>File: ufsa_v2/engine.py</p> <p>Python</p> <p>import pandas as pd import importlib from rdflib import Graph from emitters.base_emitter import BaseEmitter from parsers.base_parser import BaseParser</p> <p>class UFSAEngine:     \"\"\"Orchestrates the fetching, parsing, and emission of standards data.\"\"\"</p> <pre><code>def \\_\\_init\\_\\_(self, config\\_path: str):  \n    print(f\"Loading configuration from: {config\\_path}\")  \n    self.config \\= pd.read\\_csv(config\\_path)  \n    self.master\\_graph \\= Graph()\n\ndef \\_get\\_instance(self, module\\_path: str, class\\_name: str, base\\_class):  \n    \"\"\"Dynamically imports and instantiates a class from a module.\"\"\"  \n    try:  \n        module \\= importlib.import\\_module(module\\_path)  \n        class\\_ \\= getattr(module, class\\_name)  \n        if not issubclass(class\\_, base\\_class):  \n            raise TypeError(f\"{class\\_name} must be a subclass of {base\\_class.\\_\\_name\\_\\_}\")  \n        return class\\_()  \n    except (ImportError, AttributeError) as e:  \n        raise ImportError(f\"Could not load class '{class\\_name}' from module '{module\\_path}': {e}\")\n\ndef run(self, output\\_dir: str, emitter\\_type: str):  \n    \"\"\"Executes the full ingestion, parsing, and emission pipeline.\"\"\"  \n    print(f\"\\\\nFound {len(self.config)} standards to process.\")\n\n    for index, row in self.config.iterrows():  \n        standard\\_id \\= row  \n        print(f\"\\\\nProcessing standard: {row} ({standard\\_id})...\")\n\n        parser\\_module\\_path \\= row\\['Parser\\_Module'\\]  \n        parser\\_class\\_name \\= ''.join(word.capitalize() for word in parser\\_module\\_path.split('.')\\[-1\\].split('\\_'))\n\n        try:  \n            parser \\= self.\\_get\\_instance(parser\\_module\\_path, parser\\_class\\_name, BaseParser)  \n            standard\\_graph \\= parser.parse(  \n                url=row,  \n                concept\\_scheme\\_uri=row,  \n                standard\\_info=row.to\\_dict()  \n            )  \n            self.master\\_graph \\+= standard\\_graph  \n            print(f\"Successfully parsed and merged '{standard\\_id}'. Graph now contains {len(self.master\\_graph)} triples.\")  \n        except Exception as e:  \n            print(f\"ERROR: Failed to process standard '{standard\\_id}'. Reason: {e}\")  \n            continue\n\n    print(f\"\\\\n--- Starting emission process \\---\")  \n    print(f\"Total triples in master graph: {len(self.master\\_graph)}\")\n\n    emitter\\_module\\_path \\= f\"emitters.{emitter\\_type}\\_emitter\"  \n    emitter\\_class\\_name \\= f\"{emitter\\_type.upper()}Emitter\"\n\n    try:  \n        emitter \\= self.\\_get\\_instance(emitter\\_module\\_path, emitter\\_class\\_name, BaseEmitter)  \n        emitter.emit(self.master\\_graph, output\\_dir)  \n    except Exception as e:  \n        print(f\"ERROR: Failed to emit tables. Reason: {e}\")\n</code></pre> <p>File: ufsa_v2/parsers/__init__.py</p> <p>Python</p> <p># Leave this file empty.</p> <p>File: ufsa_v2/parsers/base_parser.py</p> <p>Python</p> <p>from abc import ABC, abstractmethod from rdflib import Graph</p> <p>class BaseParser(ABC):     \"\"\"Abstract base class for all standard parsers.\"\"\"     @abstractmethod     def parse(self, url: str, concept_scheme_uri: str, standard_info: dict) -&gt; Graph:         \"\"\"         Fetches a standard from a URL, parses it, and returns an RDF graph         conformant with the UFSA SKOS-based metamodel.         \"\"\"         pass</p> <p>File: ufsa_v2/parsers/csv_parser.py</p> <p>Python</p> <p>import pandas as pd from rdflib import Graph, URIRef, Literal from rdflib.namespace import SKOS, RDF, DCTERMS from.base_parser import BaseParser</p> <p>class CsvParser(BaseParser):     \"\"\"Parses a generic two-column CSV (Name, Code) into SKOS concepts.\"\"\"     def parse(self, url: str, concept_scheme_uri: str, standard_info: dict) -&gt; Graph:         g \\= Graph()         scheme \\= URIRef(concept_scheme_uri)         g.add((scheme, RDF.type, SKOS.ConceptScheme))         g.add((scheme, SKOS.prefLabel, Literal(standard_info.get('Standard_Name'))))         g.add((scheme, DCTERMS.creator, Literal(standard_info.get('Governing_Body'))))</p> <pre><code>    df \\= pd.read\\_csv(url)  \n    if 'Name' not in df.columns or 'Code' not in df.columns:  \n        raise ValueError(\"CSV must contain 'Name' and 'Code' columns for CsvParser.\")\n\n    for \\_, row in df.iterrows():  \n        name, code \\= str(row\\['Name'\\]), str(row\\['Code'\\])  \n        concept\\_uri \\= URIRef(f\"{concept\\_scheme\\_uri}/{code.replace(' ', '\\_')}\")  \n        g.add((concept\\_uri, RDF.type, SKOS.Concept))  \n        g.add((concept\\_uri, SKOS.inScheme, scheme))  \n        g.add((concept\\_uri, SKOS.prefLabel, Literal(name)))  \n        g.add((concept\\_uri, SKOS.notation, Literal(code)))  \n    return g\n</code></pre> <p>File: ufsa_v2/parsers/iana_csv_parser.py</p> <p>Python</p> <p>import pandas as pd from rdflib import Graph, URIRef, Literal from rdflib.namespace import SKOS, RDF, DCTERMS from.base_parser import BaseParser</p> <p>class IanaCsvParser(BaseParser):     \"\"\"Parses IANA MIME type CSVs (Name, Template) into SKOS concepts.\"\"\"     def parse(self, url: str, concept_scheme_uri: str, standard_info: dict) -&gt; Graph:         g \\= Graph()         scheme \\= URIRef(concept_scheme_uri)         g.add((scheme, RDF.type, SKOS.ConceptScheme))         g.add((scheme, SKOS.prefLabel, Literal(standard_info.get('Standard_Name'))))         g.add((scheme, DCTERMS.creator, Literal(standard_info.get('Governing_Body'))))</p> <pre><code>    df \\= pd.read\\_csv(url)  \n    if 'Name' not in df.columns or 'Template' not in df.columns:  \n        raise ValueError(\"CSV must contain 'Name' and 'Template' columns for IanaCsvParser.\")\n\n    for \\_, row in df.iterrows():  \n        name, template \\= str(row\\['Name'\\]), str(row)  \n        if pd.isna(name) or pd.isna(template): continue\n\n        concept\\_uri\\_slug \\= name.replace('/', '\\_').replace('+', '\\_')  \n        concept\\_uri \\= URIRef(f\"{concept\\_scheme\\_uri}/{concept\\_uri\\_slug}\")  \n        g.add((concept\\_uri, RDF.type, SKOS.Concept))  \n        g.add((concept\\_uri, SKOS.inScheme, scheme))  \n        g.add((concept\\_uri, SKOS.prefLabel, Literal(name)))  \n        g.add((concept\\_uri, SKOS.notation, Literal(template)))  \n    return g\n</code></pre> <p>File: ufsa_v2/parsers/json_schema_parser.py</p> <p>Python</p> <p>import requests from rdflib import Graph, URIRef, Literal from rdflib.namespace import SKOS, RDF, RDFS, DCTERMS from.base_parser import BaseParser</p> <p>class JsonSchemaParser(BaseParser):     \"\"\"Parses a JSON Schema into a hierarchical SKOS concept graph.\"\"\"     def parse(self, url: str, concept_scheme_uri: str, standard_info: dict) -&gt; Graph:         g \\= Graph()         self.scheme \\= URIRef(concept_scheme_uri)         g.add((self.scheme, RDF.type, SKOS.ConceptScheme))         g.add((self.scheme, SKOS.prefLabel, Literal(standard_info.get('Standard_Name'))))         g.add((self.scheme, DCTERMS.creator, Literal(standard_info.get('Governing_Body'))))</p> <pre><code>    response \\= requests.get(url)  \n    response.raise\\_for\\_status()  \n    schema \\= response.json()\n\n    root\\_definition\\_key \\= standard\\_info.split(' ')\\[-2\\] \\# e.g., 'Patient'  \n    if root\\_definition\\_key in schema.get('definitions', {}):  \n        root\\_schema \\= schema\\['definitions'\\]\\[root\\_definition\\_key\\]  \n        self.\\_traverse\\_properties(g, root\\_schema, None)\n\n    return g\n\ndef \\_traverse\\_properties(self, g: Graph, schema\\_node: dict, parent\\_concept: URIRef):  \n    if 'properties' not in schema\\_node:  \n        return\n\n    for prop\\_name, prop\\_schema in schema\\_node\\['properties'\\].items():  \n        concept\\_uri \\= URIRef(f\"{self.scheme}\\#{prop\\_name}\")  \n        if parent\\_concept:  \n            concept\\_uri \\= URIRef(f\"{parent\\_concept}.{prop\\_name}\")\n\n        g.add((concept\\_uri, RDF.type, SKOS.Concept))  \n        g.add((concept\\_uri, SKOS.inScheme, self.scheme))  \n        g.add((concept\\_uri, SKOS.prefLabel, Literal(prop\\_name)))\n\n        if 'description' in prop\\_schema:  \n            g.add((concept\\_uri, SKOS.definition, Literal(prop\\_schema\\['description'\\])))\n\n        if parent\\_concept:  \n            g.add((concept\\_uri, SKOS.broader, parent\\_concept))\n\n        \\# Recurse for nested objects  \n        if prop\\_schema.get('type') \\== 'object' and 'properties' in prop\\_schema:  \n            self.\\_traverse\\_properties(g, prop\\_schema, concept\\_uri)  \n        elif '$ref' in prop\\_schema:  \n            \\# Basic reference handling (non-recursive for simplicity)  \n            ref\\_name \\= prop\\_schema\\['$ref'\\].split('/')\\[-1\\]  \n            g.add((concept\\_uri, RDFS.seeAlso, Literal(f\"Refers to schema definition: {ref\\_name}\")))\n</code></pre> <p>File: ufsa_v2/parsers/rdf_parser.py</p> <p>Python</p> <p>from rdflib import Graph from.base_parser import BaseParser</p> <p>class RdfParser(BaseParser):     \"\"\"Parses an existing RDF file directly into the graph.\"\"\"     def parse(self, url: str, concept_scheme_uri: str, standard_info: dict) -&gt; Graph:         g \\= Graph()         g.parse(url)         return g</p> <p>File: ufsa_v2/parsers/shopify_graphql_parser.py</p> <p>Python</p> <p>import requests from bs4 import BeautifulSoup from rdflib import Graph, URIRef, Literal from rdflib.namespace import SKOS, RDF, DCTERMS from.base_parser import BaseParser</p> <p>class ShopifyGraphqlParser(BaseParser):     \"\"\"Scrapes a Shopify GraphQL documentation page for fields.\"\"\"     def parse(self, url: str, concept_scheme_uri: str, standard_info: dict) -&gt; Graph:         g \\= Graph()         scheme \\= URIRef(concept_scheme_uri)         g.add((scheme, RDF.type, SKOS.ConceptScheme))         g.add((scheme, SKOS.prefLabel, Literal(standard_info.get('Standard_Name'))))         g.add((scheme, DCTERMS.creator, Literal(standard_info.get('Governing_Body'))))</p> <pre><code>    response \\= requests.get(url)  \n    response.raise\\_for\\_status()  \n    soup \\= BeautifulSoup(response.content, 'html.parser')\n\n    \\# This is a brittle selector and likely to break with site updates.  \n    \\# A more robust solution would use a more stable identifier if available.  \n    fields\\_header \\= soup.find(lambda tag: tag.name in \\['h2', 'h3'\\] and 'Fields' in tag.get\\_text())  \n    if not fields\\_header:  \n        raise ValueError(\"Could not find 'Fields' section on documentation page.\")\n\n    table \\= fields\\_header.find\\_next('table')  \n    if not table:  \n        raise ValueError(\"Could not find table following 'Fields' header.\")\n\n    for row in table.find('tbody').find\\_all('tr'):  \n        cols \\= row.find\\_all('td')  \n        if len(cols) \\&gt;= 2:  \n            field\\_name\\_tag \\= cols.find('code')  \n            if not field\\_name\\_tag: continue\n\n            field\\_name \\= field\\_name\\_tag.get\\_text(strip=True)  \n            description \\= cols.\\[1\\]get\\_text(strip=True)\n\n            concept\\_uri \\= URIRef(f\"{concept\\_scheme\\_uri}\\#{field\\_name}\")  \n            g.add((concept\\_uri, RDF.type, SKOS.Concept))  \n            g.add((concept\\_uri, SKOS.inScheme, scheme))  \n            g.add((concept\\_uri, SKOS.prefLabel, Literal(field\\_name)))  \n            g.add((concept\\_uri, SKOS.definition, Literal(description)))  \n    return g\n</code></pre> <p>File: ufsa_v2/parsers/openfigi_api_parser.py</p> <p>Python</p> <p>import requests from bs4 import BeautifulSoup from rdflib import Graph, URIRef, Literal from rdflib.namespace import SKOS, RDF, DCTERMS from.base_parser import BaseParser</p> <p>class OpenfigiApiParser(BaseParser):     \"\"\"Scrapes the OpenFIGI API documentation for response fields.\"\"\"     def parse(self, url: str, concept_scheme_uri: str, standard_info: dict) -&gt; Graph:         g \\= Graph()         scheme \\= URIRef(concept_scheme_uri)         g.add((scheme, RDF.type, SKOS.ConceptScheme))         g.add((scheme, SKOS.prefLabel, Literal(standard_info.get('Standard_Name'))))         g.add((scheme, DCTERMS.creator, Literal(standard_info.get('Governing_Body'))))</p> <pre><code>    response \\= requests.get(url)  \n    response.raise\\_for\\_status()  \n    soup \\= BeautifulSoup(response.content, 'html.parser')\n\n    \\# Find the section describing the response format for the mapping endpoint  \n    response\\_header \\= soup.find('h3', id\\='response-format')  \n    if not response\\_header:  \n        raise ValueError(\"Could not find 'Response Format' section (h3 with id='response-format').\")\n\n    \\# Find the list of properties  \n    prop\\_list \\= response\\_header.find\\_next('ul')  \n    if not prop\\_list:  \n        raise ValueError(\"Could not find property list (ul) after 'Response Format' header.\")\n\n    for item in prop\\_list.find\\_all('li'):  \n        code\\_tag \\= item.find('code')  \n        if code\\_tag:  \n            field\\_name \\= code\\_tag.get\\_text(strip=True)  \n            description \\= ' '.join(item.get\\_text().split(' ')\\[2:\\]).strip() \\# Basic description extraction\n\n            concept\\_uri \\= URIRef(f\"{concept\\_scheme\\_uri}\\#{field\\_name}\")  \n            g.add((concept\\_uri, RDF.type, SKOS.Concept))  \n            g.add((concept\\_uri, SKOS.inScheme, scheme))  \n            g.add((concept\\_uri, SKOS.prefLabel, Literal(field\\_name)))  \n            g.add((concept\\_uri, SKOS.definition, Literal(description)))  \n    return g\n</code></pre> <p>File: ufsa_v2/emitters/__init__.py</p> <p>Python</p> <p># Leave this file empty.</p> <p>File: ufsa_v2/emitters/base_emitter.py</p> <p>Python</p> <p>from abc import ABC, abstractmethod from rdflib import Graph</p> <p>class BaseEmitter(ABC):     \"\"\"Abstract base class for all output emitters.\"\"\"     @abstractmethod     def emit(self, graph: Graph, output_dir: str):         \"\"\"         Takes the final RDF graph and writes it to a set of files         in a specific format.         \"\"\"         pass</p> <p>File: ufsa_v2/emitters/csv_emitter.py</p> <p>Python</p> <p>import pandas as pd import os from rdflib import Graph from rdflib.namespace import SKOS from.base_emitter import BaseEmitter</p> <p>class CsvEmitter(BaseEmitter):     \"\"\"Emits the master knowledge graph into a set of CSV files.\"\"\"     def emit(self, graph: Graph, output_dir: str):         # 1. Generate concept_schemes.csv         schemes_query \\= \"\"\"             SELECT?scheme_uri?pref_label?creator             WHERE {                ?scheme_uri a skos:ConceptScheme.                 OPTIONAL {?scheme_uri skos:prefLabel?pref_label. }                 OPTIONAL {?scheme_uri [http://purl.org/dc/terms/creator](http://purl.org/dc/terms/creator)?creator. }             }         \"\"\"         schemes_data \\= [{             \"scheme_uri\": str(r.scheme_uri),             \"pref_label\": str(r.pref_label),             \"governing_body\": str(r.creator)         } for r in graph.query(schemes_query)]         schemes_df \\= pd.DataFrame(schemes_data).drop_duplicates().sort_values('scheme_uri')         schemes_df.to_csv(os.path.join(output_dir, \"concept_schemes.csv\"), index=False)         print(f\"Generated concept_schemes.csv with {len(schemes_df)} rows.\")</p> <pre><code>    \\# 2\\. Generate concepts.csv  \n    concepts\\_query \\= \"\"\"  \n        SELECT?concept\\_uri?pref\\_label?definition?notation?scheme\\_uri  \n        WHERE {  \n           ?concept\\_uri a skos:Concept ; skos:inScheme?scheme\\_uri.  \n            OPTIONAL {?concept\\_uri skos:prefLabel?pref\\_label. }  \n            OPTIONAL {?concept\\_uri skos:definition?definition. }  \n            OPTIONAL {?concept\\_uri skos:notation?notation. }  \n        }  \n    \"\"\"  \n    concepts\\_data \\= \\[{  \n        \"concept\\_uri\": str(r.concept\\_uri), \"pref\\_label\": str(r.pref\\_label),  \n        \"definition\": str(r.definition), \"notation\": str(r.notation),  \n        \"scheme\\_uri\": str(r.scheme\\_uri)  \n    } for r in graph.query(concepts\\_query)\\]  \n    concepts\\_df \\= pd.DataFrame(concepts\\_data).drop\\_duplicates().sort\\_values('concept\\_uri')  \n    concepts\\_df.to\\_csv(os.path.join(output\\_dir, \"concepts.csv\"), index=False)  \n    print(f\"Generated concepts.csv with {len(concepts\\_df)} rows.\")\n\n    \\# 3\\. Generate semantic\\_relations.csv  \n    relations\\_query \\= \"\"\"  \n        SELECT?subject\\_uri?predicate?object\\_uri  \n        WHERE {  \n           ?subject\\_uri?predicate?object\\_uri.  \n            FILTER(?predicate IN (skos:broader, skos:narrower, skos:related,   \n                                  skos:exactMatch, skos:closeMatch,   \n                                  skos:broadMatch, skos:narrowMatch, skos:relatedMatch))  \n        }  \n    \"\"\"  \n    relations\\_data \\=  \n    relations\\_df \\= pd.DataFrame(relations\\_data).drop\\_duplicates().sort\\_values(\\['subject\\_uri', 'predicate', 'object\\_uri'\\])  \n    relations\\_df.to\\_csv(os.path.join(output\\_dir, \"semantic\\_relations.csv\"), index=False)  \n    print(f\"Generated semantic\\_relations.csv with {len(relations\\_df)} rows.\")\n</code></pre> <p>File: ufsa_v2/emitters/sql_emitter.py</p> <p>Python</p> <p>import os from rdflib import Graph from.base_emitter import BaseEmitter import pandas as pd</p> <p>class SqlEmitter(BaseEmitter):     \"\"\"Emits the master knowledge graph as SQL INSERT statements.\"\"\"     def emit(self, graph: Graph, output_dir: str):         # This is a simplified implementation. A robust version would handle         # SQL escaping, different dialects, and schema creation more formally.  </p> <pre><code>    \\# Re-use the CSV Emitter's logic to get DataFrames  \n    from.csv\\_emitter import CsvEmitter  \n    temp\\_dir \\= os.path.join(output\\_dir, \"temp\\_sql\\_gen\")  \n    if not os.path.exists(temp\\_dir):  \n        os.makedirs(temp\\_dir)\n\n    csv\\_emitter \\= CsvEmitter()  \n    csv\\_emitter.emit(graph, temp\\_dir)\n\n    sql\\_script \\= \"\"\n\n    \\# Schema Creation  \n    sql\\_script \\+= \"\"\"\n</code></pre> <p>-- UFSA v2 Schema Definition CREATE TABLE concept_schemes (     scheme_uri VARCHAR(255) PRIMARY KEY,     pref_label TEXT,     governing_body VARCHAR(255) );</p> <p>CREATE TABLE concepts (     concept_uri VARCHAR(255) PRIMARY KEY,     pref_label TEXT,     definition TEXT,     notation TEXT,     scheme_uri VARCHAR(255),     FOREIGN KEY (scheme_uri) REFERENCES concept_schemes(scheme_uri) );</p> <p>CREATE TABLE semantic_relations (     subject_uri VARCHAR(255),     predicate VARCHAR(255),     object_uri VARCHAR(255),     PRIMARY KEY (subject_uri, predicate, object_uri),     FOREIGN KEY (subject_uri) REFERENCES concepts(concept_uri),     FOREIGN KEY (object_uri) REFERENCES concepts(concept_uri) );</p> <p>-- Data Insertion \"\"\"</p> <pre><code>    \\# Data Insertion  \n    for table\\_name in \\[\"concept\\_schemes\", \"concepts\", \"semantic\\_relations\"\\]:  \n        df \\= pd.read\\_csv(os.path.join(temp\\_dir, f\"{table\\_name}.csv\"))  \n        df \\= df.where(pd.notnull(df), None) \\# Replace NaN with None for SQL NULL  \n        for \\_, row in df.iterrows():  \n            cols \\= ', '.join(row.index)  \n            vals \\= ', '.join(\\[f\"'{str(v).replace(\\\\\"'\\\\\", \\\\\"''\\\\\")}'\" if v is not None else \"NULL\" for v in row.values\\])  \n            sql\\_script \\+= f\"INSERT INTO {table\\_name} ({cols}) VALUES ({vals});\\\\n\"  \n        sql\\_script \\+= \"\\\\n\"\n\n    with open(os.path.join(output\\_dir, \"ufsa\\_v2\\_data.sql\"), \"w\", encoding=\"utf-8\") as f:  \n        f.write(sql\\_script)\n\n    print(f\"Generated ufsa\\_v2\\_data.sql script.\")\n\n    \\# Clean up temp files  \n    for f in os.listdir(temp\\_dir):  \n        os.remove(os.path.join(temp\\_dir, f))  \n    os.rmdir(temp\\_dir)\n</code></pre>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#6-output-specification-data-tables","title":"6. Output Specification: Data Tables","text":"<p>The successful execution of the engine will produce three primary CSV files in the specified output directory. These tables are relationally linked and provide a complete, structured representation of the ingested standards.</p>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#61-table-schemas","title":"6.1. Table Schemas","text":"<ul> <li>concept_schemes.csv:  </li> <li>scheme_uri (String, Primary Key)  </li> <li>pref_label (String)  </li> <li>governing_body (String)  </li> <li>concepts.csv:  </li> <li>concept_uri (String, Primary Key)  </li> <li>pref_label (String)  </li> <li>definition (Text)  </li> <li>notation (String)  </li> <li>scheme_uri (String, Foreign Key to concept_schemes.scheme_uri)  </li> <li>semantic_relations.csv:  </li> <li>subject_uri (String, Foreign Key to concepts.concept_uri)  </li> <li>predicate (String, URI of the SKOS property)  </li> <li>object_uri (String, Foreign Key to concepts.concept_uri)</li> </ul>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#62-expected-output-example-for-iso-3166-1","title":"6.2. Expected Output Example (for ISO 3166-1)","text":"<p>concept_schemes.csv (sample row):</p> <p>Code snippet</p> <p>scheme_uri,pref_label,governing_body [http://ufsa.org/v2/standards/iso_3166_1_a2,ISO](http://ufsa.org/v2/standards/iso_3166_1_a2,ISO) 3166-1 Alpha-2 Country Codes,ISO</p> <p>concepts.csv (sample rows):</p> <p>Code snippet</p> <p>concept_uri,pref_label,definition,notation,scheme_uri [http://ufsa.org/v2/standards/iso_3166_1_a2/US,United](http://ufsa.org/v2/standards/iso_3166_1_a2/US,United) States of America,,US,[http://ufsa.org/v2/standards/iso_3166_1_a2](http://ufsa.org/v2/standards/iso_3166_1_a2) [http://ufsa.org/v2/standards/iso_3166_1_a2/CA,Canada,,CA,http://ufsa.org/v2/standards/iso_3166_1_a2](http://ufsa.org/v2/standards/iso_3166_1_a2/CA,Canada,,CA,http://ufsa.org/v2/standards/iso_3166_1_a2)</p> <p>semantic_relations.csv: (This table would be empty for the ISO 3166-1 standard alone, as it is a flat list. It would be populated by hierarchical standards like FHIR.)</p>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#7-execution-and-verification-protocol","title":"7. Execution and Verification Protocol","text":"<ol> <li>Setup Environment: </li> <li>Ensure Python 3.8+ is installed.  </li> <li> <p>Create a virtual environment:      Bash      python -m venv venv      source venv/bin/activate  # On Windows: venv\\Scripts\\activate</p> </li> <li> <p>Install Dependencies: </p> </li> <li> <p>Create a requirements.txt file with the following content:      pandas      rdflib      requests      beautifulsoup4      lxml</p> </li> <li> <p>Install the dependencies:      Bash      pip install -r requirements.txt</p> </li> <li> <p>Run the Engine: </p> </li> <li>Navigate to the ufsa_v2 directory.  </li> <li> <p>Execute the main script:      Bash      python main.py --output_dir./output --emitter csv</p> </li> <li> <p>Verification: </p> </li> <li>Upon successful completion, an output/ directory will be created.  </li> <li>Verify that the directory contains three non-empty CSV files: concept_schemes.csv, concepts.csv, and semantic_relations.csv.  </li> <li>Inspect the contents of the files to confirm they contain the parsed data from the standards defined in pointer_registry.csv.</li> </ol>"},{"location":"research/1_UFSA%20v2.0%20Registry%20and%20Emitter%20Design/#works-cited","title":"Works cited","text":"<ol> <li>The Data Engineers Guide to Declarative vs Imperative for Data - DataOps.live, accessed on August 24, 2025, https://www.dataops.live/blog/the-data-engineers-guide-to-declarative-vs-imperative-for-data </li> <li>Explained: Imperative vs Declarative programming - DEV Community, accessed on August 24, 2025, https://dev.to/siddharthshyniben/explained-imperative-vs-declarative-programming-577g </li> <li>What are some examples of imperative vs. declarative programming? - Quora, accessed on August 24, 2025, https://www.quora.com/What-are-some-examples-of-imperative-vs-declarative-programming </li> <li>Imperative vs Declarative Programming - Reddit, accessed on August 24, 2025, https://www.reddit.com/r/programming/comments/rv9np3/imperative_vs_declarative_programming/ </li> <li>Business logic - Wikipedia, accessed on August 24, 2025, https://en.wikipedia.org/wiki/Business_logic </li> <li>Patterns for Taming Complex Business Logic | by \u00dcrgo Ringo | Inbank product &amp; engineering | Medium, accessed on August 24, 2025, https://medium.com/inbank-product-and-engineering/patterns-for-taming-complex-business-logic-8b63c0b98882 </li> <li>Declarative vs. Imperative Programming: 4 Key Differences | Codefresh, accessed on August 24, 2025, https://codefresh.io/learn/infrastructure-as-code/declarative-vs-imperative-programming-4-key-differences/ </li> <li>How to model Business Logic using Configuration - VI Company, accessed on August 24, 2025, https://www.vicompany.nl/en/insights/how-to-model-business-logic-using-configuration </li> <li>Patterns in Practice - Convention Over Configuration | Microsoft Learn, accessed on August 24, 2025, https://learn.microsoft.com/en-us/archive/msdn-magazine/2009/february/patterns-in-practice-convention-over-configuration </li> <li>Cognitive Load is what matters - GitHub, accessed on August 24, 2025, https://github.com/zakirullin/cognitive-load </li> <li>The Cognitive Load Theory in Software Development - The Valuable Dev, accessed on August 24, 2025, https://thevaluable.dev/cognitive-load-theory-software-developer/ </li> <li>That's not an abstraction, that's just a layer of indirection - fhur, accessed on August 24, 2025, https://fhur.me/posts/2024/thats-not-an-abstraction </li> <li>Death by design patterns, or On the cognitive load of abstractions in the code - Hacker News, accessed on August 24, 2025, https://news.ycombinator.com/item?id=36118093 </li> <li>Cognitive Load For Developers : r/programming - Reddit, accessed on August 24, 2025, https://www.reddit.com/r/programming/comments/192cwgw/cognitive_load_for_developers/ </li> <li>Cognitive load is what matters | Hacker News, accessed on August 24, 2025, https://news.ycombinator.com/item?id=42489645 </li> <li>Federated Data Governance Explained - Alation, accessed on August 24, 2025, https://www.alation.com/blog/federated-data-governance-explained/ </li> <li>www.alation.com, accessed on August 24, 2025, https://www.alation.com/blog/federated-data-governance-explained/#:\\~:text=Federated%20data%20governance%20is%20a,governance%20principles%20with%20decentralized%20execution. </li> <li>Federated Data Governance: Ultimate Guide for 2024 - Atlan, accessed on August 24, 2025, https://atlan.com/know/data-governance/federated-data-governance/ </li> <li>Understand Data Governance Models: Centralized, Decentralized &amp; Federated | Alation, accessed on August 24, 2025, https://www.alation.com/blog/understand-data-governance-models-centralized-decentralized-federated/ </li> <li>Implementing Federated Governance in Data Mesh Architecture - MDPI, accessed on August 24, 2025, https://www.mdpi.com/1999-5903/16/4/115 </li> <li>Simple Knowledge Organization System - Wikipedia, accessed on August 24, 2025, https://en.wikipedia.org/wiki/Simple_Knowledge_Organization_System </li> <li>What's SKOS, What's not, Why and What Should be Done About It - NKOS, accessed on August 24, 2025, https://nkos.dublincore.org/ASIST2015/ASISTBusch-SKOS.pdf </li> <li>SKOS Simple Knowledge Organization System Primer - W3C, accessed on August 24, 2025, https://www.w3.org/TR/skos-primer/ </li> <li>SKOS Core Vocabulary Specification - W3C, accessed on August 24, 2025, https://www.w3.org/TR/swbp-skos-core-spec/ </li> <li>SKOS Simple Knowledge Organization System Reference - W3C, accessed on August 24, 2025, https://www.w3.org/TR/2008/WD-skos-reference-20080125/ </li> <li>RDF Vocabularies - SKOS Simple Knowledge Organization System, accessed on August 24, 2025, https://www.w3.org/2004/02/skos/vocabs </li> <li>SKOS Simple Knowledge Organization System RDF Schema - W3C, accessed on August 24, 2025, https://www.w3.org/2008/05/skos </li> <li>Patient - FHIR v6.0.0-ballot2, accessed on August 24, 2025, http://build.fhir.org/patient.schema.json.html </li> <li>List of all countries with their 2 digit codes (ISO 3166-1) - DataHub.io, accessed on August 24, 2025, https://datahub.io/core/country-list </li> <li>product - GraphQL Admin - Shopify developer documentation, accessed on August 24, 2025, https://shopify.dev/docs/api/admin-graphql/latest/queries/product </li> <li>Documentation | OpenFIGI, accessed on August 24, 2025, https://www.openfigi.com/api/documentation </li> <li>Resource - FHIR v6.0.0-ballot2, accessed on August 24, 2025, https://build.fhir.org/resource.schema.json.html </li> <li>Observation - FHIR v6.0.0-ballot2, accessed on August 24, 2025, https://build.fhir.org/observation.schema.json.html </li> <li>FHIR Observation - Conduct Science, accessed on August 24, 2025, https://conductscience.com/digital-health/fhir-observation/ </li> <li>Observation resource - Veradigm Developer Portal, accessed on August 24, 2025, https://developer.veradigm.com/content/fhir/Resources/DSTU2/Observation.html </li> <li>Observation - FHIR v6.0.0-ballot3, accessed on August 24, 2025, https://build.fhir.org/observation.html </li> <li>order - GraphQL Admin - Shopify developer documentation, accessed on August 24, 2025, https://shopify.dev/docs/api/admin-graphql/latest/queries/order </li> <li>Order - Shopify developer documentation, accessed on August 24, 2025, https://shopify.dev/docs/api/admin-rest/latest/resources/order </li> <li>orders - GraphQL Admin - Shopify developer documentation, accessed on August 24, 2025, https://shopify.dev/docs/api/admin-graphql/latest/queries/orders </li> <li>Financial Instrument Global Identifier - Wikipedia, accessed on August 24, 2025, https://en.wikipedia.org/wiki/Financial_Instrument_Global_Identifier </li> <li>Overview | OpenFIGI, accessed on August 24, 2025, https://www.openfigi.com/about/overview </li> <li>FINANCIAL INSTRUMENT GLOBAL IDENTIFIER \u2122 - Bloomberg, accessed on August 24, 2025, https://assets.bwbx.io/documents/users/iqjWHBFdfxIU/rKmKGovTFMFo/v0 </li> <li>How FIGI relates to other standards in the space - OMG Issue Tracker, accessed on August 24, 2025, https://issues.omg.org/issues/FIGI-20 </li> <li>Battle Between ISIN and FIGI Codes - ISIN, CUSIP, LEI, SEDOL, WKN, CFI Codes, Database Securities Apply Application Register, accessed on August 24, 2025, https://www.isin.com/battle-between-isin-and-figi-codes/ </li> <li>20240923 FDTA Response to Proposed Rule ... - FHFA, accessed on August 24, 2025, https://www.fhfa.gov/sites/default/files/2024-09/20240923%20FDTA%20Response%20to%20Proposed%20Rule%20FHFA.docx </li> <li>Comments on Financial Data Transparency Act Joint Data Standards Under the Financial Data Transparency Act of 2022, accessed on August 24, 2025, https://www.federalreserve.gov/apps/proposals/comments/FR-0000-0136-01-C19 </li> <li>SKOS Simple Knowledge Organization System Reference - W3C, accessed on August 24, 2025, https://www.w3.org/TR/skos-reference/ </li> <li>Media Types - Internet Assigned Numbers Authority, accessed on August 24, 2025, https://www.iana.org/assignments/media-types/</li> </ol>"},{"location":"research/profiles_overlays_research/","title":"Profiles and Overlays \u2014 Research Archive","text":"<p>[Research archive of the original content formerly at \"ref/2_Profiles and Overlays.md\".]</p> <p>This page preserves the original exploratory notes on contextual Profiles/Overlays that extend a base schema without duplication. It complements the falsification/refinement narrative.</p>"},{"location":"research/profiles_overlays_research/#profiles-and-overlays-ufsa-v2-stub","title":"Profiles and Overlays (UFSA v2 \u2013 stub)","text":"<p>This short note outlines how Profiles/Overlays will extend the current engine. It complements <code>docs/0_...Falsification &amp; Implementation.md</code> section IV and aligns with the FHIR profiling pattern.</p>"},{"location":"research/profiles_overlays_research/#purpose","title":"Purpose","text":"<p>Allow a base structure to be constrained or extended in a given context without duplicating schemas, e.g.:</p> <ul> <li>Postal vs. geodetic address</li> <li>Common\u2011law vs. civil\u2011law contract</li> <li>Implementation/vendor\u2011specific tweaks</li> </ul>"},{"location":"research/profiles_overlays_research/#shape-proposed","title":"Shape (proposed)","text":"<ul> <li>Target: reference to a base structure or scheme element</li> <li>Constraints: cardinality flips, additional validations (regex, external authority checks), enums, bounds</li> <li>Extensions: add fields/notes that only apply in the profile\u2019s context</li> <li>Activation context: labels/conditions describing when to apply (jurisdiction, domain, use\u2011case)</li> </ul>"},{"location":"research/profiles_overlays_research/#minimal-representation-initial","title":"Minimal representation (initial)","text":"<p>Profiles can start as simple YAML atop the registry:</p> <pre><code>kind: Profile\napiVersion: ufsa.org/v2.0\nmetadata:\n  name: PostalAddressProfile\n  domain: core\n  target: core:Address\nspec:\n  constraints:\n    - path: postalCode\n      cardinality: required\n      validation:\n        - type: external\n          authority: usps.com/validate\n</code></pre>"},{"location":"research/profiles_overlays_research/#integration-points","title":"Integration points","text":"<ul> <li>Parsing stays unchanged; profiles are overlays applied post\u2011normalization</li> <li>Emitters may add profile\u2011annotated variants or output validation hints</li> <li>Tracker/plan: tasks to add priority profiles per domain</li> </ul>"},{"location":"research/profiles_overlays_research/#next-steps","title":"Next steps","text":"<ul> <li>Finalize a minimal on\u2011disk schema and validator</li> <li>Add CLI to apply/check profiles over emitted tables</li> <li>Pilot with Address and Contract examples from the docs</li> </ul>"}]}